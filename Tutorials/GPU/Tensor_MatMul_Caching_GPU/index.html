
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Open source compiler for generating fast, compute-intensive loops used in AI algorithms, from Microsoft Research.">
      
      
        <meta name="author" content="Microsoft">
      
      
      
        <link rel="prev" href="../Hello_MatMul_GPU/">
      
      
        <link rel="next" href="../Tensor_MatMul_ElementWiseOps_GPU/">
      
      <link rel="icon" href="../../../assets/logos/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.6">
    
    
      
        <title>Tensor MatMul Caching GPU - Accera</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.ded33207.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i%7CSource+Code+Pro:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Open Sans";--md-code-font:"Source Code Pro"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tensor-matmul-on-gpu-caching" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Accera" class="md-header__button md-logo" aria-label="Accera" data-md-component="logo">
      
  <img src="../../../assets/logos/A_lighttext.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Accera
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tensor MatMul Caching GPU
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="light-blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/microsoft/Accera/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    microsoft/Accera
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../Case%20Studies/" class="md-tabs__link">
        Case Studies
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../Install/" class="md-tabs__link">
        Install
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../Manual/" class="md-tabs__link">
        Manual
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../Reference/accera/" class="md-tabs__link">
        Reference
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../" class="md-tabs__link md-tabs__link--active">
        Tutorials
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Accera" class="md-nav__button md-logo" aria-label="Accera" data-md-component="logo">
      
  <img src="../../../assets/logos/A_lighttext.png" alt="logo">

    </a>
    Accera
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/microsoft/Accera/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    microsoft/Accera
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Case Studies
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Case Studies
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Case%20Studies/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Case%20Studies/CONTRIBUTING/" class="md-nav__link">
        CONTRIBUTING
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Install
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Install
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Install/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Install/Building_on_MacOS/" class="md-nav__link">
        Building on MacOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Install/Building_on_Ubuntu/" class="md-nav__link">
        Building on Ubuntu
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Install/Building_on_Windows/" class="md-nav__link">
        Building on Windows
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Install/Installing_Accera_on_MacOS/" class="md-nav__link">
        Installing Accera on MacOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Install/Installing_Accera_on_Ubuntu/" class="md-nav__link">
        Installing Accera on Ubuntu
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Install/Installing_Accera_on_Windows/" class="md-nav__link">
        Installing Accera on Windows
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          Manual
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Manual
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/00%20Introduction/" class="md-nav__link">
        00 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/01%20Arrays%20and%20Scalars/" class="md-nav__link">
        01 Arrays and Scalars
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/02%20Simple%20Affine%20Loop%20Nests/" class="md-nav__link">
        02 Simple Affine Loop Nests
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/03%20Schedules/" class="md-nav__link">
        03 Schedules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/04%20Fusing/" class="md-nav__link">
        04 Fusing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/05%20Targets/" class="md-nav__link">
        05 Targets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/06%20Plans%20-%20Caching/" class="md-nav__link">
        06 Plans   Caching
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/07%20Plans%20-%20Operations%20and%20Optimizations/" class="md-nav__link">
        07 Plans   Operations and Optimizations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/08%20Deferred%20Layout%20of%20Constant%20Arrays/" class="md-nav__link">
        08 Deferred Layout of Constant Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/09%20Parameters/" class="md-nav__link">
        09 Parameters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/10%20Packages/" class="md-nav__link">
        10 Packages
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manual/11%20Plans%20-%20GPU%20Tensorization/" class="md-nav__link">
        11 Plans   GPU Tensorization
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/accera/" class="md-nav__link">
        Accera
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/safety_analysis/" class="md-nav__link">
        Safety analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          Classes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          Classes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_1" id="__nav_5_3_1_label" tabindex="0">
          Array
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_1">
          <span class="md-nav__icon md-icon"></span>
          Array
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Array/Array/" class="md-nav__link">
        Array
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Array/Layout/" class="md-nav__link">
        Layout
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Array/deferred_layout/" class="md-nav__link">
        Deferred layout
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Array/slice/" class="md-nav__link">
        Slice
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Array/sub_array/" class="md-nav__link">
        Sub array
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2" id="__nav_5_3_2_label" tabindex="0">
          Dimension
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2">
          <span class="md-nav__icon md-icon"></span>
          Dimension
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Dimension/Dimension/" class="md-nav__link">
        Dimension
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_3" id="__nav_5_3_3_label" tabindex="0">
          FusedSchedule
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_3">
          <span class="md-nav__icon md-icon"></span>
          FusedSchedule
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/FusedSchedule/get_fused_indices/" class="md-nav__link">
        Get fused indices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/FusedSchedule/get_fusing_index/" class="md-nav__link">
        Get fusing index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/FusedSchedule/get_unfused_indices/" class="md-nav__link">
        Get unfused indices
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_4" id="__nav_5_3_4_label" tabindex="0">
          Nest
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_4">
          <span class="md-nav__icon md-icon"></span>
          Nest
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Nest/Nest/" class="md-nav__link">
        Nest
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Nest/create_plan/" class="md-nav__link">
        Create plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Nest/create_schedule/" class="md-nav__link">
        Create schedule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Nest/get_indices/" class="md-nav__link">
        Get indices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Nest/iteration_logic/" class="md-nav__link">
        Iteration logic
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_5" id="__nav_5_3_5_label" tabindex="0">
          Package
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_5">
          <span class="md-nav__icon md-icon"></span>
          Package
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Package/Format/" class="md-nav__link">
        Format
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Package/Mode/" class="md-nav__link">
        Mode
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Package/Package/" class="md-nav__link">
        Package
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Package/Platform/" class="md-nav__link">
        Platform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Package/add/" class="md-nav__link">
        Add
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Package/add_description/" class="md-nav__link">
        Add description
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Package/build/" class="md-nav__link">
        Build
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_6" id="__nav_5_3_6_label" tabindex="0">
          Plan
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_6">
          <span class="md-nav__icon md-icon"></span>
          Plan
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Plan/bind/" class="md-nav__link">
        Bind
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Plan/cache/" class="md-nav__link">
        Cache
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Plan/kernelize/" class="md-nav__link">
        Kernelize
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Plan/parallelize/" class="md-nav__link">
        Parallelize
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Plan/tensorize/" class="md-nav__link">
        Tensorize
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Plan/unroll/" class="md-nav__link">
        Unroll
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Plan/vectorize/" class="md-nav__link">
        Vectorize
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_7" id="__nav_5_3_7_label" tabindex="0">
          Scalar
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_7">
          <span class="md-nav__icon md-icon"></span>
          Scalar
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Scalar/Scalar/" class="md-nav__link">
        Scalar
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_8" id="__nav_5_3_8_label" tabindex="0">
          Schedule
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_8">
          <span class="md-nav__icon md-icon"></span>
          Schedule
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/create_plan/" class="md-nav__link">
        Create plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/get_indices/" class="md-nav__link">
        Get indices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/is_valid_loop_order/" class="md-nav__link">
        Is valid loop order
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/pad/" class="md-nav__link">
        Pad
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/reorder/" class="md-nav__link">
        Reorder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/skew/" class="md-nav__link">
        Skew
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/split/" class="md-nav__link">
        Split
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Schedule/tile/" class="md-nav__link">
        Tile
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_9" id="__nav_5_3_9_label" tabindex="0">
          Target
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_9">
          <span class="md-nav__icon md-icon"></span>
          Target
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Target/Architecture/" class="md-nav__link">
        Architecture
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Target/Category/" class="md-nav__link">
        Category
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Target/Model/" class="md-nav__link">
        Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Target/Runtime/" class="md-nav__link">
        Runtime
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/classes/Target/Target/" class="md-nav__link">
        Target
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
          Enumerations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_4">
          <span class="md-nav__icon md-icon"></span>
          Enumerations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/enumerations/CacheStrategy/" class="md-nav__link">
        CacheStrategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/enumerations/MMAFragmentOp/" class="md-nav__link">
        MMAFragmentOp
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/enumerations/MMASchedulingPolicy/" class="md-nav__link">
        MMASchedulingPolicy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/enumerations/MMAShape/" class="md-nav__link">
        MMAShape
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/enumerations/Role/" class="md-nav__link">
        Role
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/enumerations/ScalarType/" class="md-nav__link">
        ScalarType
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
          Functions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_5">
          <span class="md-nav__icon md-icon"></span>
          Functions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/functions/cast/" class="md-nav__link">
        Cast
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/functions/create_dimensions/" class="md-nav__link">
        Create dimensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/functions/create_parameter_grid/" class="md-nav__link">
        Create parameter grid
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/functions/create_parameters/" class="md-nav__link">
        Create parameters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reference/functions/fuse/" class="md-nav__link">
        Fuse
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Hello_MatMul/" class="md-nav__link">
        Hello MatMul
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Optimized_MatMul/" class="md-nav__link">
        Optimized MatMul
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Pi3_Cross_Compilation/" class="md-nav__link">
        Pi3 Cross Compilation
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_5" checked>
      
      
      
        <label class="md-nav__link" for="__nav_6_5" id="__nav_6_5_label" tabindex="0">
          GPU
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_5_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_6_5">
          <span class="md-nav__icon md-icon"></span>
          GPU
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Hello_MatMul_GPU/" class="md-nav__link">
        Hello MatMul GPU
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Tensor MatMul Caching GPU
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Tensor MatMul Caching GPU
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    Prerequisites
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#input-data-caching" class="md-nav__link">
    Input data caching
  </a>
  
    <nav class="md-nav" aria-label="Input data caching">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sequential-caching" class="md-nav__link">
    Sequential caching
  </a>
  
    <nav class="md-nav" aria-label="Sequential caching">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarking-results-using-hatlib" class="md-nav__link">
    Benchmarking results using hatlib
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overlapped-caching-aka-double-buffering" class="md-nav__link">
    Overlapped caching (a.k.a. Double Buffering)
  </a>
  
    <nav class="md-nav" aria-label="Overlapped caching (a.k.a. Double Buffering)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarking-results-using-hatlib_1" class="md-nav__link">
    Benchmarking results using hatlib
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-data-caching" class="md-nav__link">
    Output data caching
  </a>
  
    <nav class="md-nav" aria-label="Output data caching">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarking-results-using-hatlib_2" class="md-nav__link">
    Benchmarking results using hatlib
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Tensor_MatMul_ElementWiseOps_GPU/" class="md-nav__link">
        Tensor MatMul ElementWiseOps GPU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Tensor_MatMul_GPU/" class="md-nav__link">
        Tensor MatMul GPU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Tensor_MatMul_MultiPass/" class="md-nav__link">
        Tensor MatMul MultiPass
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Tensor_MatMul_SchedulingPolicy_GPU/" class="md-nav__link">
        Tensor MatMul SchedulingPolicy GPU
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="tensor-matmul-on-gpu-caching">Tensor MatMul on GPU: Caching</h1>
<p>In this tutorial, you will learn how to implement a Matrix Multiplication (MatMul) function that uses specialized matrix multiplication hardware on the GPU while caching data to minimize expensive data traffic to/from global memory.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>You have completed the <a href="../Tensor_MatMul_GPU/">Tensor_MatMul_GPU</a> tutorial.</li>
</ul>
<h2 id="input-data-caching">Input data caching</h2>
<p>Since accessing the same data repeatedly from the global memory can be expensive, we will use the shared memory which is available much closer to the compute units to cache the input data to achieve much faster data accesses.</p>
<p>Since for input data caching we use shared memory, we need to be careful how much of the global data we cache since shared memory is comparatively much smaller in size. For this reason, we introduce an additional split in the K-loop and we use this newly created loop index <code>kk</code> for caching:
<div class="highlight"><pre><span></span><code><span class="n">kk</span> <span class="o">=</span> <span class="n">schedule</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</code></pre></div></p>
<h3 id="sequential-caching">Sequential caching</h3>
<p>In this approach, each thread block starts caching the next tile of input data only after the computation on the current tile is complete. This is the most simple form of shared memory caching which does not involve any overlapped execution of data copy and computation pipelines. This can be achieved by adding the following lines of DSL code:</p>
<div class="highlight"><pre><span></span><code><span class="n">plan</span><span class="o">.</span><span class="n">cache</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">kk</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">MemorySpace</span><span class="o">.</span><span class="n">SHARED</span><span class="p">)</span>
<span class="n">plan</span><span class="o">.</span><span class="n">cache</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">kk</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">MemorySpace</span><span class="o">.</span><span class="n">SHARED</span><span class="p">)</span>
</code></pre></div>
<p>The complete python script with caching of input data into shared memory can be found <a href="../../hello_matmul_gpu/tensor_input_cache_matmul_gpu_generator.py">here</a>.</p>
<p>This generates the following kernel code, note the barriers in the generated code to see how caching of the next tile waits for the computation of the current tile to finish:
<div class="highlight"><pre><span></span><code><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="n">__global__</span><span class="w">  </span><span class="n">__launch_bounds__</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">tensor_input_cache_matmul_gpu_866f5763c1d8d520__gpu__</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Calculate threadid offsets and other locals</span>
<span class="w">    </span><span class="cm">/*...*/</span>

<span class="w">    </span><span class="c1">// Declare shared memory caches for A and B</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">var8</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">256</span><span class="p">];</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">var9</span><span class="p">[</span><span class="mi">256</span><span class="p">][</span><span class="mi">32</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// k-loop</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx16</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx16</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">8</span><span class="p">;</span><span class="w"> </span><span class="n">idx16</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">var17</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">idx16</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// Wait for compute on previously cached items to finish</span>
<span class="w">        </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// Cache current tile of A into shared memory</span>
<span class="w">        </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Shared</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var11</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="n">var7</span><span class="p">,</span><span class="w"> </span><span class="n">var17</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_0_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var8</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Cache current tile of B into shared memory</span>
<span class="w">        </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Shared</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var11</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">arg1</span><span class="p">,</span><span class="w"> </span><span class="n">var17</span><span class="p">,</span><span class="w"> </span><span class="n">var5</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_1_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var9</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Wait for input caching to finish</span>
<span class="w">        </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// kk-loop</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx18</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx18</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="n">idx18</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">var19</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">idx18</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// Declare matrix fragments for A, B and C</span>
<span class="w">            </span><span class="cm">/*...*/</span>

<span class="w">            </span><span class="c1">// Load C from global memory</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var11</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_22</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...);</span>

<span class="w">            </span><span class="c1">// Load A and B from shared memory cache</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">256</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var11</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_20</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">var8</span><span class="p">[</span><span class="n">var12</span><span class="p">][</span><span class="n">var19</span><span class="p">]);</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var11</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_21</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">var9</span><span class="p">[</span><span class="n">var19</span><span class="p">][</span><span class="n">var14</span><span class="p">]);</span>

<span class="w">            </span><span class="c1">// Compute matrix multiplication</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mmaMatrix_22</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_20</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_21</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_22</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// Store result into global memory</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var11</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...,</span><span class="w"> </span><span class="n">mmaMatrix_22</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div></p>
<h4 id="benchmarking-results-using-hatlib">Benchmarking results using <code>hatlib</code></h4>
<p>Similar to the previous experiments we can use hatlib to benchmark this kernel using the following command:
<div class="highlight"><pre><span></span><code>python3<span class="w"> </span>-m<span class="w"> </span>hatlib.benchmark_hat_package<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>tensor_input_cache_matmul_gpu.hat&gt;<span class="w"> </span>--cpp<span class="w"> </span>--min_time_in_sec<span class="w"> </span><span class="m">10</span><span class="w"> </span>--time_in_ms
</code></pre></div></p>
<p>This produces the following output which shows that sequential caching reduces the runtime to <strong>~3 ms</strong> which is <strong>~30%</strong> faster than the non-cached version presented in <a href="../Tensor_MatMul_GPU/#benchmarking-results-using-hatlib">Tensor_MatMul_GPU.md</a>:</p>
<div class="highlight"><pre><span></span><code><span class="w">                                    </span>function_name<span class="w">       </span>mean<span class="w">  </span>median_of_means<span class="w">  </span>mean_of_small_means<span class="w">  </span>robust_mean<span class="w">  </span>min_of_means
<span class="m">0</span><span class="w">  </span>tensor_input_cache_matmul_gpu_866f5763c1d8d520<span class="w"> </span><span class="m">3</span>.02507486<span class="w">       </span><span class="m">3</span>.02532532<span class="w">           </span><span class="m">3</span>.02407842<span class="w">   </span><span class="m">3</span>.02519751<span class="w">    </span><span class="m">3</span>.02233856
</code></pre></div>
<h3 id="overlapped-caching-aka-double-buffering">Overlapped caching (a.k.a. Double Buffering)</h3>
<p>In this approach, each thread block prefetches the next tile into registers while the current tile is being computed. This overlapped execution of data copy and compute typically achieves better performance by utilizing different hardware pipelines more efficiently. Using Accera DSL, this can be done by setting the <code>double_buffer</code> flag in the <a href="../../../Reference/classes/Plan/cache/"><code>plan.cache</code></a> call:</p>
<div class="highlight"><pre><span></span><code><span class="n">plan</span><span class="o">.</span><span class="n">cache</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">kk</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">MemorySpace</span><span class="o">.</span><span class="n">SHARED</span><span class="p">,</span> <span class="n">double_buffer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">double_buffer_location</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">MemorySpace</span><span class="o">.</span><span class="n">PRIVATE</span><span class="p">)</span>
<span class="n">plan</span><span class="o">.</span><span class="n">cache</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">kk</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">MemorySpace</span><span class="o">.</span><span class="n">SHARED</span><span class="p">,</span> <span class="n">double_buffer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">double_buffer_location</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">MemorySpace</span><span class="o">.</span><span class="n">PRIVATE</span><span class="p">)</span>
</code></pre></div>
<p>The complete python script with caching of input data using double buffering can be found <a href="../../hello_matmul_gpu/tensor_input_double_buffer_cache_matmul_gpu_generator.py">here</a>.</p>
<p>The generated kernel code looks something like this, note how the prefetch of the next tile and the computation of the current tile happen without synchronization to achieve <em>global memory latency hiding</em>:
<div class="highlight"><pre><span></span><code><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="n">__global__</span><span class="w">  </span><span class="n">__launch_bounds__</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">tensor_input_double_buffer_cache_matmul_gpu_ce60189b3e52267d__gpu__</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Calculate threadid offsets and other locals</span>
<span class="w">    </span><span class="cm">/*...*/</span>

<span class="w">    </span><span class="c1">// Declare register caches for prefetching input data of A and B</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">var8</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">var9</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">1</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Declare shared memory caches for A and B</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">var10</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">256</span><span class="p">];</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">var11</span><span class="p">[</span><span class="mi">256</span><span class="p">][</span><span class="mi">32</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Cache tile 0 of A from global memory to shared memory</span>
<span class="w">    </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Shared</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="n">var7</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_0_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var10</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Cache tile 0 of B from global memory to shared memory</span>
<span class="w">    </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Shared</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">arg1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">var5</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_1_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var11</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Wait for tile 0 data to finish copying</span>
<span class="w">    </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// k-loop (Current tile)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx18</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx18</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">7</span><span class="p">;</span><span class="w"> </span><span class="n">idx18</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Prefetch next tile of A from global memory to registers</span>
<span class="w">        </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Private</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="n">var7</span><span class="p">,</span><span class="w"> </span><span class="n">var20</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_0_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var9</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Prefetch next tile of B from global memory to registers</span>
<span class="w">        </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Private</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">arg1</span><span class="p">,</span><span class="w"> </span><span class="n">var20</span><span class="p">,</span><span class="w"> </span><span class="n">var5</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_1_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var8</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// kk-loop</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx24</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx24</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="n">idx24</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// Declare matrix fragments for A, B and C</span>
<span class="w">            </span><span class="cm">/*...*/</span>

<span class="w">            </span><span class="c1">// Perform matmul on the current tile from shared memory</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_28</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...);</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">256</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_26</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">var10</span><span class="p">[</span><span class="n">var14</span><span class="p">][</span><span class="n">var25</span><span class="p">]);</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_27</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">var11</span><span class="p">[</span><span class="n">var25</span><span class="p">][</span><span class="n">var16</span><span class="p">]);</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mmaMatrix_28</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_26</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_27</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_28</span><span class="p">);</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...,</span><span class="w"> </span><span class="n">mmaMatrix_28</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// Wait for matmul on current tile to finish</span>
<span class="w">        </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// Copy prefetched data of A from registers to shared memory</span>
<span class="w">        </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Private</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Shared</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var11</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_4_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var8</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Copy prefetched data of B from registers to shared memory</span>
<span class="w">        </span><span class="n">block_copy</span><span class="o">&lt;</span><span class="n">CopyMode</span><span class="o">::</span><span class="n">Striped</span><span class="p">,</span><span class="w"> </span><span class="cm">/*SRC_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*DST_ROW_MAJOR*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*STRIDE*/</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="cm">/*WPT*/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*TILE_R,C*/</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="cm">/*BLOCK_DIM_X,Y,Z*/</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Private</span><span class="p">,</span><span class="w"> </span><span class="n">MemSpace</span><span class="o">::</span><span class="n">Shared</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var10</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">affine_map_func_3_i0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">var9</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Wait for copy to finish before starting next tile</span>
<span class="w">        </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Last tile (loop peeling)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx19</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx19</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="n">idx19</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Declare matrix fragments for A, B and C</span>
<span class="w">        </span><span class="cm">/*...*/</span>

<span class="w">        </span><span class="c1">// Perform matmul on the last tile from shared memory</span>
<span class="w">        </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_23</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...);</span>
<span class="w">        </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">256</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_21</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">var10</span><span class="p">[</span><span class="n">var14</span><span class="p">][</span><span class="n">var20</span><span class="p">]);</span>
<span class="w">        </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_22</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">var11</span><span class="p">[</span><span class="n">var20</span><span class="p">][</span><span class="n">var16</span><span class="p">]);</span>
<span class="w">        </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mmaMatrix_23</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_21</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_22</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_23</span><span class="p">);</span>
<span class="w">        </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var13</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...,</span><span class="w"> </span><span class="n">mmaMatrix_23</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div></p>
<h4 id="benchmarking-results-using-hatlib_1">Benchmarking results using <code>hatlib</code></h4>
<p>Benchmarking the above kernel with hatlib shows that double-buffer caching further reduces the runtime to <strong>~1.45 ms</strong> which is <strong>~66%</strong> faster than the non-cached version presented in <a href="../Tensor_MatMul_GPU/#benchmarking-results-using-hatlib">Tensor_MatMul_GPU.md</a>:</p>
<div class="highlight"><pre><span></span><code><span class="w">                                       </span>function_name<span class="w">       </span>mean<span class="w">  </span>median_of_means<span class="w">  </span>mean_of_small_means<span class="w">  </span>robust_mean<span class="w">  </span>min_of_means
<span class="m">0</span><span class="w">  </span>tensor_input_double_buffer_cache_matmul_gpu_ce...<span class="w"> </span><span class="m">1</span>.45501032<span class="w">       </span><span class="m">1</span>.45495605<span class="w">           </span><span class="m">1</span>.45370367<span class="w">   </span><span class="m">1</span>.45489838<span class="w">    </span><span class="m">1</span>.45116257
</code></pre></div>
<h2 id="output-data-caching">Output data caching</h2>
<p>Similar to input caching, the result data can also be cached to prevent unnecessary global memory accesses. Here we will see how we can accumulate the result in registers before copying it to global memory. This is can be done by adding:</p>
<div class="highlight"><pre><span></span><code><span class="n">plan</span><span class="o">.</span><span class="n">cache</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">MemorySpace</span><span class="o">.</span><span class="n">MMA_FRAGMENT</span><span class="p">)</span>
</code></pre></div>
<p>The complete python script with both input and output caching can be found <a href="../../hello_matmul_gpu/tensor_input_output_cache_matmul_gpu_generator.py">here</a>.</p>
<div class="highlight"><pre><span></span><code><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="n">__global__</span><span class="w">  </span><span class="n">__launch_bounds__</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">tensor_input_output_cache_matmul_gpu_1b4d39ede237d688__gpu__</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">arg2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Calculate threadid offsets and other locals</span>
<span class="w">    </span><span class="cm">/*...*/</span>

<span class="w">    </span><span class="c1">// Declare register caches for prefetching input data of A and B</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">var8</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">var9</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">1</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Declare shared memory caches for A and B</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">var10</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">256</span><span class="p">];</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">var11</span><span class="p">[</span><span class="mi">256</span><span class="p">][</span><span class="mi">32</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Declare fragment cache (registers) for output, C</span>
<span class="w">    </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">rocwmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">mmaMatrix_12</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Fill output cache with data from global memory</span>
<span class="w">    </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var18</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_12</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...);</span>

<span class="w">    </span><span class="c1">// Cache tile 0 of A and B from global memory to shared memory</span>
<span class="w">    </span><span class="cm">/*...*/</span>

<span class="w">    </span><span class="c1">// Wait for tile 0 data to finish copying</span>
<span class="w">    </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// k-loop (Current tile)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx19</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx19</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">7</span><span class="p">;</span><span class="w"> </span><span class="n">idx19</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Prefetch next tile of A and B from global memory to registers</span>
<span class="w">        </span><span class="cm">/*...*/</span>

<span class="w">        </span><span class="c1">// kk-loop</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx25</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx25</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="n">idx25</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// Declare matrix fragments for A and B</span>
<span class="w">            </span><span class="cm">/*...*/</span>

<span class="w">            </span><span class="c1">// Load A and B from shared memory cache</span>
<span class="w">            </span><span class="cm">/*...*/</span>

<span class="w">            </span><span class="c1">// Compute matrix multiplication and accumulate in fragment cache</span>
<span class="w">            </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mmaMatrix_12</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_27</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_28</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_12</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// Wait for matmul on current tile to finish</span>
<span class="w">        </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// Copy prefetched data of A and B from registers to shared memory</span>
<span class="w">        </span><span class="cm">/*...*/</span>

<span class="w">        </span><span class="c1">// Wait for copy to finish before starting next tile</span>
<span class="w">        </span><span class="n">__builtin_amdgcn_s_barrier</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Last tile (loop peeling)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">idx20</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx20</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="n">idx20</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Declare matrix fragments for A and B</span>
<span class="w">        </span><span class="cm">/*...*/</span>

<span class="w">        </span><span class="c1">// Load A and B from shared memory cache</span>
<span class="w">        </span><span class="cm">/*...*/</span>

<span class="w">        </span><span class="c1">// Compute matrix multiplication and accumulate in fragment cache</span>
<span class="w">        </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mmaMatrix_12</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_22</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_23</span><span class="p">,</span><span class="w"> </span><span class="n">mmaMatrix_12</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Store result into global memory ONCE!</span>
<span class="w">    </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">rocwmma</span><span class="o">::</span><span class="n">layout_t</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="o">&gt;</span><span class="p">(</span><span class="n">var18</span><span class="p">,</span><span class="w"> </span><span class="n">arg2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...,</span><span class="w"> </span><span class="n">mmaMatrix_12</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>
<h4 id="benchmarking-results-using-hatlib_2">Benchmarking results using <code>hatlib</code></h4>
<p>Benchmarking the above kernel with hatlib shows that double-buffer caching combined with output caching further reduces the runtime to <strong>~1.34 ms</strong> which is an overall <strong>~69%</strong> improvement compared to the non-cached version presented in <a href="../Tensor_MatMul_GPU/#benchmarking-results-using-hatlib">Tensor_MatMul_GPU.md</a>:</p>
<div class="highlight"><pre><span></span><code><span class="w">                                       </span>function_name<span class="w">       </span>mean<span class="w">  </span>median_of_means<span class="w">  </span>mean_of_small_means<span class="w">  </span>robust_mean<span class="w">  </span>min_of_means
<span class="m">0</span><span class="w">  </span>tensor_input_output_cache_matmul_gpu_1b4d39ede...<span class="w"> </span><span class="m">1</span>.33967323<span class="w">       </span><span class="m">1</span>.33956711<span class="w">           </span><span class="m">1</span>.33841698<span class="w">   </span><span class="m">1</span>.33953149<span class="w">    </span><span class="m">1</span>.33726944
</code></pre></div>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      2023-04-17
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022+ Microsoft Research [MIT License]
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/microsoft/Accera" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.51198bba.min.js"></script>
      
    
  </body>
</html>