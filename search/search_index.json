{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#problem-at-hand","title":"Problem at Hand","text":"<p>Writing highly optimized compute-intensive code in a traditional programming language is strenuous and time-consuming. Not only does it require advanced engineering skills such as fluency in Assembly language, but a deep understanding of computer architecture is also indispensable. Manual optimization of even the simplest numerical algorithms demands a significant engineering effort. Needless to say, a highly optimized numerical code is often prone to bugs, lacks readability, and offers little to no usability. Code maintenance becomes a nightmare resulting in the reimplementation of the same logic every time an architecture level change is introduced.</p>"},{"location":"#accera-an-optimized-solution","title":"Accera: An Optimized Solution","text":"<p>Accera is a compiler that enables you to experiment with loop optimizations without hand-writing Assembly code. With Accera, these problems and impediments can be addressed in an optimized way. It is available as a Python library and supports cross-compiling to a wide range of processor targets.</p> <p>Accera has THREE primary goals:</p> <ul> <li>Performance: To guarantee the fastest implementation for any compute-intensive algorithm.</li> <li>Readability: To ensure effective implementation of algorithms without sacrificing the readability of code.</li> <li>Writability: To provide a user-friendly programming model, designed for agility and maintainability.</li> </ul>"},{"location":"#install","title":"Install","text":"<p>To install for Linux, macOS, or Windows (requires Python 3.7-3.10):</p> <pre><code>pip install accera\n</code></pre> <p>See the Install Instructions for more details on installing pre-built Python 3 packages and how to build Accera from the source.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>In this example, we will:</p> <ul> <li>Implement matrix multiplication with a ReLU activation (matmul + ReLU), commonly used in machine learning algorithms.</li> <li>Generate two implementations: a naive algorithm and loop-based transformations.</li> <li>Compare the execution time of both implementations.</li> </ul>"},{"location":"#run-in-your-browser","title":"Run in your browser","text":"<p>No installation is required. This will launch a Jupyter notebook with the quickstart example running in the cloud.</p>"},{"location":"#run-on-your-machine","title":"Run on your machine","text":"<ol> <li> <p>Create a Python 3 script called <code>quickstart.py</code>:</p> <pre><code>import accera as acc\n\n# define placeholder inputs/output\nA = acc.Array(role=acc.Role.INPUT, shape=(512, 512))\nB = acc.Array(role=acc.Role.INPUT, shape=(512, 512))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, shape=(512, 512))\n\n# implement the logic for matmul and relu\nmatmul = acc.Nest(shape=(512, 512, 512))\ni1, j1, k1 = matmul.get_indices()\n@matmul.iteration_logic\ndef _():\n    C[i1, j1] += A[i1, k1] * B[k1, j1]\n\nrelu = acc.Nest(shape=(512, 512))\ni2, j2 = relu.get_indices()\n@relu.iteration_logic\ndef _():\n    C[i2, j2] = acc.max(C[i2, j2], 0.0)\n\npackage = acc.Package()\n\n# fuse the i and j indices of matmul and relu, add to the package\nschedule = acc.fuse(matmul.create_schedule(), relu.create_schedule(), partial=2)\npackage.add(schedule, args=(A, B, C), base_name=\"matmul_relu_fusion_naive\")\n\n# transform the schedule, add to the package\ni, j, f, k = schedule.get_indices()\nii, jj = schedule.tile({\n    i: 16,\n    j: 16\n}) # loop tiling\nschedule.reorder(j, i, f, k, jj, ii) # loop reordering\nplan = schedule.create_plan()\nplan.unroll(ii) # loop unrolling\npackage.add(plan, args=(A, B, C), base_name=\"matmul_relu_fusion_transformed\")\n\n# build a dynamically-linked package (a .dll or .so) that exports both functions\nprint(package.build(name=\"hello_accera\", format=acc.Package.Format.HAT_DYNAMIC))\n</code></pre> </li> <li> <p>Ensure that you have a compiler in your PATH:</p> <ul> <li>Windows: Install Microsoft Visual Studio and run <code>vcvars64.bat</code> to setup the command prompt.</li> <li>Linux/macOS: Install gcc</li> </ul> <p>Don't have a compiler handy? We recommend trying Accera in your browser instead </p> </li> <li> <p>Install Accera:</p> <pre><code>pip install accera\n</code></pre> </li> <li> <p>Generate the library that implements two versions of matmul + ReLU:</p> <pre><code>python quickstart.py\n</code></pre> </li> <li> <p>To consume and compare the library functions, create a file called <code>benchmark.py</code> in the same location:</p> <pre><code>import hatlib as hat\nimport numpy as np\n\n# load the package\n_, functions = hat.load(\"hello_accera.hat\")\n\n# call one of the functions with test inputs\nA_test = np.random.rand(512, 512).astype(np.float32)\nB_test = np.random.rand(512, 512).astype(np.float32)\nC_test = np.zeros((512, 512)).astype(np.float32)\nC_numpy = np.maximum(C_test + A_test @ B_test, 0.0)\n\nmatmul_relu = functions[\"matmul_relu_fusion_transformed\"]\nmatmul_relu(A_test, B_test, C_test)\n\n# check correctness\nnp.testing.assert_allclose(C_test, C_numpy, atol=1e-3)\n\n# benchmark all functions\nhat.run_benchmark(\"hello_accera.hat\", batch_size=5, min_time_in_sec=5)\n</code></pre> </li> <li> <p>Run the benchmark to get the execution time results:</p> <pre><code>python benchmark.py\n</code></pre> </li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<p>The Manual is the best introductory resource for the Accera Python programming model.</p> <p>In particular, the schedule transformations describe how you can experiment with different loop transformations with just a few lines of Python code.</p> <p>Finally, the <code>.hat</code> format is just a C header file containing the metadata. Learn more about the HAT format and benchmarking.</p>"},{"location":"#how-it-works","title":"How it works","text":"<p>In a nutshell, Accera takes the Python code that defines the loop schedule and algorithm while converting it into MLIR intermediate representation (IR). Accera's compiler then takes this IR through a series of MLIR pipelines to perform transformations. The result is a binary library with a C header file. The library implements the algorithms that are defined in Python and it is compatible with the target.</p> <p>To peek into the stages of IR transformation that Accera does, try replacing <code>format=acc.Package.Format.HAT_DYNAMIC</code> with <code>format=acc.Package.Format.MLIR_DYNAMIC</code> in <code>quickstart.py</code>, re-run the script, and search the <code>_tmp</code> subfolder for the intermediate <code>*.mlir</code> files. We plan to document these IR constructs in the future.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Get familiar with Accera's concepts and Python constructs in the Documentation page.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Step-by-step examples are available on the Tutorials page. We're working on adding more complementary examples and tutorials.</p>"},{"location":"#contributions","title":"Contributions","text":"<p>Accera is a research platform-in-progress that can certainly benefit from your contributions. We would love your feedback, recommendations, and feature requests. Not to mention that we are excited to answer your questions. Let\u2019s collaborate! Please file a  Github issue or send us a pull request. Please review the Microsoft Code of Conduct to learn more.</p>"},{"location":"#credits","title":"Credits","text":"<p>Accera is built using several open source libraries, including: LLVM, pybind11, toml++, tomlkit, vcpkg, pyyaml, and HAT. For testing, we used numpy and catch2.</p>"},{"location":"#license","title":"License","text":"<p>This project is released under the MIT License.</p>"},{"location":"Case%20Studies/","title":"Accera Case Studies","text":"<p>Accera case studies are community-provided samples that showcase the Accera language and programming model. To contribute a case study of your own, follow these instructions.</p>"},{"location":"Case%20Studies/#matmul-grid-search","title":"MatMul Grid Search","text":"<ul> <li>MatMul Grid Search</li> </ul>"},{"location":"Case%20Studies/#convolution","title":"Convolution","text":"<ul> <li> <p>NCHWc 2D Convolution Grid Search</p> </li> <li> <p>Unrolled 2D Convolution Grid Search</p> </li> </ul>"},{"location":"Case%20Studies/#three-matrix-multiplication","title":"Three Matrix Multiplication","text":"<ul> <li>(Coming soon)</li> </ul>"},{"location":"Case%20Studies/CONTRIBUTING/","title":"Contributing Guide","text":"<p>Thank you for investing your time contributing a community case study!</p> <p>In this guide, you will get an overview of the contribution workflow.</p>"},{"location":"Case%20Studies/CONTRIBUTING/#getting-started","title":"Getting started","text":"<p>Read our Code of Conduct to keep our community approachable and respectable.</p> <p>Refer to the Manual and Tutorials to familiarize yourself with the Accera language and programming model.</p>"},{"location":"Case%20Studies/CONTRIBUTING/#components-of-a-good-case-study","title":"Components of a good case study","text":"<p>A good case study should have these components and characteristics:</p> <ol> <li> <p>Solves one specific task, such as matrix multiplication, matrix convolution, vector addition. If you have a series of tasks to solve, break them up into multiple case studies that reference one another.</p> </li> <li> <p>Includes working Accera Python code implementing that task. At the end of the case study, the code should produce a HAT package using <code>accera.Package.build()</code>.</p> </li> <li> <p>Describes the thought process, considerations, pros and cons of your implementation in a <code>README.md</code>.</p> </li> <li> <p>If the case study generates several implementations (for example, using Parameter Grids), include the following:</p> </li> <li>Benchmark results on a target machine (for example, your laptop). You can run <code>hatlib.run_benchmark</code> on your HAT package.</li> <li> <p>A description of the make and model of that target machine you used (for example, Intel Xeon E5). If you are unsure, you can use the output of this command:</p> <pre><code>python -m cpuinfo\n</code></pre> </li> </ol> <p>For some examples, refer to the published case studies in the Table of Contents.</p>"},{"location":"Case%20Studies/CONTRIBUTING/#publishing-your-case-study","title":"Publishing your case study","text":"<p>All community case studies are published directly from the author's GitHub repository and linked to from the Accera GitHub repository.</p> <p>Once you are ready to publish your case study: 1. Make your case study GitHub repository public (if you haven't done so already).</p> <ol> <li> <p>Edit Case Studies/README.md to add your case study to the Table of Contents. The link should point to the git SHA for your latest commit. The format to use is: https://github.com/user/repo/blob/git_sha/path_to_case_study/README.md.</p> </li> <li> <p>Create a Pull Request to submit your edits to Case Studies/README.md.</p> </li> </ol>"},{"location":"Install/","title":"Install from PyPI","text":"<p>The quickest way to get up and running is to install the pre-built Python packages:</p> <ul> <li>MacOS</li> <li>Ubuntu</li> <li>Windows</li> </ul>"},{"location":"Install/#build-and-install","title":"Build and Install","text":"<p>You can also build and install the latest version of Accera by following these instructions:</p> <ul> <li>MacOS</li> <li>Ubuntu</li> <li>Windows</li> </ul>"},{"location":"Install/Building_on_MacOS/","title":"Building on MacOS","text":""},{"location":"Install/Building_on_MacOS/#installing-on-macos","title":"Installing on MacOS","text":""},{"location":"Install/Building_on_MacOS/#install-dependencies","title":"Install Dependencies","text":"<p>Accera requires the following tools and libraries:</p> <ul> <li>A C++ compiler that supports C++ 17, such as <code>clang</code>, which is bundled in XCode</li> <li>CMake 3.14 or newer</li> <li>Python 3.7 or newer</li> <li>Ninja</li> <li>Ccache</li> <li>LLVM OpenMP 5, if using parallelization</li> </ul> <p>Homebrew is a package manager that makes it easy to install the prerequisites. Homebrew can be downloaded and installed by:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>If you already have Homebrew installed, update it to the latest version by typing:</p> <pre><code>brew update\n</code></pre> <p>Install the dependencies:</p> Intel MacOS Apple Silicon <code>brew install cmake python ninja-build ccache libomp pkg-config</code> <code>brew install cmake python ninja ccache libomp pkg-config</code>"},{"location":"Install/Building_on_MacOS/#clang","title":"Clang","text":"<p>Select the <code>clang</code> compiler from XCode:</p> <pre><code>xcode-select --install\n</code></pre>"},{"location":"Install/Building_on_MacOS/#clone-accera","title":"Clone Accera","text":"<p>A version of git should already be included in XCode.</p> <p>Clone the git repository:</p> <pre><code>git clone --recurse-submodules https://github.com/microsoft/Accera\n</code></pre>"},{"location":"Install/Building_on_MacOS/#build-and-install-accera","title":"Build and install Accera","text":"<p>Run the <code>build.sh</code> script to install dependencies and build the Accera Python package (replace <code>&lt;path_to_accera&gt;</code> with the path to the cloned Accera repository).</p> <pre><code>cd &lt;path_to_accera&gt;\nsh ./build.sh\n</code></pre> <p>Update or install the resulting <code>.whl</code> file from the <code>dist</code> sudirectory. The name depends on your Python version, your OS and your CPU architecture. <pre><code>pip install -U ./dist/accera-0.0.1-cp37-cp37-macosx_10_15_x86_64.whl --find-links=dist\n</code></pre></p>"},{"location":"Install/Building_on_MacOS/#build-and-install-using-cmake","title":"Build and install using CMake","text":"<p>Accera can also be built using CMake (intended for expert users).</p>"},{"location":"Install/Building_on_MacOS/#install-dependencies_1","title":"Install dependencies","text":"<pre><code>cd &lt;path_to_accera&gt;\ngit submodule init\ngit submodule update\n./external/vcpkg/bootstrap-vcpkg.sh\n./external/vcpkg/vcpkg install catch2 tomlplusplus accera-llvm --overlay-ports=external/llvm\n</code></pre> <p>The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend reserving at least 20GB of disk space for the LLVM build.</p>"},{"location":"Install/Building_on_MacOS/#configure-cmake","title":"Configure CMake","text":"<pre><code>cd &lt;path_to_accera&gt;\nmkdir build\ncd build\n\ncmake .. -DCMAKE_BUILD_TYPE=Release -G Ninja\n</code></pre>"},{"location":"Install/Building_on_MacOS/#build-and-run-tests","title":"Build and run tests","text":"<pre><code>cmake --build . --config Release\nctest -C Release\n</code></pre>"},{"location":"Install/Building_on_MacOS/#install","title":"Install","text":"<pre><code>cmake --build . --config Release --target install\n</code></pre>"},{"location":"Install/Building_on_Ubuntu/","title":"Building on Ubuntu","text":""},{"location":"Install/Building_on_Ubuntu/#installing-on-ubuntu","title":"Installing on Ubuntu","text":""},{"location":"Install/Building_on_Ubuntu/#quickstart","title":"Quickstart","text":"<p>If you have access to Codespaces, you can launch a Linux VM in the browser or in Visual Studio Code with all the pre-requisites installed:</p> <ol> <li>Go to https://github.com/microsoft/Accera, use the \"&lt;&gt; Code\" drop-down menu, and in the Codespaces tab, click Create codespace on main.</li> <li><code>sh build.sh</code></li> </ol> <p>Step 2 will take some time to build Accera's LLVM fork. Grab a coffee and come back in about an hour or so.</p>"},{"location":"Install/Building_on_Ubuntu/#build-script","title":"Build Script","text":"<p>If you do not have access to Codespaces or prefer to build locally, you can use the <code>build.sh</code> script to build Accera.</p>"},{"location":"Install/Building_on_Ubuntu/#install-dependencies","title":"Install Dependencies","text":"<p>Accera requires the following tools and libraries:</p> <ul> <li>A C++ compiler that supports C++ 17, such as GCC 8</li> <li>CMake 3.14 or newer</li> <li>Python 3.7 or newer</li> <li>Ninja</li> <li>Ccache</li> <li>LLVM OpenMP 5, if using parallelization</li> </ul> <pre><code>sudo apt update\nsudo apt-get install gcc-8 g++-8 cmake python3 python3-pip ninja-build ccache libomp-11-dev pkg-config zip\n</code></pre> <p>Some Ubuntu distributions install an older version of CMake. Check the version of cmake using <code>cmake --version</code>, and download a newer version if older than 3.14.</p>"},{"location":"Install/Building_on_Ubuntu/#clone-accera","title":"Clone Accera","text":"<p>Install git if you don't already have it:</p> <pre><code>sudo apt-get install git\n</code></pre> <p>Clone the git repository</p> <pre><code>git clone --recurse-submodules https://github.com/microsoft/Accera\n</code></pre>"},{"location":"Install/Building_on_Ubuntu/#build-and-install-accera","title":"Build and install Accera","text":"<p>Run the <code>build.sh</code> script to install dependencies and build the Accera Python package (replace <code>&lt;path_to_accera&gt;</code> with the path to the cloned Accera repository).</p> <pre><code>cd &lt;path_to_accera&gt;\nsh ./build.sh\n</code></pre> <p>Update or install the resulting <code>.whl</code> files from the <code>dist</code> subdirectory. The <code>--find-links</code> option tells pip to look at the <code>dist</code> subdirectory for the dependent packages.  The name depends on your Python version, your OS and your CPU architecture.  <pre><code>pip install -U ./dist/accera-0.0.1-cp37-cp37m-linux_x86_64.whl --find-links=dist\n</code></pre></p>"},{"location":"Install/Building_on_Ubuntu/#cmake-builds","title":"CMake Builds","text":"<p>Accera can also be built using CMake (intended for expert users).</p>"},{"location":"Install/Building_on_Ubuntu/#install-dependencies_1","title":"Install dependencies","text":"<pre><code>cd &lt;path_to_accera&gt;\ngit submodule init\ngit submodule update\n./external/vcpkg/bootstrap-vcpkg.sh\n./external/vcpkg/vcpkg install catch2 tomlplusplus accera-llvm --overlay-ports=external/llvm\n</code></pre> <p>The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend reserving at least 20GB of disk space for the LLVM build.</p>"},{"location":"Install/Building_on_Ubuntu/#configure-cmake","title":"Configure CMake","text":"<pre><code>cd &lt;path_to_accera&gt;\nmkdir build\ncd build\n\ncmake .. -DCMAKE_BUILD_TYPE=Release -G Ninja\n</code></pre>"},{"location":"Install/Building_on_Ubuntu/#build-and-run-tests","title":"Build and run tests","text":"<pre><code>cmake --build . --config Release\nctest -C Release\n</code></pre>"},{"location":"Install/Building_on_Ubuntu/#install","title":"Install","text":"<pre><code>cmake --build . --config Release --target install\n</code></pre>"},{"location":"Install/Building_on_Windows/","title":"Building on Windows","text":""},{"location":"Install/Building_on_Windows/#installing-on-windows","title":"Installing on Windows","text":""},{"location":"Install/Building_on_Windows/#install-dependencies","title":"Install Dependencies","text":""},{"location":"Install/Building_on_Windows/#visual-studio","title":"Visual Studio","text":"<p>Accera requires a C++ compiler that supports C++ 17. You can download Visual Studio 2019 Enterprise Edition or Visual Studio 2022 Community Edition. Install Update 10 or later, which includes the LLVM OpenMP libraries only for VS 2019.</p> <p>Select Desktop Development with C++.</p> <p>Accera requires Spectre-mitigated libraries:</p> <ol> <li>Go to Individual Components</li> <li>Type in \"Spectre\" in the search box</li> <li>Select the latest version of the MSVC libraries, e.g., MSVC v142 - VS 2019 C++ x64/x86 Spectre-mitigated libs (Latest) (your actual version may vary)</li> </ol>"},{"location":"Install/Building_on_Windows/#cmake","title":"CMake","text":"<p>Accera requires CMake 3.14 or newer. A version of CMake that satisfies this requirement is included with Visual Studio 2019 and Visual Studio 2022.</p>"},{"location":"Install/Building_on_Windows/#python","title":"Python","text":"<p>Accera's packages require Python 3.7 64-bit or newer, plus a version of <code>pip</code> that supports 64-bit packages (<code>win_amd64</code>). One way to obtain this is to download and install Miniconda. Download \"Miniconda3 Windows 64-bit\".</p>"},{"location":"Install/Building_on_Windows/#optional-create-a-conda-environment","title":"Optional: Create a conda environment","text":"<p>After installing Miniconda, you can optionally create an environment to manage different Python versions.</p> <p>From an \"Anaconda Prompt\", create and then activate an environment for Python 3.7 (or a newer version if you prefer). Make sure to activate an environment from other applications that you use to develop Accera.</p> <pre><code>conda create -n py37 python=3.7\nconda activate py37\n</code></pre>"},{"location":"Install/Building_on_Windows/#clone-accera","title":"Clone Accera","text":"<p>Visual Studio 2019 and 2022 include a version of <code>git</code>. To use it, launch Visual Studio 2019 or 2022, and select <code>Clone a repository</code>.</p> <p>Repository location:</p> <pre><code>https://github.com/microsoft/Accera\n</code></pre>"},{"location":"Install/Building_on_Windows/#build-and-install-accera","title":"Build and install Accera","text":"<p>From a command line with Python in your PATH, such as an Anaconda Command Prompt, setup the Visual Studio command line environment (<code>vcvars64.bat</code>) and then run <code>build.bat</code> to generate the Accera Python packages. </p> <p>For Visual Studio 2022: <pre><code>\"%ProgramFiles%\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\n</code></pre></p> <p>For Visual Studio 2019: <pre><code>\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\n</code></pre></p> <pre><code>cd &lt;path_to_accera&gt;\nbuild.bat\n</code></pre> <p>Replace <code>&lt;path_to_accera&gt;</code> with the path to the cloned Accera repository.</p> <p>Update or install the resulting <code>.whl</code> file from the <code>dist</code> subdirectory. The <code>--find-links</code> option tells pip to look at the <code>dist</code> subdirectory for the dependent packages. The whl filename depends on your Python version, your OS, and your CPU architecture.</p> <pre><code>pip install -U dist\\accera-0.0.1-cp37-cp37m-win_amd64.whl --find-links=dist\n</code></pre>"},{"location":"Install/Building_on_Windows/#build-and-install-using-cmake","title":"Build and install using CMake","text":"<p>Accera can also be built using CMake (intended for expert users).</p>"},{"location":"Install/Building_on_Windows/#install-dependencies_1","title":"Install dependencies","text":"<pre><code>cd &lt;path_to_accera&gt;\ngit submodule init\ngit submodule update\nexternal\\vcpkg\\bootstrap-vcpkg.bat\nexternal\\vcpkg\\vcpkg install catch2:x64-windows tomlplusplus:x64-windows accera-llvm:x64-windows --overlay-ports=external\\llvm\n</code></pre> <p>The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend reserving at least 20GB of disk space for the LLVM build.</p>"},{"location":"Install/Building_on_Windows/#configure-cmake","title":"Configure CMake","text":"<pre><code>cd &lt;path_to_accera&gt;\nmkdir build\ncd build\n\n# For Visual Studio 2019:\ncmake .. -DCMAKE_BUILD_TYPE=Release -G\"Visual Studio 16 2019\" -Ax64\n\n# For Visual Studio 2022:\ncmake .. -DCMAKE_BUILD_TYPE=Release -G\"Visual Studio 17 2022\" -Ax64\n</code></pre>"},{"location":"Install/Building_on_Windows/#build-and-run-tests","title":"Build and run tests","text":"<pre><code>cmake --build . --config Release -- /m\nctest -C Release\n</code></pre>"},{"location":"Install/Building_on_Windows/#install","title":"Install","text":"<pre><code>cmake --build . --config Release --target install -- /m\n</code></pre>"},{"location":"Install/Installing_Accera_on_MacOS/","title":"Installing Accera on MacOS","text":""},{"location":"Install/Installing_Accera_on_MacOS/#installing-on-macos","title":"Installing on MacOS","text":""},{"location":"Install/Installing_Accera_on_MacOS/#install-dependencies","title":"Install dependencies","text":"<p>Accera requires the following tools and libraries for building the generated code:</p> <ul> <li>A C++ compiler, such as <code>clang</code>, which is bundled in XCode</li> <li>Python 3.7 or newer</li> <li>OpenMP 5, if using parallelization</li> </ul> <p>Homebrew is a package manager that makes it easy to install the prerequisites. Homebrew can be downloaded and installed by:</p> <pre><code>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n</code></pre> <p>If you already have Homebrew installed, update it to the latest version by typing:</p> <pre><code>brew update\n</code></pre> <p>Install the dependencies:</p> <pre><code>brew install cmake python@3.7\n</code></pre> <p>Install the optional dependency if using parallelization:</p> <pre><code>brew install libomp\n</code></pre>"},{"location":"Install/Installing_Accera_on_MacOS/#clang","title":"Clang","text":"<p>Select the <code>clang</code> compiler from XCode:</p> <pre><code>xcode-select --install\n</code></pre>"},{"location":"Install/Installing_Accera_on_MacOS/#install-accera","title":"Install Accera","text":"<p>The <code>accera</code> Python package can be installed from PyPI:</p> <pre><code>pip install accera\n</code></pre>"},{"location":"Install/Installing_Accera_on_Ubuntu/","title":"Installing Accera on Ubuntu","text":""},{"location":"Install/Installing_Accera_on_Ubuntu/#installing-on-ubuntu","title":"Installing on Ubuntu","text":""},{"location":"Install/Installing_Accera_on_Ubuntu/#install-dependencies","title":"Install dependencies","text":"<p>Accera requires the following tools and libraries for building the generated code:</p> <ul> <li>A C++ compiler, such as GCC 8</li> <li>Python 3.7 or newer</li> <li>OpenMP 5, if using parallelization</li> </ul> <p>Ubuntu 20.04 is recommended. A quick way to start is to use a new Docker container for Ubuntu 20.04:</p> <pre><code>docker run -v $PWD:/code -it --entrypoint \"/bin/bash\" ubuntu:focal\n</code></pre> <p>Install Accera's dependencies:</p> <pre><code>apt update\napt-get install gcc-8 g++-8 python3 python3-pip libncurses5\n</code></pre> <p>Install the optional dependency if using parallelization:</p> <pre><code>apt-get install libomp-11-dev\n</code></pre>"},{"location":"Install/Installing_Accera_on_Ubuntu/#install-accera","title":"Install Accera","text":"<p>The <code>accera</code> Python package can be installed from PyPI:</p> <pre><code>pip install accera\n</code></pre>"},{"location":"Install/Installing_Accera_on_Windows/","title":"Installing Accera on Windows","text":""},{"location":"Install/Installing_Accera_on_Windows/#installing-on-windows","title":"Installing on Windows","text":""},{"location":"Install/Installing_Accera_on_Windows/#install-dependencies","title":"Install dependencies","text":""},{"location":"Install/Installing_Accera_on_Windows/#visual-studio","title":"Visual Studio","text":"<p>Accera's generated code requires a C++ compiler. Download Visual Studio 2019 Enterprise Edition or Visual Studio 2022 Community Edition, and select Desktop development with C++ during installation.</p> <p>If you've selected VS 2019 and would like to use parallelization, ensure that Update 10 or later is installed. Both VS 2019 Update 10 or later and VS 2022 include the LLVM OpenMP libraries.</p>"},{"location":"Install/Installing_Accera_on_Windows/#python","title":"Python","text":"<p>Accera's packages require Python 3.7 64-bit or newer, plus a version of <code>pip</code> that supports 64-bit packages (<code>win_amd64</code>). One way to obtain this is to download and install Miniconda. Download \"Miniconda3 Windows 64-bit\".</p>"},{"location":"Install/Installing_Accera_on_Windows/#optional-create-a-conda-environment","title":"Optional: Create a conda environment","text":"<p>After installing Miniconda, you can optionally create an environment to manage different Python versions.</p> <p>From an \"Anaconda Prompt\", create and then activate an environment for Python 3.7 (or a newer version if you prefer):</p> <pre><code>conda create -n py37 python=3.7\nconda activate py37\n</code></pre>"},{"location":"Install/Installing_Accera_on_Windows/#install-accera","title":"Install Accera","text":"<p>The <code>accera</code> Python package can be installed from PyPI:</p> <pre><code>pip install accera\n</code></pre>"},{"location":"Manual/","title":"Accera v1.2 Manual","text":"<ul> <li>Introduction</li> <li>Arrays and Scalars</li> <li>Simple Affine Loop Nests</li> <li>Schedules</li> <li>Fusing</li> <li>Targets</li> <li>Plans - Caching</li> <li>Plans - Vectorization and Parallelization</li> <li>Deferred layout of constant arrays</li> <li>Parameters</li> <li>Packages</li> </ul>"},{"location":"Manual/00%20Introduction/","title":"Introduction","text":"<p>Accera is a framework with a Python-based Domain-specific Language (eDSL) that produces optimized compute-intensive code. Accera's primary focus is the optimization of affine and semi-affine nested for-loops for CPU and GPU targets.</p> <p>Optimization of compute-intensive code in a traditional programming language is not only challenging and time-consuming, but manual optimization of the simplest numerical algorithms demands significant engineering effort and requires an advanced understanding of computer architecture and fluency in C++, C, or Assembly Language. Even with all these efforts, implemented code is prone to critical bugs and requires extensive engineering effort for maintenance. Accera aims at resolving all these issues by providing optimized solutions for compute-intensive algorithms that are highly efficient, readable, and maintainable. </p> <p>Accera has THREE primary goals:</p> <ul> <li>Performance: To generate the fastest implementation for any compute-intensive algorithm.</li> <li>Readability: To ensure effective implementation of algorithms without sacrificing the readability of code.</li> <li>Writability: To provide a user-friendly programming model designed for agility and maintainability.</li> </ul> <p>Accera is designed based on the following guiding principles: </p>"},{"location":"Manual/00%20Introduction/#1-strict-separation-of-logic-from-implementation","title":"1: Strict separation of logic from implementation","text":"<p>Traditional programming languages are prone to the tight coupling of code logic (what the program does) with its implementation (how the program is implemented). Consider an example of multiplying a 16\u00d711 matrix A by an 11\u00d710 matrix B. The algorithm's logic calculates the sum over k of A[i,k]\u00b7B[k,j] for each value of i and j. In Python, this logic can be expressed as: <pre><code># C += A @ B\nfor i in range(16):\n    for j in range(10):\n        for k in range(11):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre> The above code expresses more than just the logic of matrix multiplication. It insists on a specific execution flow: first perform all the steps required to calculate <code>C(0,0)</code> in ascending order of <code>k</code>; then proceed to <code>C(0,1)</code>. However, in principle, a single order of execution should not be imposed because the iterations of this loop can be performed in any order while keeping the logic intact. Moreover, the above logic doesn't utilize important optimization techniques, such as double-buffered caching or vectorization.</p> <p>Accera, on the other hand, provides a strict distinction between logic and its implementation. The programmer first implements the logic without performance considerations using a pseudocode-like syntax independent of the target platform. Once the logic is specified, only then does the programmer move to define the concrete implementation details. </p>"},{"location":"Manual/00%20Introduction/#2-mindfully-trade-off-safety-versus-expressivity","title":"2: Mindfully trade-off safety versus expressivity","text":"<p>Accera offers a programming model where a default implementation of the specified logic can be transformed and manipulated in different ways. If used correctly, these transformations are safe, which means that the underlying logic remains intact. This allows the programmer to entirely focus on the performance of the logic without worrying about its correctness. Moreover, these safe transformations allow automatic search algorithms to aggressively search the space of transformations to converge faster and find better optima. </p> <p>Traditionally, this safety is achieved by trading off the true potential of a programming language since it demands restricting its scope. Nevertheless, extensive constraints significantly restrict the expressivity and the power of the programming language, eventually preventing the end-users from developing highly-optimized and sophisticated implementations. </p> <p>Accera moderates this trade-off between safety and expressivity by explicitly defining what level of safety guarantees are being given by each transformation under different circumstances. Some situations are safer than others. However, the programmer knows exactly what safeties are being guaranteed in all cases. </p>"},{"location":"Manual/00%20Introduction/#3-the-programmer-is-in-control","title":"3: The programmer is in control","text":"<p>Accera gives the programmer maximum control over the generated logic by providing access to the underlying knobs that determine how algorithms are optimized. Convenience methods and carefully used default values can prevent verbosity. As per the use case, these helper methods can always be tuned, even overridden. </p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/","title":"Section 1: Arrays and Scalars","text":""},{"location":"Manual/01%20Arrays%20and%20Scalars/#arrays","title":"Arrays","text":"<p>Accera stores data in multi-dimensional arrays of scalar elements where all the array elements share the same primary data type (e.g., float32, int8). An array has a constant number of dimensions d known at compile-time (e.g., a matrix is a 2-dimensional array). Each dimension has a positive size, and the sequence of d sizes is called the shape of the array. An element of an array is referred to by a d-coordinate zero-based index vector.</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#affine-memory-layout","title":"Affine memory layout","text":"<p>Arrays are multi-dimensional, while computer memories have a linear (one-dimensional) address space. There are many strategies to represent a multi-dimensional array in one-dimensional computer memory. Accera arrays must have an affine memory layout, where each array has an affine memory map that is a d-dimensional vector denoted by a and a memory offset value denoted by o. The array element that corresponds to the index vector i is stored at memory address i\u00b7 a+o (where i\u00b7 a denotes a vector dot product).</p> <p>Affine memory maps are rich enough to represent many standard array layouts. For example, in affine maps, 2-dimensional arrays (matrices) can be represented as row-major, column-major, triangular, banded, and Toeplitz matrices. However, affine maps cannot represent z-ordering or striped or blocked layouts.</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#array-shape","title":"Array shape","text":"<p>In an affine memory map, each dimension corresponds to an element, where the dimension having the largest absolute value of the element is called the major dimension. The user must specify all dimension sizes except for the major dimension when constructing an Array. Accera assumes that the size is arbitrary (or infinite) if the major dimension is not specified. In other words, the iterations of the loops determine how much of the array is visited along this dimension. </p> <p>For example, a row-major matrix must have a compile-time-constant number of columns. However, the number of rows can be left undefined, and the loops' sizes control how many rows are processed.</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#compile-time-and-runtime-dimension-sizes","title":"Compile-time and runtime dimension sizes","text":"<p>The number of dimensions of Accera arrays are known at compile-time. However, the user can choose to specify the sizes of each dimension at compile-time or at runtime. Runtime dimension sizes are only resolved at runtime, typically as inputs to an Accera function.</p> <p>For example, a function that implements generalized matrix multiply can receive the <code>M</code>, <code>N</code>, <code>K</code> dimension sizes as inputs along with the <code>M</code> \u00d7 <code>N</code>, <code>M</code> \u00d7 <code>K</code>, and <code>N</code> \u00d7 <code>K</code> Arrays.</p> <p>Furthermore, an Array can have a mixture of compile-time and runtime dimension sizes.</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#default-and-inferred-memory-layout","title":"Default and inferred memory layout","text":"<p>Although the user can explicitly specify the memory map, Accera offers some conveniences. The user can set the layout as <code>FIRST_MAJOR</code> (e.g., for two-dimensional arrays, first-major is equivalent to row-major) or <code>LAST_MAJOR</code>. In both cases, the affine map is inferred from the array shape. Specifically, if the layout is <code>LAST_MAJOR</code> and the shape is denoted by the vector s, then the map a is set to [1, s0, s0\u00d7s1, s0\u00d7s1\u00d7s2, ...]. If the layout is <code>FIRST_MAJOR</code> and the dimension equals 4, then a is set to [s0\u00d7s1\u00d7s2, s1\u00d7s2, s2, 1]. In both cases, the size of the major dimension is not used in the definition of a. This indicates that the major dimension size is not needed. If no layout is specified, the default layout is <code>FIRST_MAJOR</code>.</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#array-properties","title":"Array properties","text":"<p>Accera arrays are defined with either internal scope or external scope. An internal array is a private array that exists inside a specific Accera function only and cannot be accessed outside of that function. An external array is defined outside of an Accera function and passed in as an argument. The memory layout of an external array is specified as a part of the Accera function signature. Moreover, external arrays are assumed to be disjoint, i.e., they do not share any memory.</p> <p>Accera arrays are either mutable or immutable. The elements of a mutable array can be set by an Accera function, while an immutable array is read-only.</p> <p>Array properties are not explicitly set by the user but are implied by the role of the array (see below).</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#array-roles","title":"Array roles","text":"<p>Accera supports the following four array roles where each role is treated differently. </p> <ul> <li>Input</li> <li>Input/Output</li> <li>Output</li> <li>Constant</li> <li>Temporary </li> </ul>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#input-arrays","title":"Input arrays","text":"<p>Input arrays are immutable external arrays whose element type, shape, and affine layout can be known at compile-time. However, their contents are only available at runtime. If the Accera function is emitted as a function in C, each input array is passed as a const pointer argument. For example, we can construct a 10\u00d720 input array of 32-bit floating-point numbers by writing <pre><code>import accera as acc\n\nA = acc.Array(shape=(10, 20), role=acc.Role.INPUT, element_type=acc.ScalarType.float32)\n</code></pre> The layout of this array would be the default layout, which is <code>acc.Array.Layout.FIRST_MAJOR</code>.</p> <p>The shape (and similarly, the layout) of Input arrays can also be set at runtime:</p> <pre><code>N = acc.create_dimensions()\nA = acc.Array(shape=(N, 20), role=acc.Role.INPUT, element_type=acc.ScalarType.float32)\n</code></pre>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#inputoutput-arrays","title":"Input/output arrays","text":"<p>Input/Output arrays are similar to the input arrays except that they are mutable external arrays, i.e., their values can be changed. This type of array is used to output the results of the loop-nest computation. If the Accera function is emitted as a function in C, each input array is passed as a non-const pointer argument.</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#output-arrays","title":"Output arrays","text":"<p>Output arrays are variable-shaped mutable external arrays whose shapes and affine layout are known at runtime. The key differences with Input/Output arrays are:</p> <ul> <li>Output arrays are dynamically allocated at runtime. The caller of an Accera function that uses Output arrays will need to implement the <code>__accera_allocate</code> function to allocate memory (and also perform the subsequent deallocation).</li> <li>Output arrays are uninitialized by default. Accera will produce an error if operators such as <code>+=</code> are used on an Output array without prior initialization through assignment.</li> <li>For simplicity, output dimensions (<code>acc.Role.OUTPUT</code>) must be used for specifying an Output array shape or layout (this limitation may be lifted in the future).</li> </ul> <p>Output arrays are useful for operations that adjust the array shape depending on the input values. For example, the Range operation generates variable output sizes based on the start, end, and step inputs:</p> <pre><code>import accera as acc\n\n# inputs\nStart = acc.Scalar()\nEnd = acc.Scalar()\nStep = acc.Scalar()\n\n# compute the variable output size\nN = acc.create_dimensions(role=acc.Role.OUTPUT)\nN.value = acc.floor((End - Start) / Step)\n\n# create an Output array with the variable output size\nA = acc.Array(shape=(N, ), role=acc.Role.OUTPUT, element_type=acc.ScalarType.float32)\n</code></pre> <p>The layout of this array is the default layout, which is <code>acc.Array.Layout.FIRST_MAJOR</code>.</p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#constant-arrays","title":"Constant arrays","text":"<p>These are the only Accera arrays whose contents are known at compile-time. Constant arrays are immutable internal arrays whose memory layout can be chosen automatically without any external constraints since they are internally scoped. For example, a constant array can be automatically laid out according to the loop nest's memory access pattern. The layout of a constant array could even depend on its contents (e.g., its sparsity pattern). The dimension sizes of a constant array must be known at compile-time.</p> <p>We must provide the constant array data (the element values) when constructing it. This data can be any Python buffer or a numpy array: <pre><code>import accera as acc\nimport numpy as np\n\nmatrix = np.random.rand(16, 16)\nB = acc.Array(role=acc.Role.CONST, data=matrix)\n</code></pre></p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#temporary-arrays","title":"Temporary arrays","text":"<p>Temporary arrays are mutable internal arrays that are used when two Accera schedules are fused into one (more on fusing in Section 4). The elements of a temporary array are initialized to zeros and used to store intermediate values. Similar to constant arrays, temporary arrays can be laid out arbitrarily. In fact, the Accera compiler can even choose not to store them in physical memory at all. </p>"},{"location":"Manual/01%20Arrays%20and%20Scalars/#scalars","title":"Scalars","text":"<p>A scalar represents a single number whose value is mutable and set at runtime. Scalars are useful as input arguments to functions or when computing a single-valued numeric result.</p> <p>Section 2 lists the operations can be performed on scalars.</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/","title":"Section 2: Simple affine loop nests","text":"<p>This section introduces loop nests and their different types that are provided in Accera programming model.</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#affine-loop-nests","title":"Affine loop nests","text":"<p>Many important compute-intensive workloads can be expressed using nested for-loops. An algorithm that can be defined using nested for-loops is called a loop nest. Accera only supports the class of affine loop nests. A loop nest is affine if the indices of the elements accessed on each iteration are an affine function of the loop iterator variables. For example, the following loop nest is affine: <pre><code>for i in range(M):\n    for j in range(N):\n        C[2*i+2, j+2] += A[3*i, j] + B[j, i]\n</code></pre> because <code>2*i+2</code>, <code>j+2</code>, <code>3*i</code>, <code>j</code> and <code>i</code> are all affine functions of the iterator variables <code>i</code> and <code>j</code>.</p> <p>On the other hand, the following loop nest is not affine: <pre><code>for i in range(M):\n    for j in range(N):\n        C[i*i, j] += A[i*i, j] + B[i*j, i]\n</code></pre> because <code>i*i</code> and <code>i*j</code> are quadratic (non-affine) functions of <code>i</code> and <code>j</code>.</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#simple-affine-loops-nests-aka-simple-nests","title":"Simple affine loops nests, a.k.a. simple nests","text":"<p>Simple Affine Loop Nests, hereinafter referred to as simple nests, is an important subclass of affine loop nests that satisfies the following properties: 1. The loops are perfectly nested: all the computation is entirely contained within the deepest loop. 2. All the loops are normalized: each loop starts at 0, increments by 1, and ends at a compile-time constant size. 3. The loop iterations are order invariant: the logic doesn't change if the loop iterations are executed in a different sequential order. 4. No conditional exit: the loop doesn't contain break or continue commands.</p> <p>The matrix-matrix multiplication example given in the introduction is an example of a simple nest. Another example is 2-dimensional convolution, which is the fundamental operation in convolutional neural networks, and can be written in Python as: <pre><code># Convolve M x N data matrix A with S x T filter matrix B and add output to matrix C\nfor i in range(M):\n    for j in range(N):\n        for k in range(S):\n            for l in range(T):\n                C[i, j] += A[i + k, j + l] * B[k, l]\n</code></pre></p> <p>While Accera supports arbitrary affine loop nests, the programmer defines the logic of their algorithms using simple nests. More complex nests are obtained by applying schedule transformations (see Section 3) or by fusing multiple schedules (see Section 4).</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#defining-the-loop-nest-logic","title":"Defining the loop nest logic","text":"<p>The programmer's goal is to create a highly optimized target-specific implementation of an affine loop nest. The first step towards this goal is to define the logic of one or more simple nests. The logic is a target-independent pseudo-code of a simple nest, written without considering performance. For example, the following code defines the logic of the matrix-matrix multiplication loop nest:</p> <p><pre><code># Import accera\nimport accera as acc\n\n# Define matrix sizes\nM = 16\nN = 10\nS = 11\n\nA = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, S))\nB = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(S, N))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n\n# Define a simple affine loop nest and name its loops i, j, k\nnest = acc.Nest(shape=(M, N, S))\ni, j, k = nest.get_indices()\n\n# Define the logic of each iteration in the nest\n@nest.iteration_logic\ndef _():\n    C[i,j] += A[i,k] * B[k,j]\n</code></pre> We start by defining the arrays that participate in the computation: <code>A</code> and <code>B</code> are input arrays and <code>C</code> is an input/output array. Next, we initialize <code>nest</code> to be an empty skeleton of a loop nest, with nested loops of sizes <code>M</code>, <code>N</code>, <code>S</code>. These loops are logical -- think of them as pseudo-code loops -- they do not define the execution order of the iterations. The index variables that correspond to the three loops are named <code>i, j, k</code> respectively.</p> <p>The last part of the example sets the iteration logic to <code>C[i, j] += A[i, k] * B[k, j]</code>. Note that this iteration logic follows an affine memory access pattern. The syntax in the example makes use of Python decorators and is shorthand for the more explicit syntax: <pre><code>def logic_fn():\n    C[i, j] += A[i, k] * B[k, j]\n\nnest.iteration_logic(logic_fn)\n</code></pre></p> <p>The iteration spaces above have compile-time shapes. We can define runtime shapes by replacing any or all of the constant matrix sizes <code>M</code>, <code>N</code>, and <code>S</code> with an <code>acc.Dimension</code> placeholder:</p> <pre><code>M = acc.create_dimensions() # replace M with a runtime dimension\nN = 10 # a compile-time dimension\nS = 11\n\nA = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, S))\nB = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(S, N))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n\n# Define a simple affine loop nest and name its loops i, j, k\nnest = acc.Nest(shape=(M, N, S))\n</code></pre> <p>The iteration space dimensions will now be runtime variables that need to be provided to the function (more on this later).</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#supported-operations","title":"Supported operations","text":"<p>The iteration logic can include the following operations (assuming <code>accera</code> was imported as <code>acc</code>):</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#assignment-operators","title":"Assignment operators","text":"Operation Types (Operands must be of same type) Description <code>a = b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Assigns the value of scalar b to scalar a"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#arithmetic-operators","title":"Arithmetic operators","text":"Operation Types (Operands must be of same type) Description <code>a + b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the sum of scalars a and b <code>a - b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the difference between scalars a and b <code>a * b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the product of scalars a and b <code>a / b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the quotient of scalars a and b. If the operands are integers, an integer division result is returned <code>a ** b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the b'th power of scalar a <code>a // b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the floor of the quotient of scalars a and b <code>a % b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the signed remainder after dividing scalar a by scalar b <code>-a</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the additive inverse of scalar a <p>Comment: Accera also supports the corresponding compound-assignment operators, such as <code>a += b</code>, <code>a -= b</code>, etc.</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#relational-operators","title":"Relational operators","text":"Operation Types (Operands must be of same type) Description <code>a == b</code> <code>acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if scalar a equals scalar b, else False <code>a != b</code> <code>acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if scalar a is not equal to scalar b, else False <code>a &lt; b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if scalar a is strictly smaller than scalar b, else False <code>a &lt;= b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if scalar a is smaller than or equal to scalar b, else False <code>a &gt; b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if scalar a is strictly greater than scalar b, else False <code>a &gt;= b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if scalar a is greater than or equal to scalar b, else False"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#logical-operators","title":"Logical operators","text":"Operation Types (Operands must be of same type) Description <code>acc.logical_and(a, b)</code> <code>acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if scalars a and b are non-zero, else False <code>acc.logical_or(a, b)</code> <code>acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if either scalar a or scalar b are non-zero, else False <code>acc.logical_not(a)</code> <code>acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if a is zero, else False"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#bitwise-operators","title":"Bitwise operators","text":"Operation Types (Operands must be of same type) Description <code>a &amp; b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise AND of the bits in scalars a and b <code>a \\| b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise OR of the bits in scalars a and b <code>a ^ b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise XOR of the bits in scalars a and b <code>~a</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise inverse of the bits in scalar a <code>a &lt;&lt; b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns scalar a whose bitwise representation is shifted left by b bits <code>a &gt;&gt; b</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns scalar a whose bitwise representation is shifted right by b bits <p>Comment: Accera also supports the corresponding compound-assignment operators, such as <code>a &amp;= b</code>, <code>a |= b</code>, etc.</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#intrinsics","title":"Intrinsics","text":"Operation Types (Operands must be of same type) Description <code>acc.abs(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the absolute value of scalar a <code>acc.max(a, b)</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the larger of the two scalars a and b <code>acc.min(a, b)</code> <code>acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the smaller of the two scalars a and b <code>acc.ceil(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the value of scalar a rounded up to the nearest integer as an int64 type <code>acc.floor(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the value of scalar a rounded down to the nearest integer as an int64 type <code>acc.sqrt(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the square root of scalar a <code>acc.exp(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the exponential e raised to the scalar a <code>acc.log(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the natural logarithm (base e) of scalar a <code>acc.log10(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the common logarithm (base 10) of scalar a <code>acc.log2(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the binary logarithm (base 2) of scalar a <code>acc.sin(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the sine of scalar a, where a is in radians <code>acc.cos(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the cosine of scalar a, where a is in radians <code>acc.tan(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the tangent of scalar a, where a is in radians <code>acc.sinh(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the hyperbolic sine of scalar a, where a is in radians <code>acc.cosh(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the hyperbolic cosine of scalar a, where a is in radians <code>acc.tanh(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the hyperbolic tangent of scalar a, where a is in radians"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#implicit-type-casting","title":"Implicit type casting","text":"<p>Accera operators require operands to be the same type. Computations that use multiple types can take advantage of Accera's implicit type casting support when converting from smaller-sized types to larger-sized types.</p> <p>To do implicit casting, simply assign a source type to its implicitly-castable destination type. No additional casting operation is needed for converting between these types.</p> Source types Destination type (implicitly-castable) <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.uint8</code> <code>acc.ScalarType.int8</code> <code>acc.ScalarType.bool</code>,  <code>acc.ScalarType.int8</code> <code>acc.ScalarType.uint8</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.uint16</code> <code>acc.ScalarType.int16</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code> <code>acc.ScalarType.uint16</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code>, <code>acc.ScalarType.uint32</code> <code>acc.ScalarType.int32</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code>, <code>acc.ScalarType.int32</code> <code>acc.ScalarType.uint32</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code>, <code>acc.ScalarType.int32</code>, <code>acc.ScalarType.uint32</code>, <code>acc.ScalarType.uint64</code> <code>acc.ScalarType.int64</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code>, <code>acc.ScalarType.int32</code>, <code>acc.ScalarType.uint32</code>, <code>acc.ScalarType.int64</code> <code>acc.ScalarType.uint64</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code> <code>acc.ScalarType.float16</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code> <code>acc.ScalarType.bfloat16</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code>, <code>acc.ScalarType.int32</code>, <code>acc.ScalarType.uint32</code>, <code>acc.ScalarType.int64</code>, <code>acc.ScalarType.float16</code>, <code>acc.ScalarType.bfloat16</code> <code>acc.ScalarType.float32</code> <code>acc.ScalarType.bool</code>, <code>acc.ScalarType.int8</code>, <code>acc.ScalarType.uint8</code>, <code>acc.ScalarType.int16</code>, <code>acc.ScalarType.uint16</code>, <code>acc.ScalarType.int32</code>, <code>acc.ScalarType.uint32</code>, <code>acc.ScalarType.int64</code>, <code>acc.ScalarType.float16</code>, <code>acc.ScalarType.bfloat16</code>, <code>acc.ScalarType.float32</code> <code>acc.ScalarType.float64</code> <p>To override the casting behavior above, or cast a larger-sized type to a smaller-sized type, use the <code>acc.cast</code> operation.</p> <p>Comment: implicit casting of constants may result in truncation.</p>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#accera-program-stages","title":"Accera program stages","text":"<p>Let\u2019s take a step back to describe the stages of Accera program:</p> <ul> <li><code>Nest</code>: A nest captures the logic of a simple nest, without any optimizations or implementation details.</li> <li><code>Schedule</code>: A <code>Nest</code> is used to create a schedule. The schedule controls the order in which the nest iterations are visited. Multiple schedules can be fused into a single schedule, which may no longer represent a simple nest.</li> <li><code>Plan</code>: A <code>Schedule</code> is used to create a plan. A plan controls the implementation details that are specific for a target platform (e.g., data caching strategy, vectorization, assignment of arrays and caches to different types of memory).</li> <li><code>Package</code>: A <code>Plan</code> is used to create a function in a function package. The package is then compiled and emitted.</li> </ul> <p>Once a package is emitted, the Accera functions contained in it can be called from external client code. This external code is typically not written using Accera.</p> <p>Accera currently supports the following package formats:</p> <ul> <li>HAT, which is a schematized version of a standard C library. The external client code can be written in C or C++ and linked with the HAT package.</li> <li>MLIR, which uses standard MLIR dialects. The external code must also be in MLIR.</li> </ul> <p>Overall, to build and emit <code>nest</code> (defined above), we would write:</p> <pre><code># create a default schedule from the nest\nschedule = nest.create_schedule()\n\n# create a default plan from the schedule\nplan = schedule.create_plan()\n\n# create a HAT package. Create a function in the package based on the plan\npackage = acc.Package()\npackage.add(plan, args=(A, B, C), base_name=\"simple_matmul\")\n\n# build the HAT package\npackage.build(format=acc.Package.Format.HAT_DYNAMIC, name=\"linear_algebra\")\n</code></pre> <p>It may not be immediately clear why so many stages are needed just to compile a simple nest. Therefore, let\u2019s discuss each stage in detail to understand their importance. </p> <p>In the example above, the call to <code>package.add</code> takes three arguments: the first is the plan that defines the function's implementation; the second is the order of the input and input/output arrays in the function signature; and the third is a base name for the function. The full name of the function is the base name followed by an automatically-generated unique identifier. For example, the function in the example could appear in the package as <code>simple_matmul_8f24bef5</code>. The automatically-generated suffix ensures that each function in the package has a unique name. More details on function packages can be found in Section 10.</p> <p>The Array shapes above are known at compile-time. If one or all of the shapes are known at runtime, we provide dimensions as arguments to the function:</p> <pre><code>M, N, S = acc.create_dimensions() # runtime dimensions\n\nA = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, S))\nB = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(S, N))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n\n...\n\n# create a default schedule from the nest\nschedule = nest.create_schedule()\n\n# create a default plan from the schedule\nplan = schedule.create_plan()\n\n# create a HAT package. Create a function in the package based on the plan, with\n# the dimensions as additional arguments (in any order)\npackage = acc.Package()\npackage.add(plan, args=(M, N, S, A, B, C), base_name=\"simple_matmul_runtime_shapes\")\n\n# build the HAT package\npackage.build(format=acc.Package.Format.HAT_DYNAMIC, name=\"linear_algebra\")\n</code></pre>"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#convenience-syntax","title":"Convenience syntax","text":"<p>For convenience, Accera also provides shortcuts to avoid unnecessary verbosity. Specifically, we can create a function in a package directly from a nest, as follows: <pre><code>package.add(nest, args=(A, B, C), base_name=\"simple_matmul\")\n</code></pre> The abbreviated syntax makes it seem like a callable function is generated directly from <code>nest</code>. However, what actually happens behind the scenes is that <code>nest</code> creates a default schedule, which creates a default plan, which is added as a function in the package. Accera has a similar convenience syntax to create a function from a schedule: <pre><code>package.add(schedule, args=(A, B, C), base_name=\"simple_matmul\")\n</code></pre> and to create a plan directly from a nest: <pre><code>plan = nest.create_plan()\n</code></pre></p>"},{"location":"Manual/03%20Schedules/","title":"Section 3: Schedules","text":"<p>We begin with <code>nest</code> from Section 2 which captures the logic of matrix-matrix multiplication. We use <code>nest</code> to create a <code>Schedule</code> that controls the execution order of the nest's iterations. Schedules are target-independent in the sense that the same schedule can be used to emit code for multiple target platforms.</p> <p>We create a default schedule as follows: <pre><code>schedule = nest.create_schedule()\n</code></pre></p> <p>The default schedule is equivalent to the following straightforward for-loop version of the loop nest: <pre><code>for i in range(3):\n    for j in range(12):\n        for k in range(15):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre> In other words, each of the logical pseudo-code loops in <code>nest</code> becomes an actual for-loop in the default schedule. The for-loop sizes can be known at compile-time or at runtime.</p> <p>We can now transform this schedule in various ways. However, these transformations do not change the underlying logic defined in <code>nest</code> and merely change the order of the loop iterations. We can even generate as many independent schedules as we want by calling <code>nest.create_schedule()</code>.</p>"},{"location":"Manual/03%20Schedules/#iteration-spaces-a-geometric-representation-of-schedules","title":"Iteration spaces: a geometric representation of schedules","text":"<p>In the Accera programming model, a schedule is geometrically interpreted as a multi-dimensional discrete hypercube called the iteration space of the nest. The elements of the iteration space represent the individual iterations of the loop nest. Initially, the dimensions of the iteration space correspond to the logical loops defined in <code>nest</code>. </p> <p>For example, the default iteration space for the matrix-matrix multiplication nest forms a three-dimensional discrete hypercube, whose shape is (3, 12, 15):</p> <p></p> <p>The (3, 12, 15) iteration space. The arrows labelled 1, 2, and 3 indicate the dimension order and direction.</p>"},{"location":"Manual/03%20Schedules/#how-does-an-iteration-space-imply-an-order-over-the-iterations","title":"How does an iteration space imply an order over the iterations?","text":"<p>The dimensions of the iteration space are ordered, and this order corresponds to the original order of the logical loops in <code>nest</code> by default. In fact, the order over the dimensions induces a lexicographic sequence over the individual elements of the iteration space.</p> <p></p> <p>Video showing sequence of iterations for the (3, 12, 15) iteration space.</p> <p>This geometric interpretation of schedules helps us visualize how different transformations modify them. While some transformations merely rearrange the elements of the iteration space, others increase its dimensions, and some even pad the space with empty (no-op) elements. The transformed iteration space defines a new lexicographic order over the individual iterations.</p> <p>Comment: It is important not to confuse arrays, like <code>A</code>, <code>B</code>, <code>C</code>, with iteration spaces, like <code>schedule</code>. A possible source of confusion could be that both arrays and iteration spaces have a multidimensional rectilinear structure (i.e., they both look like hypercubes). However, arrays and iteration spaces are fundamentally different. Arrays are data structures whose elements are scalars. Iteration spaces are abstract geometric representations of schedules and their elements represent individual iterations of a loop nest. Transformations apply to iteration spaces, not to arrays.</p> <p>Comment: Accera's geometric interpretation of schedules resembles the iteration domain polyhedron, which is the cornerstone of the polyhedral model of compiler optimization. However, unlike polyhedrons, Accera iteration spaces are not embedded in a continuous space and cannot be manipulated by algebraic transformations. Accera iteration spaces always remain rectilinear and are inherently discrete objects.</p>"},{"location":"Manual/03%20Schedules/#iteration-space-slices","title":"Iteration space slices","text":"<p>Iteration space slices is an abstract concept that affects different aspects of the Accera programming model. Since the iteration space dimensions are ordered, each element of the iteration space can be identified by a vector of coordinates.  For example, the vector (1, 6, 7) identifies the iteration at position 1 along the first dimension, position 6 along the second dimension, and position 7 along the third dimension. If one or more coordinates are replaced with the wildcard symbol <code>*</code>, we get an iteration space slice, which is a set of iterations obtained by replacing the wildcard with all possible values. For example, (*, *, 2) represents a slice containing all the elements with 2 as their last coordinate. The dimension of a slice equals the number of wildcards in its definition.</p> <p></p> <p>The (3, 12, 15) iteration space. Highlighted elements belong to the (*, *, 2) slice.</p> <p>Iteration space slices in four dimensions, denoted by indices (<code>i</code>, <code>j</code>, <code>jj</code>, <code>k</code>):</p> (1, *, *, *) (*, *, *, 3) (2, *, 0, *)"},{"location":"Manual/03%20Schedules/#loops-indices-and-dimensions","title":"Loops, indices, and dimensions","text":"<p>When we defined <code>nest</code>, we used variables such as <code>i</code>, <code>j</code>, and <code>k</code> to name the loops in the loop-nest. When we described the default schedule using equivalent for-loops, <code>i</code>, <code>j</code>, and <code>k</code> became the index variables of those loops. Now, when we represent a schedule as an iteration space, these variables are used as the names of the corresponding iteration space dimensions. From here on, we move seamlessly between these different representations and use the terms loop, index, and dimension interchangeably.</p>"},{"location":"Manual/03%20Schedules/#schedule-transformations","title":"Schedule transformations","text":"<p>Iteration space transformations change the shape of the iteration space, possibly by adding dimensions or padding the space with empty elements.</p> <p>The iterations space always retains its rectilinear (hypercube) shape. In some cases, Accera transformations must pad the iteration space with empty elements to avoid reaching a jagged iteration space structure.</p>"},{"location":"Manual/03%20Schedules/#reorder","title":"<code>reorder</code>","text":"<pre><code># Reorder the indices.\nschedule.reorder(k, i, j)\n</code></pre> <p>The <code>reorder</code> transformation sets the order of indices in the schedule. From the iteration space point-of-view, <code>reorder</code> performs a pivot rotation of the iteration space, which orients its dimensions in a specified order. Since the iteration space elements are executed in lexicographic order, pivoting the iteration space is equivalent to reordering the loops.</p> <p>For example, we can write: <pre><code>schedule.reorder(j, k, i)\n</code></pre> After this transformation, <code>schedule</code> becomes equivalent to: <pre><code>for j in range(12):\n    for k in range(15):\n        for i in range(3):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre></p> Default schedule, order is (i, j, k) After <code>reorder(j, k, i)</code>, order is (j, k, i)"},{"location":"Manual/03%20Schedules/#invalid-orders","title":"Invalid orders","text":"<p>Some orders are not allowed. Describing these restrictions in full will require concepts that are yet to be introduced. Therefore, we are stating these restrictions here and will discuss them later in the upcoming sections. The restrictions are: 1. The inner dimension created by a <code>split</code> transformation (see below) must be ordered later than its corresponding outer dimension. 2. The fusing dimension created by a <code>fuse</code> operation (see Section 4) must always precede any unfused dimensions.</p> <p>Also note that <code>reorder</code> can also have the following overloaded form: <pre><code>schedule.reorder(order=(j, k, i))\n</code></pre> This form is better suited for use with parameters (see Section 9).</p>"},{"location":"Manual/03%20Schedules/#split","title":"<code>split</code>","text":"<pre><code># Splits dimension i into equally-sized parts, orients those parts along a new dimension ii, and stacks those parts along dimension i\nii = schedule.split(i, size)\n</code></pre> <p>From the iteration space point-of-view, the <code>split</code> transformation takes a dimension <code>i</code> and a <code>size</code>, modifies <code>i</code>, and creates a new dimension <code>ii</code>. Assume that the original size of dimension <code>i</code> was n: The <code>split</code> transformation splits the dimension <code>i</code> into ceil(n/size) parts of size <code>size</code>, orients each of these parts along dimension <code>ii</code>, and stacks the ceil(n/size) parts along the dimension <code>i</code>. If the split size does not divide the dimension size, empty elements are added such that the split size does divide the dimension size. As a result of the split, the size of <code>i</code> becomes ceil(n/size), the size of the new dimension <code>ii</code> equals <code>size</code>, and the iteration space remains rectilinear.</p> <p>In loop terms, <code>ii = split(i, size)</code> splits loop <code>i</code> into two loops: an inner loop <code>ii</code> and an outer loop, which inherits the original name <code>i</code>. Note that the outer loop always precedes the corresponding inner loop in the loop ordering.</p> <p>For example, starting from <code>nest</code> defined in Section 2, we could write: <pre><code>schedule = nest.create_schedule()\njj = schedule.split(j, 3)\n</code></pre> The resulting iteration space has a shape of (3, 4, 3, 15) and corresponds to the following python code: <pre><code>for i in range(3):\n    for j in range(0, 12, 3): # length 4, stride 3\n        for jj in range(3):\n            for k in range(15):\n                C[i, j+jj] += A[i, k] * B[k, j+jj]\n</code></pre> Note that loop <code>j</code> is no longer normalized (it has a stride of 3 rather than 1), which means that the nest is no longer a simple nest. As mentioned in the previous section, <code>Nest</code> objects always represent simple nests, but <code>Schedule</code> objects can represent more complex affine loop nests.</p> Default schedule After <code>split(j, 3)</code> <p>After performing a split, both the outer index and the inner index can be split again. For example, <pre><code>schedule = nest.create_schedule()\njj = schedule.split(j, 3)\njjj = schedule.split(j, 2) # split the outer index j again\n</code></pre> After the first split, the iteration space has the shape (3, 4, 3, 15). After the second split, the shape becomes (3, 2, 2, 3, 15). The transformed schedule corresponds to the following Python code: <pre><code>for i in range(3):\n    for j in range(0, 12, 6): # length 2, stride 6\n        for jjj in range(0, 6, 3): # length 2, stride 3\n            for jj in range(3):\n                for k in range(15):\n                    C[i, j+jj+jjj] += A[i, k] * B[k, j+jj+jjj]\n</code></pre></p> <p>The split does not necessarily need to divide the dimension size. For example, consider the following code: <pre><code>schedule = nest.create_schedule()\njj = schedule.split(j, 5)  # original size of dimension j was 12\n</code></pre> From the iteration space point-of-view, this code splits dimension <code>j</code> into three parts of size 5, where the last part is padded with empty (no-op) elements. Before the transformation, the iteration space shape is (3, 12, 15), and after the transformation, the shape is (3, 3, 5, 15) (so, 135 empty elements were added).</p> Default schedule (no-op elements in blue) After <code>split(j, 5)</code> <p>In loop form, the transformed iteration space corresponds to the following Python code: <pre><code>for i in range(3):\n    for j in range(0, 12, 5):\n        for jj in range(5):\n            for k in range(15):\n                if j+jj &lt; 12\n                C[i, j+jj] += A[i, k] * B[k, j+jj]\n</code></pre> Note that Accera optimizes away costly <code>if</code> statements by unswitching the loops, which results in code that looks more like this: <pre><code>for i in range(3):\n    for j in range(0, 10, 5):\n        for jj in range(5):\n            for k in range(15):\n                C[i, j+jj] += A[i, k] * B[k, j+jj]\n        # loop unswitching: handle the last iteration of the j loop separately\n        for j in range(10, 12):\n            for k in range(15):\n                C[i, j] += A[i, k] * B[k, j]\n</code></pre></p>"},{"location":"Manual/03%20Schedules/#meaningless-splits","title":"Meaningless splits","text":"<p>Next, we will describe Accera\u2019s behavior in a few degenerate cases. If the split size equals the dimension size, the transformation simply renames the split dimension. For example, <pre><code>schedule = nest.create_schedule()\njj = schedule.split(j, 12) # original size of dimension j was 12\n</code></pre> After the split, the size of <code>j</code> becomes 1 and the size of <code>jj</code> is <code>12</code>. The new shape of the iteration space is (3, 1, 12, 15). The dimension <code>j</code> becomes meaningless and therefore the schedule is basically unchanged.</p> <p>If the split size exceeds the dimension size, Accera will treat it as if the split size doesn't divide the dimension size. This special case is handled by adding empty elements. For example, <pre><code>schedule = nest.create_schedule()\njj = schedule.split(j, 13)  # original size of dimension j was 12\n</code></pre> After the split, the size of <code>j</code> becomes 1 and the size of <code>jj</code>, 13. The new shape of the iteration space is (3, 1, 13, 15), which means that 45 empty elements were added. These empty elements are removed during code generation, which means that the schedule is basically unchanged.</p> <p>Finally, note that <code>jj = schedule.split(j, 1)</code> simply adds a meaningless new dimension <code>jj</code> of size 1, and again, the schedule is unchanged.</p>"},{"location":"Manual/03%20Schedules/#convenience-syntax-tile","title":"Convenience syntax: <code>tile</code>","text":"<p>The <code>tile</code> transformation is a convenience syntax and does not provide any unique functionality. Consider the following code <pre><code>schedule = nest.create_schedule()\njj, kk = schedule.tile({\n    j: 2,\n    k: 3\n})\n</code></pre> The <code>tile</code> transformation above is shorthand for the following sequence of transformations: <pre><code>jj = schedule.split(j, 2)\nkk = schedule.split(k, 3)\n</code></pre></p> <p>It will result in a sequence of indices that are ordered as: <pre><code>(i, j, jj, k, kk)\n</code></pre> In other words, the <code>tile</code> transformation takes a tuple of indices and a tuple of sizes, splitting each index by the corresponding size. The indices involved in the split are then ordered such that each of the outer indices (parent index) precedes its inner indices (child index). On the other hand, indices that did not participate in the transformation retain their relative positions.</p>"},{"location":"Manual/03%20Schedules/#skew","title":"<code>skew</code>","text":"<pre><code># Skew dimension i with respect to dimension j.\nschedule.skew(i, j)\n</code></pre> <p>The <code>skew</code> transformation is the easiest to explain for a two-dimensional iteration space of shape (N, M). Skewing dimension <code>i</code> (the row dimension) with respect to <code>j</code> (the column dimension) modifies the iteration space column-by-column: column <code>j</code> gets j empty elements added to its start and M-j-1 empty elements to its end. As a result, each column grows from size N to size N+M-1. Geometrically, the original iteration space elements take the form of a 45-degree parallelogram, embedded within a bounding rectangle of shape (N+M-1, M). The element that used to be at coordinate (i, j) moves to coordinate (i+j, j).</p> <p>Similarly, skewing <code>j</code> with respect to <code>i</code> adds empty elements at the beginning and end of each row, resulting in an iteration space of shape (N, N+M-1). In higher dimensions, we simply apply the two-dimensional skew transformation independently to each two-dimensional slice along the two specified dimensions.</p> <p>To demonstrate the importance of this transformation, consider convolving a 10-element vector with a 3-element filter. The loop logic for this operation is defined as follows: <pre><code>import accera as acc\n\nN = 10  # input size\nK = 3  # filter size\nM = N - K + 1  # output size = 8\n\nA = acc.Array(role=acc.Role.INPUT, shape=(N,))\nB = acc.Array(role=acc.Role.INPUT, shape=(K,))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, shape=(M,))\n\nnest = acc.Nest(shape=(M, K))\ni, j = nest.get_indices()\n\n@nest.iteration_logic\ndef _():\n    C[i] += A[i+j] * B[j]\n\nschedule = nest.create_schedule()\n</code></pre> <code>schedule</code> corresponds to an iteration space of shape (8,3), where the first dimension corresponds to the 8 elements of the output vector. This schedule calculates the outputs one by one: first <code>C[0]</code>, then <code>C[1]</code>, etc.</p> <p>Here is the equivalent Python code: <pre><code>for i in range(8):\n    for j in range(3):\n        C[i] += A[i+j] * B[j]\n</code></pre></p> <p>Now, say that we apply the <code>skew</code> transformation as follows: <pre><code>schedule.skew(i, j)\n</code></pre> This transformation results in an iteration space of shape (10, 3), where the first dimension corresponds to the 10 elements of the input. This transformed schedule processes the input elements one-by-one: it extracts all the information from <code>A[0]</code> (<code>A[0]</code> is only used in the calculation of <code>C[0]</code>), then moves on to <code>A[1]</code> (which contributes to both <code>C[0]</code> and <code>C[1]</code>), and so on.</p> <p>In this example, the default schedule achieves memory locality with respect to array <code>C</code>, whereas the skewed schedule achieves memory locality with respect to array <code>A</code>.</p> <p>In loop form, the transformed iteration space corresponds to the following Python code:</p> <pre><code>for i in range(10):\n    for j in range(3):\n        if (i-j) &gt;= 0 and (i-j) &lt; 8:\n            C[i-j] += A[i] * B[j]\n</code></pre> <p>Behind the scenes, unswitching the loops results in code that looks more like this:</p> <pre><code># triangle of height 2, width 3\nfor j in range(1):\n    C[0-j] += A[0] * B[j]\nfor j in range(2):\n    C[1-j] += A[1] * B[j]\n\n# rectangle of shape (6, 3)\nfor i in range(2, 8):\n    for j in range(3):\n        C[i-j] += A[i] * B[j]\n\n# upside-down triangle of height 2, width 3\nfor j in range(2):\n    C[6+j] += A[8] * B[2-j]\nfor j in range(1):\n    C[7+j] += A[9] * B[2-j]\n</code></pre> <p>Finally, note that some loops have small sizes that can be replaced by unrolls. To enable the unrolling of these small loops, we can use this optional parameter:</p> <pre><code>schedule.skew(i, j, unroll_loops_smaller_than=3)\n</code></pre> <p>This will unroll all loops that are smaller than 3, which include the <code>range(2)</code> and <code>range(1)</code> loops in the example above.</p>"},{"location":"Manual/03%20Schedules/#pad","title":"<code>pad</code>","text":"<pre><code># Adds empty elements to the beginning of dimension i.\nschedule.pad(i, size)\n</code></pre> <p>The <code>pad</code> transformation pads the beginning of dimension <code>i</code> with empty elements. This operation is meaningless by itself, but can be useful when used with splitting or fusing.</p>"},{"location":"Manual/03%20Schedules/#order-invariant-schedules-and-safety","title":"Order-invariant schedules and safety","text":"<p>A schedule is order-invariant if its underlying logic doesn't depend on the execution order of its iterations. For example, schedules created from a single <code>Nest</code> (via <code>create_schedule()</code>) are order-invariant. All of the schedules discussed so far have been order-invariant.</p> <p>A schedule is safe if its underlying logic is guaranteed to remain intact regardless of the applied transformations. Not all schedules are safe, but order-invariant schedules are. This is because the transformations introduced in this section only change the execution order of iterations without adding or removing any work.</p> <p>In Section 4, we introduce fused schedules, which are not order-invariant, but may still be safe.</p>"},{"location":"Manual/04%20Fusing/","title":"Section 4: Fusing","text":"<p>With <code>fuse</code> operation, multiple schedules can be combined into a single schedule representing the union of the work in the original schedules. These fused schedules can be transformed by any of the transformations presented in Section 3.</p>"},{"location":"Manual/04%20Fusing/#full-fusing","title":"Full fusing","text":"<pre><code>import accera as acc\n\n# Fuse three schedules to create a fused schedule\nschedule = acc.fuse(schedule0, schedule1, ...)\n</code></pre> <p>Full fusing is the most straightforward, where each dimension is fused with the corresponding dimension from other schedules. </p>"},{"location":"Manual/04%20Fusing/#full-fusing-of-same-shaped-iteration-spaces","title":"Full fusing of same-shaped iteration spaces","text":"<p>First, consider the simplest case where we fuse schedules with identical iteration space shapes. This fusing assigns a new dimension called fusing dimension to the fused schedule <code>schedule</code> that does not exist in the original schedules. By default, the fusing dimension is the last dimension in the fused schedule. Its size is equal to the number of fused schedules. The slices along the fusing dimension contain a copy of the iteration logic of <code>schedule0</code>, <code>schedule1</code>. The first slice along the fusing dimension contains a copy of the iteration logic of <code>schedule0</code>, the second slice contains that of <code>schedule1</code>, and so on. Since the fusing dimension is the last dimension, the fused schedule is logically equivalent to executing an iteration of <code>schedule0</code>, followed by an iteration of <code>schedule1</code>, and so on.</p> <p>Consider a scenario where we want first to shift and then scale each element of a matrix. In other words, we want to perform the equivalent of the below Python code: <pre><code>C = (C + A) * B\n</code></pre></p> <p>If all three matrices are 10 by 10, one way to do this without fusing is to write:  <pre><code>A = acc.Array(role=acc.Role.INPUT, shape=(10, 10))\nB = acc.Array(role=acc.Role.INPUT, shape=(10, 10))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, shape=(10, 10))\n\n# Create nest_simple and schedule_simple\nnest_simple = acc.Nest(shape=(10, 10))\ni, j = nest_simple.get_indices()\n\n@nest_simple.iteration_logic\ndef _():\n    C[i, j] = (C[i, j] + A[i, j]) * B[i, j]\n\nschedule_simple = nest_simple.create_schedule()\n</code></pre> Note that each iteration in <code>schedule_simple</code> executes simultaneously on all three arrays. However, there can be a case where concurrent operation on these arrays creates excessive pressure on the computer\u2019s memory cache, resulting in lower performance. In such a case, simultaneous operation on two arrays instead of three has a computational advantage.</p> <p>Therefore, we may first want to compute <code>C += A</code> and then compute <code>C *= B</code>.  Better yet, we may want to compute <code>C</code> in 2\u00d72 blocks. We first compute <code>C[0:2, 0:2] += A[0:2, 0:2]</code>. Subsequently, we compute <code>C[0:2, 0:2] *= B[0:2, 0:2]</code>. Finally, we move on to the next block and compute <code>C[2:4, 0:2] += A[2:4, 0:2]</code>, and so on. This way, fusing offers remarkable flexibility to explore all of these different execution possibilities. </p> <p>First, we define two separate nests, one for the <code>C += A</code> logic and one for the <code>C *= B</code> logic, and get their corresponding default schedules:  <pre><code># Create nest0 and schedule0\nnest0 = acc.Nest(shape=(10, 10))\ni0, j0 = nest0.get_indices()\n\n@nest0.iteration_logic\ndef _():\n    C[i0, j0] += A[i0, j0]\n\nschedule0 = nest0.create_schedule()\n\n# Create nest1 and schedule1\nnest1 = acc.Nest(shape=(10, 10))\ni1, j1 = nest1.get_indices()\n\n@nest1.iteration_logic\ndef _():\n    C[i1, j1] *= B[i1, j1]\n\nschedule1 = nest1.create_schedule()\n</code></pre></p> <p>Before fusing, both <code>schedule0</code> and <code>schedule1</code> have a shape (10, 10). Now, let\u2019s fuse them: <pre><code># Create a fused schedule\nschedule = acc.fuse(schedule0, schedule1)\ni, j, f = schedule.get_indices()\n</code></pre> Fusing creates a new fused schedule <code>schedule</code> with a shape (10, 10, 2). It does not change <code>schedule0</code> and <code>schedule1</code>. The last dimension in <code>schedule</code> is the so-called fusing dimension <code>f</code>. Its slice (*, *, 0) contains a copy of <code>schedule0</code>, and its slice (*, *, 1) contains a copy of <code>schedule1</code>.</p> Before fusing After <code>fuse(schedule0, schedule1)</code> <p>In loop form, <code>schedule</code> is now equivalent to the following Python code:</p> <pre><code>for i in range(10):\n    for j in range(10):\n        # f = 0\n        C[i, j] += A[i, j]\n        # f = 1\n        C[i, j] *= B[i, j]\n</code></pre> <p></p> <p>Resulting iteration sequence for <code>C = (C + A) * B</code>. (White elements represent <code>C + A</code>; purple elements are <code>C * B</code>)</p>"},{"location":"Manual/04%20Fusing/#tiling","title":"Tiling","text":"<p>Recall that we discussed computing the output block-by-block: first computing <code>C[0:2, 0:2] += A[0:2, 0:2]</code>, then computing <code>C[0:2, 0:2] *= B[0:2, 0:2]</code>, and so on. This can be achieved with the following sequence of transformations: <pre><code>ii, jj = schedule.tile({\n    i: 2,\n    j: 2\n})\nschedule.reorder(i, j, f, ii, jj)\n</code></pre> The resulting <code>schedule</code> is equivalent to the following Python code: <pre><code>for i in range(0, 10, 2):\n    for j in range(0, 10, 2):\n        # f = 0\n        for ii in range(2):\n            for jj in range(2):\n                C[i+ii, j+jj] += A[i+ii, j+jj]\n        # f = 1\n        for ii in range(2):\n            for jj in range(2):\n                C[i+ii, j+jj] *= B[i+ii, j+jj]\n</code></pre></p>"},{"location":"Manual/04%20Fusing/#constraints-of-fusing-dimension","title":"Constraints of Fusing Dimension","text":"<p>The fusing dimension comes with certain constraints that are discussed from the <code>safety</code> perspective with examples. </p>"},{"location":"Manual/04%20Fusing/#constraint-1-the-fusing-dimension-is-executed-sequentially","title":"Constraint 1: the fusing dimension is executed sequentially","text":"<p>Unlike other dimensions that allow parallelization, vectorization, or tensorization (see Section 7 ), none of these operations can be applied to the fusing dimension. The fusing dimension must be executed sequentially. This constraint enables the safety guarantee discussed below.   </p>"},{"location":"Manual/04%20Fusing/#safety","title":"Safety","text":"<p>Before applying any subsequent transformations, the fused schedule is always logically equivalent to executing the original schedules sequentially for each value of the fused dimensions. However, is it safe? Recall that a schedule is considered safe if the underlying logic is guaranteed to be unchanged regardless of the applied transformation. The safety of a fused schedule depends on circumstances that may break logic equivalence: </p> <p>Accera preserves the order of the fused schedules for each value of the fused dimensions, regardless of how the fused schedule is transformed. For example, in the example above, the fused dimensions are <code>i</code> and <code>j</code>. Therefore, for any concrete value of <code>i</code> and <code>j</code>, the corresponding operation from <code>schedule0</code> is guaranteed to execute before the corresponding operation from <code>schedule1</code>, regardless of how the fused schedule is transformed. More specifically, for each <code>i</code> and <code>j</code>, the operation <code>C[i, j] += A[i, j]</code> is guaranteed to execute before the operation <code>C[i, j] *= B[i, j]</code>, no matter how we transform the fused schedule. Since those are the only operations that interact with <code>C[i,j]</code>, the Accera guarantee is sufficient, and we can claim that the fused schedule is safe. With this assurance, the programmer can apply any sequence of transformations without worrying about the correctness of the resulting implementation.</p> <p>However, not every fusing operation creates a safe schedule. For example, consider a scenario where we fused <code>schedule0</code> and <code>schedule1</code> differently: <pre><code># Reorder schedule1 before fusing\nschedule1.reorder(j1, i1)\n# Fuse schedule0 with the reordered schedule1\nschedule_t = acc.fuse(schedule0, schedule1)\na, b, f = schedule_t.get_indices()\n</code></pre> In this unnatural example, <code>i0</code> and <code>j1</code> are fused and named <code>a</code>. Similarly,<code>i1</code> and <code>j0</code> are fused and named <code>b</code>. As mentioned above, Accera guarantees that, for each value of <code>a</code> and <code>b</code>, the operation <code>C[a, b] += A[a, b]</code> is executed before <code>C[b, a] *= B[b, a]</code>. The fusing operation itself preserves the logical equivalence. However, the underlying logic is changed with the transformation performed before fusion:  <pre><code>schedule1.reorder(j1, i1)\n</code></pre> To understand this change in the logic, note that the resulting schedule is equivalent to the following Python code: <pre><code>for a in range(10):\n    for b in range(10):\n        C[a, b] += A[a, b]\n        C[b, a] *= B[b, a]\n</code></pre> The above code sets <code>C[1,0]</code> to <code>C[1,0] * B[1,0] + A[1,0]</code>, whereas the original fused logic set <code>C[1,0]</code> to <code>(C[1,0] + A[1,0]) * B[1,0]</code>. In this case, we can conclude that <code>schedule_t</code> is definitely not safe. If the programmer decides to create an unsafe schedule, they take upon themselves the responsibility of maintaining logical equivalence.</p>"},{"location":"Manual/04%20Fusing/#fusing-iteration-spaces-with-different-shapes","title":"Fusing iteration spaces with different shapes","text":"<p>If the iterations spaces have different shapes, Accera matches their shapes by padding them appropriately with empty cells.</p>"},{"location":"Manual/04%20Fusing/#partial-fusing","title":"Partial fusing","text":"<p>Instead of fusing all the dimensions, we may want to fuse a subset of dimensions, leaving the rest unfused. To fuse the first s dimensions, we use the syntax: <pre><code># Fuse the first s dimensions of three schedules\nschedule = acc.fuse((schedule0, schedule1, ...), partial=s)\n</code></pre> The order of the dimensions in the fused schedule is as follows: first the fused dimensions s, then the fusing dimension <code>f</code>, followed by the unfused dimensions of <code>schedule0</code>, <code>schedule1</code>, and so on.</p> <p>We can easily calculate the number of dimensions in the fused schedule. For example, if we fuse the first s dimensions of a d0-dimensional space <code>schedule0</code> and a d1-dimensional space <code>schedule1</code>, the fused iteration space will have s fused dimensions, d0 + d1 - 2s unfused dimensions, and the special fusing dimension <code>f</code>, for a total of d0 + d1 - s + 1 dimensions.</p> <p>The <code>fuse</code> operation uses padding to ensure that the fused iteration space is not jagged in any direction. For example, say that we partially fuse the first 2 dimensions of <code>schedule0</code>, which is 4-dimensional, and <code>schedule1</code>, which is 3-dimensional: <pre><code>schedule = acc.fuse((schedule0, schedule1), partial=2)\ni, j = schedule.get_fused_indices()\nf = schedule.get_fusing_index()\nk, l, m = schedule.get_unfused_indices()\n# Alternative way:\n# i, j, f, k, l, m = schedule.get_indices()\n</code></pre> First comes the fused dimensions <code>i</code> and <code>j</code>. Nest is the fusing dimensions <code>f</code> of size 2, followed by the unfused dimensions <code>k</code> and <code>l</code> from <code>schedule0</code> and <code>m</code> from <code>schedule1</code>. The slice (*, *, 0, *, *, 0) contains a copy of <code>schedule0</code>, the slice (*, *, 1, 0, 0, *) contains a copy of <code>schedule1</code>, and the rest of <code>schedule</code> is padded with empty elements. Note that full fusing is a special case of partial fusing, where <code>s</code> is the larger of the dimensions of <code>schedule0</code> and <code>schedule1</code>.</p>"},{"location":"Manual/04%20Fusing/#constraint-2-the-fusing-dimension-always-precedes-unfused-dimensions","title":"Constraint 2: the fusing dimension always precedes unfused dimensions","text":"<p>Another constraint introduced by partial fusing is that the fusing dimension must precede all of the unfused dimensions in its dimension order. This constraint applies to dimensions derived from the fusing dimension and the unfused dimensions via splitting.</p>"},{"location":"Manual/04%20Fusing/#safety_1","title":"Safety","text":"<p>The safety guarantees for partial fusing are a natural extension of the guarantees for full fusing. For each value of the fused dimensions, Accera preserves the fused schedules' order regardless of how the fused schedule is transformed. In other words, for each concrete value of fused dimensions, all the corresponding work in <code>schedule0</code> (across all of its unfused dimensions) is performed before the corresponding work in <code>schedule1</code> (across all of its unfused dimensions). This remains true no matter how we transform the fused schedule. While fusing, the programmer needs to consider if this property implies safety. The below examples shows how this can be done. </p>"},{"location":"Manual/04%20Fusing/#partial-fusing-example-fully-connected-neural-layer-with-activation","title":"Partial fusing example: fully-connected neural layer with activation","text":"<p>Consider applying an element-wise operation, such as the ReLU function of AI, to the result of a matrix-matrix multiplication. This is called a fully connected layer with a ReLU activation in the language of neural networks. The function <code>relu(x)</code> is simply <code>max(x,0)</code>.</p> <p>Imagine that we have an element-wise operator <code>relu</code>, and we want to implement the equivalent Python code: <pre><code>C = relu(C + A @ B)\n</code></pre> Here, <code>A</code> has a shape of (8, 4), <code>B</code> has a shape of (4, 8), and <code>C</code> has a shape of (8, 8). Let\u2019s now define two nests, one for <code>C += A @ B</code> and the other for <code>C = relu(C)</code>, and obtain their corresponding default schedules: <pre><code># Create nest0 and schedule0\nnest0 = acc.Nest(shape=(8, 8, 4))\ni0, j0, k0 = nest0.get_indices()\n\n# nest0 performs C += A @ B\n@nest0.iteration_logic\ndef _():\n    C[i0, j0] += A[i0, k0] * B[k0, j0]\n\nschedule0 = nest0.create_schedule()\n\n# Create nest1 and schedule1\nnest1 = acc.Nest(shape=(8, 8))\ni1, j1 = nest1.get_indices()\n\n# nest1 performs C = relu(C)\n@nest1.iteration_logic\ndef _():\n    C[i1, j1] = acc.max(C[i1, j1], 0)\n\nschedule1 = nest1.create_schedule()\n</code></pre> In <code>schedule0</code> and <code>schedule1</code>, the first dimension represents the rows of <code>C</code> and the second dimension represents the columns of <code>C</code>. Additionally, <code>schedule0</code> has a third dimension that <code>schedule1</code> does not have. Therefore, we fuse the first two dimensions of the iteration spaces and leave the third dimension of <code>schedule0</code> unfused. <pre><code>schedule = acc.fuse((schedule0, schedule1), partial=2)\ni, j = schedule.get_fused_indices()\nf = schedule.get_fusing_index()\nk0 = schedule.get_unfused_indices()[0]\n# Alternative way:\n# i, j, f, k0 = schedule.get_indices()\n</code></pre></p> <p>The fused iteration space <code>schedule</code> has a shape of (8, 8, 2, 4). Its slice (*, *, 0, *) contains a copy of <code>schedule0</code>, the slice (*, *, 1, 0) contains a copy of <code>schedule1</code>, and the rest of its elements are padded. Note that the code above overwrites the index <code>k0</code>, which initially was an index of <code>schedule0</code>. However, now it corresponds to the unfused index in <code>schedule</code>. Note that the name <code>k0</code> is a stylistic choice, we could have chosen a different name.</p> Before fusing After <code>fuse((schedule0, schedule1), partial=2)</code> (padded elements in blue)"},{"location":"Manual/04%20Fusing/#safety_2","title":"Safety","text":"<p>Is <code>schedule</code> safe? Recall that for each value of <code>i</code> and <code>j</code>, Accera guarantees that the corresponding work in <code>schedule0</code> (<code>C[i,j] += A[i,k0] * B[k0,j]</code> for all values of <code>k0</code>) is executed before the corresponding work in <code>schedule1</code> (<code>C[i,j] = max(C[i,j], 0)</code>), and this holds regardless of how the fused schedule is transformed. Since these are the only operations that touch <code>C[i,j]</code> and the <code>ReLU</code> operation is always executed last, this warrants that <code>schedule</code> is safe. Therefore, we can focus all of our attention on optimizing performance without worrying about correctness from this point onwards.</p> <p>The resulting schedule is now equivalent to the following Python code:</p> <pre><code>for i in range(16):\n    for j in range(10):\n        # f = 0\n        for k0 in range(11):\n            C[i,j] += A[i,k0] * B[k0,j]\n        # f = 1\n        C[i,j] = max(C[i,j], 0)\n</code></pre> <p></p> <p>Iteration sequence for <code>C = relu(C + A @ B)</code>. (White elements represent <code>C + A @ B</code>; purple elements are <code>relu(C)</code>; blue elements are padding.)</p>"},{"location":"Manual/04%20Fusing/#partial-fusing-example-multiplying-three-matrices","title":"Partial fusing example: multiplying three matrices","text":"<p>Consider fusing two matrix-matrix multiplications to get matrix-matrix-matrix multiplication. Specifically, say that our goal is to calculate the equivalent of the following Python code: <pre><code>E += A @ B @ D\n</code></pre> Where <code>A</code> has a shape (4, 5), <code>B</code> (5, 6), <code>D</code> (6, 10), and <code>E</code> (4, 10).</p> <p>We start by defining the arrays. In addition to <code>A</code>, <code>B</code>, <code>D</code>, and <code>E</code>, we define a temporary array <code>C</code> to store the intermediate result of <code>A@B</code>. <pre><code>A = acc.Array(role=acc.Role.INPUT, shape=(4, 5))\nB = acc.Array(role=acc.Role.INPUT, shape=(5, 6))\nC = acc.Array(role=acc.Role.TEMP, shape=(4, 6))\nD = acc.Array(role=acc.Role.INPUT, shape=(6, 10))\nE = acc.Array(role=acc.Role.INPUT_OUTPUT, shape=(4, 10))\n</code></pre> Note that <code>C</code> has the role of <code>TEMP</code>. Temporary arrays are mutable and initialized with zeros. Moreover, these arrays are logical objects that may not exist in memory during the entire computation.</p> <p>Next, define a simple nest to compute <code>C += A @ B</code> and another simple nest to compute <code>E += C @ D</code>. <pre><code># Create nest0 and schedule0 for C = A @ B\nnest0 = acc.Nest(shape=(4, 6, 5))\ni0, j0, k0 = nest0.get_indices()\n\n@nest0.iteration_logic\ndef _():\n    C[i0, j0] += A[i0, k0] * B[k0, j0]\n\nschedule0 = nest0.create_schedule()\n\n# Create nest1 and schedule1 E += C @ D\nnest1 = acc.Nest(shape=(4, 10, 6))\ni1, j1, k1 = nest1.get_indices()\n\n@nest1.iteration_logic\ndef _():\n    E[i1, j1] += C[i1, k1] * D[k1, j1]\n\nschedule1 = nest1.create_schedule()\n</code></pre> The temporary array <code>C</code> stores the output of <code>schedule0</code>, which is then used as one of the inputs of <code>schedule1</code>. Dimensions <code>i0</code> and <code>j0</code> correspond to the rows and columns of <code>C</code> in <code>schedule0</code>. Similarly, dimensions <code>i1</code> and <code>k1</code> correspond to the rows and columns of <code>C</code> in <code>schedule1</code>. Therefore, we fuse <code>i0</code> with <code>i1</code> and <code>j0</code> with <code>k1</code>. We need to correctly line up the dimensions of the two iteration spaces and perform partial fusing. <pre><code>schedule1.reorder(i1, k1, j1)\nschedule = acc.fuse((schedule0, schedule1), partial=2)\ni, j = schedule.get_fused_indices()\nf = schedule.get_fusing_index()\nk0, j1 = schedule.get_unfused_indices()\n# Alternative way:\n# i, j, f, k0, j1 = schedule.get_indices()\n</code></pre></p> Before <code>reorder(i1, k1, j1)</code> After <code>reorder(i1, k1, j1)</code> <p>The fused iteration space has a shape of (4, 6, 2, 5, 10). <code>i</code> is the result of fusing <code>i0</code> and <code>i1</code>, <code>j</code> is the result of fusing <code>j0</code> and <code>k1</code> and <code>f</code> is the fusing dimension. On the other hand, <code>k0</code> is the unfused dimension from <code>schedule0</code>, and <code>j1</code> is the unfused dimension from <code>schedule1</code>. The slice (*, *, 0, *, 0) contains a copy of <code>schedule0</code> and the slice (*, *, 1, 0, *) contains a copy of <code>schedule1</code>. The rest of the iteration space is padded with empty elements.</p> <p></p> <p>After <code>fuse((schedule0, schedule1), partial=2)</code> (White elements represent <code>C += A @ B</code>; purple elements are <code>E += C @ D</code>; blue elements are padding.)</p>"},{"location":"Manual/04%20Fusing/#safety_3","title":"Safety","text":"<p>Is <code>schedule</code> safe? Again, recall that for each value of <code>i</code> and <code>j</code>, Accera guarantees that all of the corresponding work in <code>schedule0</code> (<code>C[i, j] += A[i, k0] * B[k0, j]</code> for all values of <code>k0</code>) is executed before any of the corresponding work in <code>schedule1</code> (<code>E[i, j1] += C[i, j] * D[j, j1]</code> for all values of <code>j1</code>). In other words, each element of <code>C</code> is entirely computed before it is used. This confirms that the <code>schedule</code> is safe.</p> <p>Initially, the fused schedule is equivalent to the following Python code: <pre><code>for i in range(4):\n    for j in range(6):\n        for f in range(2):\n            for k0 in range(5):\n                for j1 in range(7):\n                    if f == 0 and j1 == 0:\n                        # f = 0, create C[i, j]\n                        C[i, j] += A[i, k0] * B[k0, j]\n                    if f == 1 and k0 == 0:\n                        # f = 1, use C[i, j]\n                        E[i, j1] += C[i, j] * D[j, j1]\n</code></pre></p> <p>The simplified loops after unswitching:</p> <pre><code>for i in range(4):\n    for j in range(6):\n        # f = 0, create C[i, j]\n        for k0 in range(5):\n            C[i, j] += A[i, k0] * B[k0, j]\n        # f = 1, use C[i, j]\n        for j1 in range(7):\n            E[i, j1] += C[i, j] * D[j, j1]\n</code></pre> <p>The advantage of this schedule is that only one element of <code>C</code> is active at any time in the computation. Accera can reuse the same memory location to store the active element of <code>C</code> instead of storing all of <code>C</code> in physical memory.</p>"},{"location":"Manual/04%20Fusing/#tiling_1","title":"Tiling","text":"<p>As a further optimization, we can compute a 2\u00d73 block of <code>C</code>. Do all the work that uses this block and then move on to the next block: <pre><code>ii, jj = schedule.tile({\n    i: 2,\n    j: 3\n})\nschedule.reorder(i, j, f, ii, jj, k0, j1)\n</code></pre> This schedule is equivalent to the following Python code: <pre><code>for i in range(0, 4, 2):\n    for j in range(0, 6, 3):\n        # f = 0\n        for ii in range(2):\n            for jj in range(3):\n                for k0 in range(11):\n                    C[i+ii, j+jj] += A[i+ii, k0] * B[k0, j+jj]\n        # f = 1\n        for ii in range(2):\n            for jj in range(3):\n                for j1 in range(7):\n                    E[i+ii, j1] += C[i+ii, j+jj] * D[j+jj, j1]\n</code></pre></p>"},{"location":"Manual/05%20Targets/","title":"Section 5: Targets","text":"<p>Accera is a cross compiler, which means that it can generate executable code for different target platforms. A target is described using the <code>Target</code> class. Accera already supports many different targets, for example: <pre><code>import accera as acc\n\ncorei9 = acc.Target(Target.Model.INTEL_7960X, num_threads=44)\n</code></pre> or</p> <p><pre><code>v100 = acc.Target(Target.Model.NVIDIA_V100)\n</code></pre> or</p> <pre><code>corei7 = acc.Target(known_name=\"Intel 7700T\")\n</code></pre> <p>To query the list of known names: <pre><code>dir(acc.Targets.Model)\n</code></pre></p> <p>We can also define custom targets: <pre><code>my_target = acc.Target(name=\"Custom processor\", category=acc.Target.Category.CPU, architecture=acc.Target.Architecture.X86_64, family=\"Broadwell\", extensions=[\"MMX\", \"SSE\", \"SSE2\", \"SSE3\", \"SSSE3\", \"SSE4\", \"SSE4.1\", \"SSE4.2\", \"AVX\", \"AVX2\", \"FMA3\"], num_cores=22, num_threads=44, frequency_GHz=3.2, turbo_frequency_GHz=3.8, cache_sizes=[32, 256, 56320], cache_lines=[64, 64, 64])\n</code></pre></p> <p>One benefit of targets is that they provide a standard way of accessing useful constants. For example, we may want to split an iteration space dimension by the number of elements that fit in a vector register. <pre><code>schedule.split(i, size=corei9.vector_bytes/4)\n</code></pre> We may tile the iteration space for GPU targets based on input shapes and available resources like shared memory. If you are not sure of what to use, try starting with the default: <pre><code># find block_x and block_y in powers of two, such that block_x*block_y=v100.default_block_size.\nblock_x = pow(2, math.log2(v100.default_block_size)//2)\nblock_y = v100.default_block_size // block_x\nii, jj = schedule.tile({\n    i: block_x,\n    j: block_y\n})\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/","title":"Section 6: Plans - Caching","text":"<p>In the previous sections, we defined the logic and then scheduled its iterations. Now, let's move on to completing the implementation with target-specific options.</p> <p>First, we create a plan from the schedule: <pre><code>plan = schedule.create_plan()\n</code></pre> The Accera programming model allows us to create multiple plans from a single schedule. More importantly, we can modify individual plans without changing the schedule. We can manually specify the target platform by calling <code>create_plan</code> that takes a <code>target</code> argument. The default value of this <code>target</code> argument is <code>acc.Target.HOST</code>, which refers to the current host computer.</p> <p>In this section, we discuss how to add data caching strategies to a plan.</p> <p>Not yet implemented: Data caching strategies are not supported when one or more of the Array's dimension sizes are specified at runtime.</p>"},{"location":"Manual/06%20Plans%20-%20Caching/#key-slices","title":"Key slices","text":"<p>Recall that a slice is a set of iteration space elements that match a coordinate template with wildcards, such as <code>(1, *, 3)</code>. A key-slice is a slice with only right-aligned wildcards, such as <code>(1, 2, *)</code> and <code>(3, *, *)</code>. The level of a key-slice is determined by the number of wildcards in its definition. For example, <code>(1, 2, *)</code> is a level 1 key-slice and <code>(3, *, *)</code> is a level 2 key-slice.</p> <p>Note that the <code>key-slices</code> are changed by reordering the dimensions of the iteration space. However, it is always true that the entire d-dimensional iteration space is a level d key-slice and each individual element is a level zero key-slice. For a total of d+1 different key-slices, each iteration belongs to one key-slice from each level, zero to d. When the schedule is executed, the key-slices containing the current iteration are called the current key-slices.</p> <p>In the Accera programming model, key-slices are significant because they partition the iteration space into sets of consecutive iterations. Therefore, they can describe the phases of computation at different levels of granularity. The term key-slice suggests using them to key (trigger) different actions. Specifically, each time the current level-l key slice changes, we use this event to trigger a cache update.</p> <p>As mentioned above, a key-slice can be identified by its level. Another way to specify a key-slice is to take advantage of the iteration space dimensions being named in the order. To specify a key-slice for a dimension, replace it and subsequent dimensions with wildcard symbols. For example, if the names of the iteration space dimensions are <code>(i, j, k)</code>, then a key-slice that corresponds to the dimension <code>j</code> is one of <code>(0, *, *)</code>, <code>(1, *, *)</code>, etc. Both ways of specifying a key-slice are useful and Accera uses them interchangeably.</p>"},{"location":"Manual/06%20Plans%20-%20Caching/#active-elements-and-active-blocks","title":"Active elements and active blocks","text":"<p>A loop nest operates on the data that is stored in arrays. Each key-slice can access a subset of the array elements, which we call the active elements that correspond to that specific key-slice. Since the current iteration belongs to key-slices at different levels, we need to define corresponding sets of active elements at different levels.</p> <p>More precisely, array <code>A</code> elements that are read from or written to by the iterations of the current level l key-slice are called the level l active elements of <code>A</code>.  This set of elements does not necessarily take the shape of a block. Therefore, the level l active block of <code>A</code> can be defined as the smallest block of elements that contains all of the level l active elements in <code>A</code>. Accera uses active blocks to define caching strategies.</p> <p>Just like we can specify a key-slice using a dimension, we can also refer to the active block that corresponds to a specific dimension. For example, if the names of the iteration space dimensions are <code>(i, j, k)</code> and the current iteration is one of the iterations for which <code>i=3</code>, then the active block in <code>A</code> that corresponds to dimension <code>j</code> is the block that includes all the elements touched by the key-slice <code>(3, *, *)</code>.</p>"},{"location":"Manual/06%20Plans%20-%20Caching/#caches","title":"Caches","text":"<p>An Accera cache is a local copy of an active block. A cache is contiguous in memory and its memory layout may differ from the layout of the original array. The loop nest iterations operate on the cache elements instead of the original array elements.</p> <p>The contents of the active block are copied into the cache at the start of the corresponding key-slice. If the array is mutable (namely, an input/output array or a temporary array), the cache contents are copied back into the original array at the end of the key-slice.</p>"},{"location":"Manual/06%20Plans%20-%20Caching/#caching-by-level","title":"Caching by level","text":"<p>To define a cache for a given array, all we need is to specify the desired level.For example: <pre><code>AA = plan.cache(A, level=2)\n</code></pre> The return value <code>AA</code> is a handle that can be used to refer to the cache in subsequent operations. We can choose the cache layout, just as we did when we defined the original array. <pre><code>AA = plan.cache(A, level=2, layout=acc.Array.Layout.FIRST_MAJOR)\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/#caching-by-dimension","title":"Caching by dimension","text":"<p>As mentioned above, we can specify an active block using a dimension. We use this to define a cache as follows: <pre><code>AA = plan.cache(A, index=j)\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/#caching-by-element-budget","title":"Caching by element budget","text":"<p>Note that the current active blocks of an array are nested, and their sizes are monotonic (nondecreasing) in their level. Therefore, we can also select the largest active block that does not exceed a certain number of elements: <pre><code>AA = plan.cache(A, max_elements=1024)\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/#thrifty-caching","title":"Thrifty caching","text":"<p>By default, Accera caching strategies are thrifty in the sense that the data is physically copied into an allocated cache only if the cached data somehow differs from the original active block. Therefore, if the original active block is already in the correct memory layout and resides contiguous in memory. Accera skips the caching steps and uses the original array instead. Note that a physical copy is created on a GPU if the cache is supposed to be allocated a different type of memory than the original array (e.g., the array is in global memory, but the cache is supposed to be in shared memory).</p> <p>For example, assume that <code>A</code> is a two-dimensional array and its active block at the chosen level is always one of its rows. If <code>A</code> is row-major, the rows are already stored contiguously. Additionally, the data in the active block and the data to be copied to cache are identical: both are contiguous and share the same memory layout. In this case, there is no benefit in using cache over the original array. The thrifty caching strategy will skip the caching steps and use the original array instead.</p> <p>On the other hand, if <code>A</code> is column-major, its rows are not stored contiguously. In this case, copying the active row into a contiguous temporary location could be computationally advantageous. Therefore, the thrifty caching strategy would create the cache and populate it with the data.</p> <p>Thrifty caching can be turned off using the optional argument <code>thrifty=False</code>. If turned off, a physical copy is always created.</p>"},{"location":"Manual/06%20Plans%20-%20Caching/#hierarchical-caching","title":"Hierarchical caching","text":"<p>Caches can be composed hierarchically. Namely, a high-level key-slice can trigger a copy from the original array into a big cache, and a lower level key-slice can be used to trigger a copy from the big cache into a smaller cache.</p> <p>For example, <pre><code>AA = plan.cache(A, level=4)\nAAA = plan.cache(AA, level=2)\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/#multicaching","title":"Multicaching","text":"<p>While caches are defined with a key-slice <code>level</code>, a higher-level key slice <code>trigger_level</code> can be specified as the trigger key-slice for copying multiple successive active blocks of elements to a local copy. These copied active blocks have their layouts defined as usual, and only the trigger level for copying them has been changed. Since active blocks are not mutually exclusive, this can result in the same element being copied into multiple locations as separate caches. Therefore, a <code>trigger_level</code> may only be specified on an <code>INPUT</code> or <code>CONST</code> array as Accera does not support multicache write coherence.</p> <p>For example, <pre><code>AA = plan.cache(A, level=2, trigger_level=4)\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/#mapping-caches-to-specific-types-of-memory","title":"Mapping caches to specific types of memory","text":"<p>Some target platforms have different types of memory that can hold Accera caches. In the case of a GPU target, caches can be located in global or shared memory. To explicitly choose the location of the cache, we write: <pre><code>AA = plan.cache(A, level=4, location=v100.MemorySpace.SHARED)\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/#double-buffering","title":"Double buffering","text":"<p>Caches can double-buffer data by loading the next active block's cache data into a temporary buffer during the current active block's usage and then moving that data into the cache buffer after the current active block is done being used. If the cache trigger level is the highest level in the loopnest then this does nothing as it is dependent on having another loop outside of the cache trigger loop. In shared memory caches on GPU this temporary buffer will automatically be allocated in private memory. Since the next iteration's data is loaded into a temporary buffer while the current iteration's data is in the cache buffer, any overlap in these active blocks would result in a write coherency issue similar to what occurs with Multicaching. Because of this, <code>double_buffer</code> may only be specified on an <code>INPUT</code> or <code>CONST</code> array as Accera does not perform multicache write coherence. <pre><code>AA = plan.cache(A, level=3, double_buffer=True)\n</code></pre></p> <p>Full schedule with equivalent pseudo-code: <pre><code>...\nM, N, K = 1024, 1024, 1024\nm_tile, n_tile, k_tile = 32, 64, 128\nnest = Nest((M, N, K))\ni, j, k = nest.get_indices()\n@nest.iteration_logic\ndef _():\n    C[i,j] += A[i,k] * B[k,j]\nschedule = nest.create_schedule()\nschedule.tile({\n    i: m_tile,\n    j: n_tile,\n    k: k_tile\n})\nschedule.reorder(i, j, k, ii, jj, kk)\n\nplan = schedule.create_plan()\nplan.cache(A, index=ii, double_buffer=True)\n...\n</code></pre> equivalent to: <pre><code>for i in range(0, M, m_tile):\n    for j in range(0, N, n_tile):\n        for ii_cache in range(0, m_tile):\n            for kk_cache in range(0, k_tile):\n                cache_A[ii_cache, kk_cache] = A[i+ii_cache, kk_cache]\n        for k in range(0, K-k_tile, k_tile): # Note: this loop doesn't run for the final K tile\n            for ii_cache in range(0, m_tile):\n                for kk_cache in range(0, k_tile):\n                    temp_A[ii_cache, kk_cache] = A[i+ii_cache, (k + k_tile) + kk_cache]\n            for ii in range(0, m_tile):\n                for jj in range(0, n_tile):\n                    for kk in range(0, k_tile):\n                        C[i+ii, j+jj] += cache_A[ii, kk] * B[k+kk, j+jj]\n            for ii_cache in range(0, m_tile):\n                for kk_cache in range(0, k_tile):\n                    cache_A[ii_cache, kk_cache] = temp_A[ii_cache, kk_cache]\n        for ii in range(0, m_tile):\n            for jj in range(0, n_tile):\n                for kk in range(0, k_tile):\n                    C[i+ii, j+jj] += cache_A[ii, kk] * B[k+kk, j+jj]\n</code></pre></p>"},{"location":"Manual/06%20Plans%20-%20Caching/#caching-strategy","title":"Caching strategy","text":"<p>In GPUs, the mapping between threads and data can be controlled by specifying the <code>strategy</code> option. Currently, Accera supports <code>BLOCKED</code> and <code>STRIPED</code> access patterns and they are explained in detail at <code>accera.CacheStrategy</code>. The choice of which pattern to use will depend on the hardware architecture and the intended algorithm the cache will be used for, since different access patterns incur different performance overhead.</p> <pre><code>AA = plan.cache(A, level=3, double_buffer=True, strategy=CacheStrategy.BLOCKED)\n</code></pre> <p>The above example will create a cache where each thread copies a contiguous chunk (block) of elements based on their thread index.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/","title":"Section 7: Plans - Operations and Optimizations","text":"<p>We can control target-specific operations and optimizations using a plan. Examples include instruction pipelining, applying SIMD vector instructions, and so on.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#unroll","title":"<code>unroll</code>","text":"<p>By default, each dimension of the iteration space is implemented as a for-loop. The <code>unroll</code> instruction marks a dimension for unrolling rather than looping. Imagine the following nest that multiplies the entries of an array by a constant: <pre><code>import accera as acc\n\nmy_target = acc.Target(type=acc.Target.Category.CPU)\n\nnest = acc.Nest(shape=(3,5))\ni, j = nest.get_indices()\n\n@nest.iteration_logic\ndef _():\n    A[i, j] *= 2.0\n\nplan = nest.create_plan(my_target)\n</code></pre> If we build <code>plan</code> as is, the resulting implementation would be equivalent to the following Python code: <pre><code>for i in range(3):\n    for j in range(5):\n        A[i, j] *= 2.0\n</code></pre> If we add the instruction <code>plan.unroll(index=j)</code>, the resulting implementation becomes equivalent to: <pre><code>for i in range(3):\n    A[i, 0] *= 2.0\n    A[i, 1] *= 2.0\n    A[i, 2] *= 2.0\n    A[i, 3] *= 2.0\n    A[i, 4] *= 2.0\n</code></pre> If, instead of unrolling <code>j</code>, we add the instruction <code>plan.unroll(index=i)</code>, the resulting implementation becomes equivalent to: <pre><code>for j in range(5):\n    A[0, j] *= 2.0\nfor j in range(5):\n    A[1, j] *= 2.0\nfor j in range(5):\n    A[2, j] *= 2.0\n</code></pre> And, of course, we can also unroll both dimensions, removing for-loops completely.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#vectorize","title":"<code>vectorize</code>","text":"<p>Modern target platforms support SIMD vector instructions. These instructions perform the same operation on an entire vector of elements, all at once. By default, each dimension of an iteration space becomes a for-loop. The <code>vectorize</code> instruction instead labels a dimension for vectorized execution, rather than for-looping.</p> <p>For example, assume that a host supports 256-bit vector instructions, indicating that its vector instructions operate on eight floating-point elements at once. Also, consider that we already have arrays <code>A</code>, <code>B</code>, and <code>C</code>, and we write the following code: <pre><code>nest = acc.Nest(shape=(64,))\ni = nest.get_indices()\n\n@nest.iteration_logic\ndef _():\n    C[i] = A[i] * B[i]\n\nschedule = nest.create_schedule()\nii = schedule.split(i, 8)\n\nplan = nest.create_plan()\nplan.vectorize(index=ii)\n</code></pre> The dimension marked for the vectorization is of size 8, which is a supported vector size on the specific target platform. Therefore, the resulting binary will contain something like: <pre><code>  00000001400010B0: C5 FC 10 0C 11     vmovups     ymm1,ymmword ptr [rcx+rdx]\n  00000001400010B5: C5 F4 59 0A        vmulps      ymm1,ymm1,ymmword ptr [rdx]\n  00000001400010B9: C4 C1 7C 11 0C 10  vmovups     ymmword ptr [r8+rdx],ymm1\n  00000001400010BF: 48 8D 52 20        lea         rdx,[rdx+20h]\n  00000001400010C3: 48 83 E8 01        sub         rax,1\n  00000001400010C7: 75 E7              jne         00000001400010B0\n</code></pre> Note how the multiplication instruction vmulps and the memory move instruction vmovups deal with eight 32-bit floating-point values at a time.</p> <p>Different targets support different vector instructions having different vector sizes. The following table includes iteration logic that vectorizes correctly on most targets with vectorization support, such as Intel Haswell, Broadwell or newer, and ARM v7/A32. Other examples of iteration logic may or may not vectorize correctly. Variables prefixed with v are vector types, and those prefixed with s are scalar types.</p> Vector pseudocode Equivalent to Supported types <code>v1 += s0 * v0</code> <code>for i in range(vector_size):</code> <code>v1[i] += s0 * v0[i]</code> float32 <code>v1 += v0 * s0</code> <code>for i in range(vector_size):</code> <code>v1[i] += v0[i] * s0</code> float32 <code>v1 += v0 / s0</code> <code>for i in range(vector_size):</code> <code>v1[i] += v0[i] / s0</code> float32 <code>v1 -= s0 * v0</code> <code>for i in range(vector_size):</code> <code>v1[i] -= s0 * v0[i]</code> float32 <code>v1 -= v0 * s0</code> <code>for i in range(vector_size):</code> <code>v1[i] -= v0[i] * s0</code> float32 <code>v1 -= v0 / s0</code> <code>for i in range(vector_size):</code> <code>v1[i] -= v0[i] / s0</code> float32 <code>v2 += v0 * v1</code> <code>for i in range(vector_size):</code> <code>v2[i] += v0[i] * v1[i]</code> float32 vector inner (dot) product: <code>s0 += dot(v0, v1)</code> <code>for i in range(vector_size):</code> <code>s0 += v0[i] * v1[i]</code> float32 <code>v2 = v0 + v1</code> <code>for i in range(vector_size):</code> <code>v2[i] = v0[i] + v1[i]</code> int8/16/32/64, float32 <code>v2 = v0 - v1</code> <code>for i in range(vector_size):</code> <code>v2[i] = v0[i] - v1[i]</code> int8/16/32/64, float32 <code>v2 = v0 * v1</code> <code>for i in range(vector_size):</code> <code>v2[i] = v0[i] * v1[i]</code> int8/16/32/64, float32 <code>v2 = v0 / v1</code> <code>for i in range(vector_size):</code> <code>v2[i] = v0[i] / v1[i]</code> float32 <code>v1 = abs(v[0])</code> <code>for i in range(vector_size):</code> <code>v1[i] = abs(v0[i])</code> int8/16/32/64, float32 <code>v2 = (v0 == v1)</code> <code>for i in range(vector_size):</code> <code>v2[i] = 0XF..F if v0[i] == v1[i] else 0</code> int8/16/32/64, float32 <code>v2 = (v0 &gt; v1)</code> <code>for i in range(vector_size):</code> <code>v2[i] = 0XF..F if v0[i] &gt; v1[i] else 0</code> int8/16/32/64, float32 <code>v2 = (v0 &gt;= v1)</code> <code>for i in range(vector_size):</code> <code>v2[i] = 0XF..F if v0[i] &gt;= v1[i] else 0</code> int8/16/32/64, float32 <code>v2 = (v0 &lt; v1)</code> <code>for i in range(vector_size):</code> <code>v2[i] = 0XF..F if v0[i] &lt; v1[i] else 0</code> int8/16/32/64, float32 <code>v2 = (v0 &lt;= v1)</code> <code>for i in range(vector_size):</code> <code>v2[i] = 0XF..F if v0[i] &lt;= v1[i] else 0</code> int8/16/32/64, float32 <code>v1 = v0 &lt;&lt; s0</code> <code>for i in range(vector_size):</code> <code>v1[i] = v0[i] &lt;&lt; s0</code> int16/32/64, float32 <code>v1 = v0 &gt;&gt; s0</code> <code>for i in range(vector_size):</code> <code>v1[i] = v0[i] &gt;&gt; s0</code> int16/32/64, float32 <code>s0 = sum(v0)</code> <code>for i in range(vector_size):</code> <code>s0 += v0[i]</code> int8/16/32/64, float32 <code>s0 = max(v0 + v1)</code> <code>for i in range(vector_size):</code> <code>s0 = max(v0[i] + v1[i], s0)</code> int8/16/32/64, float32 <code>s0 = max(v0 - v1)</code> <code>for i in range(vector_size):</code> <code>s0 = max(v0[i] - v1[i], s0)</code> int8/16/32/64, float32 <p>Additionally, Accera can perform vectorized load and store operations to/from vector registers and memory if the memory locations are contiguous.</p> <p>To vectorize dimension <code>i</code>, the number of active elements that corresponds to dimension <code>i</code> must exactly match the vector instruction width of the target processor. For example, if the target processor has vector instructions that operate on either 4 or 8 floating-point elements at once, then the number of active elements can either be 4 or 8. Additionally, those active elements must occupy adjacent memory locations (they cannot be spread out).</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#tensorize","title":"<code>tensorize</code>","text":"<p>Some hardware also have specialized instructions for performing matrix multiplications. These instructions operate on certain matrix dimensions with specific data types. The tensorization instructions take tiles of the <code>A</code>, <code>B</code>, and <code>C</code> matrices and compute the <code>C = A * B + C</code> operation.</p> <p>The <code>tensorize</code> operation takes 3 indices and a tensorization shape (MMA shape) along with some tuning parameters:</p> <pre><code>plan.tensorize(indices=(i,j,k), mma_shape=MMAShape.M16xN16xK4_B1)\n</code></pre> <p>Tensorization is limited and is only valid on loop structures of the form</p> <pre><code>for i in range(M):\n    for k in range(N):\n        for j in range(K):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre> <p>Where there is <code>MxNxK</code> tensorization hardware support using the <code>A</code>, <code>B</code>, and <code>C</code> element data types. Tensorization support for GPUs in Accera is explained in more detail here.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#convenience-syntax-kernelize","title":"Convenience syntax: <code>kernelize</code>","text":"<p>The <code>kernelize</code> instruction is a convenience syntax that does not provide any unique functionality. Specifically, <code>kernelize</code> is equivalent to a sequence of <code>unroll</code> instructions, followed by an optional <code>vectorize</code> instruction.</p> <p>A typical Accera design pattern is to first break a loop-nest into tiles and then apply an optimized kernel to each tile. For example, imagine that the loop nest multiplies two 256\u00d7256 matrices and the kernel is a highly optimized procedure for multiplying 4\u00d74 matrices. Accera will introduce different ways to write highly optimized kernels in the future. However, currently, it only supports automatic kernelization using the <code>kernelize</code> instruction. As mentioned above, <code>kernelize</code> is shorthand for unrolling and vectorizing. These instructions structure the code in a way that makes it easy for downstream compiler heuristics to automatically generate kernels.</p> <p>Consider, once again, the matrix multiplication example we discussed previously in Section 2. Assume that we declare the schedule and reorder as follows:</p> <p><pre><code>schedule = nest.create_schedule()\nschedule.reorder(i, k, j)\n</code></pre> Notice that <code>i, k, j</code> are the last three dimensions in the iteration space and the resulting implementation becomes equivalent to:</p> <pre><code>for i in range(M):\n    for k in range(S):\n        for j in range(N):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre> <p>The instruction: <pre><code>plan.kernelize(unroll_indices=(i, k), vectorize_indices=j)\n</code></pre> is just shorthand for <pre><code>plan.unroll(i)\nplan.unroll(k)\nplan.vectorize(j)\n</code></pre> Applying this sequence of instructions allows the compiler to automatically create an optimized kernel from loops <code>i, k, j</code>.</p> <p>For simplicity, assume that the matrix sizes defined by M, N, and S are 3, 4, and 2 respectively.</p> <p>After applying <code>kernelize</code>, the schedule is equivalent to the following Python code: <pre><code>C[0,0:4] += A[0,0] * B[0,0:4] # vectorized\nC[0,0:4] += A[0,1] * B[1,0:4] # vectorized\nC[1,0:4] += A[1,0] * B[0,0:4] # vectorized\nC[1,0:4] += A[1,1] * B[1,0:4] # vectorized\nC[2,0:4] += A[2,0] * B[0,0:4] # vectorized\nC[2,0:4] += A[2,1] * B[1,0:4] # vectorized\n</code></pre></p> <p>This would result in the following vectorized instructions on an Intel Haswell CPU: <pre><code>  0000000000000200: C4 C1 78 10 00     vmovups     xmm0,xmmword ptr [r8]\n  0000000000000205: C4 E2 79 18 09     vbroadcastss xmm1,dword ptr [rcx]\n  000000000000020A: C5 F8 10 12        vmovups     xmm2,xmmword ptr [rdx]\n  000000000000020E: C4 E2 69 A8 C8     vfmadd213ps xmm1,xmm2,xmm0\n  0000000000000213: C5 F8 10 5A 10     vmovups     xmm3,xmmword ptr [rdx+10h]\n  0000000000000218: C4 E2 79 18 61 04  vbroadcastss xmm4,dword ptr [rcx+4]\n  000000000000021E: C4 E2 61 A8 E1     vfmadd213ps xmm4,xmm3,xmm1\n  0000000000000223: C4 E2 79 18 49 08  vbroadcastss xmm1,dword ptr [rcx+8]\n  0000000000000229: C4 E2 69 A8 C8     vfmadd213ps xmm1,xmm2,xmm0\n  000000000000022E: C4 E2 79 18 69 0C  vbroadcastss xmm5,dword ptr [rcx+0Ch]\n  0000000000000234: C4 E2 61 A8 E9     vfmadd213ps xmm5,xmm3,xmm1\n  0000000000000239: C4 E2 79 18 49 10  vbroadcastss xmm1,dword ptr [rcx+10h]\n  000000000000023F: C4 E2 69 A8 C8     vfmadd213ps xmm1,xmm2,xmm0\n  0000000000000244: C4 E2 79 18 41 14  vbroadcastss xmm0,dword ptr [rcx+14h]\n  000000000000024A: C4 E2 61 A8 C1     vfmadd213ps xmm0,xmm3,xmm1\n  000000000000024F: C4 C1 58 58 09     vaddps      xmm1,xmm4,xmmword ptr [r9]\n  0000000000000254: C4 C1 50 58 51 10  vaddps      xmm2,xmm5,xmmword ptr [r9+10h]\n  000000000000025A: C4 C1 78 58 41 20  vaddps      xmm0,xmm0,xmmword ptr [r9+20h]\n</code></pre></p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#parallelize","title":"<code>parallelize</code>","text":"<p>The <code>parallelize</code> instruction performs one or more loops in parallel on multiple cores.</p> <p><pre><code>xeonPlat = acc.Target(\"Intel 9221\", num_threads=16)\nplan = schedule.create_plan(xeonPlat)\nplan.parallelize(indices=(i,j,k))\n</code></pre> Specifying multiple dimensions is equivalent to the <code>collapse</code> argument in OpenMP. Therefore, the dimensions must be contiguous in the iteration space dimension order.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#static-scheduling-policy","title":"Static scheduling policy","text":"<p>Static scheduling strategy is invoked by setting the argument <code>policy=\"static\"</code> in the call to <code>parallelize</code>. If n iterations are parallelized across c cores, the static scheduling partitions the work into c fixed parts, some of size floor(n/c), some of size ceil(n/c), and executes each part on a different core.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#dynamic-scheduling-policy","title":"Dynamic scheduling policy","text":"<p>Dynamic scheduling strategy is invoked by setting the argument <code>policy=\"dynamic\"</code> in the call to <code>parallelize</code>. Dynamic scheduling creates a single work queue that is shared across different cores.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#specifying-thread-limit","title":"Specifying thread limit","text":"<p>Setting the argument <code>max_threads</code> to a positive integer value will tell the compiler to have an upper bound on the number of threads used for distributing the workload.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#not-yet-implemented-pinning-to-specific-cores","title":"Not yet implemented: Pinning to specific cores","text":"<p>The <code>pin</code> argument allows the parallel work to be pinned to specific cores.</p>"},{"location":"Manual/07%20Plans%20-%20Operations%20and%20Optimizations/#bind","title":"<code>bind</code>","text":"<p>Some target platforms, such as GPUs, are specifically designed to execute nested loops. They can take an entire grid of work and schedule its execution on multiple cores. On a GPU, this grid is broken up into multiple blocks, where each block contains multiple threads. Block iterators and thread iterators are identified by special variables in the <code>Target</code> object. To take advantage of a target platform's ability to execute grids, we must bind dimensions of the iteration space with these special iterator variables.</p> <p>For example, <pre><code>v100 = acc.Target(\"Tesla V100\")\nplan.bind(mapping={\n        i: v100.GridUnit.BLOCK_X,\n        j: v100.GridUnit.THREAD_X,\n        k: v100.GridUnit.THREAD_Y\n    }\n)\n</code></pre></p>"},{"location":"Manual/08%20Deferred%20Layout%20of%20Constant%20Arrays/","title":"Section 8: Deferred layout of constant arrays","text":"<p>Let's revisit the memory layout of constant arrays. As explained in Section 1, the contents of constant arrays are known at compile-time, and these contents are immutable. Accera stores constant arrays in a non-standard memory layout optimized for a particular plan. In some cases, storing multiple copies of each array element may even prove advantageous (e.g., storing a matrix in row-major and column-major layouts).</p>"},{"location":"Manual/08%20Deferred%20Layout%20of%20Constant%20Arrays/#deferred-layout-based-on-a-cache","title":"Deferred layout based on a cache","text":"<p>Accera's cache strategy creates local copies of an array's active blocks. The constant array can be arranged based on the defined cache. Specifically, the array is stored by serializing the active blocks consecutively. If the caching strategy is <code>thrifty=True</code>, the active blocks are ready to use without copying the data.   </p> <p>To define an array layout based on a cache, Accera DSL has to overcome the chicken-and-egg paradox. While on the one hand, arrays need to be defined even before the nest logic. On the other hand, array layout depends on a cache, which is defined only as a part of a plan. In Accera, we overcome this situation by splitting the array definition into two parts. Though we still define the constant array upfront, we avoid committing to a specific layout.  <pre><code>import accera as acc\nimport numpy as np\n\nmatrix = np.random.rand(16, 16)\nA = acc.Array(role=acc.Role.CONST, data=matrix, layout=acc.Array.Layout.DEFERRED)\n</code></pre> Now we define the nest logic, the schedule, and the plan. Consider that we define a plan named <code>plan</code> and use this plan to define a cache <code>A</code> based on dimension <code>i</code>: <pre><code>AA = plan.cache(A, i, layout=acc.Array.Layout.FIRST_MAJOR, thrifty=True)\n</code></pre> We can now use the cache <code>AA</code> to determine the layout of the original array <code>A</code>: <pre><code>A.deferred_layout(cache=AA)\n</code></pre></p>"},{"location":"Manual/09%20Parameters/","title":"Section 9: Parameters","text":"<p>Accera's parameters are placeholders that get replaced with concrete values when adding a function to a package. A parameter can be used in a <code>Nest</code>, a <code>Schedule</code>, or a <code>Plan</code>.</p>"},{"location":"Manual/09%20Parameters/#parameterized-nests","title":"Parameterized nests","text":"<p>Recall that a <code>Nest</code> represents the loop-nest logic. We can parameterize the nest's shape and iteration logic. For example, consider the following parameterized version of matrix multiplication:</p> <p><pre><code># Create parameters\nP0, P1, P2, P3 = acc.create_parameters()\n\nA = acc.Array(role=acc.Role.INPUT, shape=(P0, P2))\nB = acc.Array(role=acc.Role.INPUT, shape=(P2, P1))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, shape=(P0, P1))\n\n# Define a simple nest\nnest = acc.Nest(shape=(P0, P1, P2))\ni, j, k = nest.get_indices()\n\n# Define the loop nest logic and add it to the nest\n@nest.iteration_logic\ndef _():\n    C[i, j] += P3 * A[i, k] * B[k, j]\n\n# create a package\npackage = acc.Package()\n\n# Use the templated nest to add two different functions to the package\npackage.add(nest, args=(A, B, C), parameters={P0:16, P1:16, P2:16, P3:1.0}, base_name=\"matmul_16_16_16_1\")\npackage.add(nest, args=(A, B, C), parameters={P0:32, P1:32, P2:32, P3:2.0}, base_name=\"matmul_32_32_32_2\")\n</code></pre> In the above scenario, the shape of the nest is parameterized by (<code>P0</code>, <code>P1</code>, <code>P2</code>) and its iteration logic includes the parameter <code>P3</code>. The nest is used twice with different settings of these parameters to create two separate functions in the package.</p>"},{"location":"Manual/09%20Parameters/#parameterized-schedules-and-plans","title":"Parameterized schedules and plans","text":"<p>Parameters can also appear in schedules and plans. For example, we can add the following code snippet:</p> <pre><code>P4, P5 = acc.create_parameters()\n\n# Create a parameterized schedule\nschedule = nest.create_schedule()\nii = schedule.split(i, size=P4)\n\n# Create a parameterized plan\nplan = schedule.create_plan()\nplan.cache(A, level=P5)\n\n# Add another function to the package\npackage.add(plan, args=(A, B, C), parameters={P0:16, P1:16, P2:16, P3:1.0, P4:4, P5:2}, base_name=\"alternative_matmul_16_16_16\")\n</code></pre>"},{"location":"Manual/09%20Parameters/#supported-operations","title":"Supported operations","text":"<p>Accera's parameters support the basic arithmetic operations and other relational/bitwise/intrinsics operations. For example, we can add the following code snippet instead:</p> <pre><code>fma_unit_count, vector_size, P5 = acc.create_parameters()\n\n# Create a parameterized schedule\nschedule = nest.create_schedule()\nii = schedule.split(i, size=fma_unit_count * vector_size)\niii = schedule.split(ii, size=vector_size)\n\n# Create a parameterized plan\nplan = schedule.create_plan()\nplan.cache(A, level=P5)\n\n# Add another function to the package\npackage.add(plan, args=(A, B, C), parameters={P0:16, P1:16, P2:16, P3:1.0, fma_unit_count:4, vector_size:16, P5:2}, base_name=\"alternative_matmul_16_16_16\")\n</code></pre> <p>The supported operations include the following operations:</p>"},{"location":"Manual/09%20Parameters/#arithmetic-operators","title":"Arithmetic operators","text":"Operation Types Description <code>a + b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the sum of parameters (or parameter and scalar) a and b <code>a - b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the difference between parameters (or parameter and scalar) a and b <code>a * b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the product of parameters (or parameter and scalar) a and b <code>a / b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the quotient of parameters (or parameter and scalar) a and b <code>a ** b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the b'th power of parameter a <code>a // b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the floor of the quotient of parameters (or parameter and scalar) a and b <code>a % b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the signed remainder after dividing parameter a by parameter or scalar b <code>-a</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns the additive inverse of parameter a"},{"location":"Manual/09%20Parameters/#comparison-operations","title":"Comparison Operations","text":"Operation Types Description <code>a == b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if parameter or scalar a equals parameter or scalar b, else False <code>a != b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if parameter or scalar a is not equal to parameter or scalar b, else False <code>a &lt; b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if parameter or scalar a is strictly smaller than parameter or scalar b, else False <code>a &lt;= b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if parameter or scalar a is smaller than or equal to parameter or scalar b, else False <code>a &gt; b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if parameter or scalar a is strictly greater than parameter or scalar b, else False <code>a &gt;= b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64, acc.ScalarType.float16/32/64</code> Returns True if parameter or scalar a is greater than or equal to parameter or scalar b, else False"},{"location":"Manual/09%20Parameters/#bitwise-operators","title":"Bitwise operators","text":"Operation Types Description <code>a &amp; b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise AND of the bits in parameters (or parameter and scalar) a and b <code>a \\| b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise OR of the bits in parameters (or parameter and scalar) a and b <code>a ^ b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise XOR of the bits in parameters (or parameter and scalar) a and b <code>~a</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns the bitwise inverse of the bits in parameter a <code>a &lt;&lt; b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns parameter a whose bitwise representation is shifted left by b bits <code>a &gt;&gt; b</code> <code>acc.DelayedParameter, acc.ScalarType.int8/16/32/64, acc.ScalarType.uint8/16/32/64</code> Returns parameter a whose bitwise representation is shifted right by b bits"},{"location":"Manual/09%20Parameters/#intrinsics","title":"Intrinsics","text":"Operation Types Description <code>acc.abs(a)</code> <code>acc.ScalarType.float16/32/64</code> Returns the absolute value of parameter a"},{"location":"Manual/09%20Parameters/#tuple-parameter-values","title":"Tuple parameter values","text":"<p>Parameters can be used as placeholders for tuples, specifically for tuples of indices. For example, assume that we want to parameterize the order of the iteration space dimensions. We can then write: <pre><code>P6 = acc.create_parameters()\nschedule.reorder(order=P6)\n</code></pre> Later, we can set the value of <code>P6</code> to the index tuple <code>(j,k,i)</code>.</p>"},{"location":"Manual/09%20Parameters/#create-parameters-from-an-entire-parameter-grid","title":"Create parameters from an entire parameter grid","text":"<p>Consider the parameterized nest defined above. Rather than setting a specific value for each parameter, imagine that we have a set of different values for each parameter. For example, consider that we want <code>P0</code> to have a value in set <code>{8, 16}</code>, <code>P1</code> in <code>{16, 32}</code>, <code>P2</code> to be always <code>16</code>, and <code>P3</code> in <code>{1,2}</code>. We can define the parameter grid with this data, which lists all the valid parameter combinations. In our case, this grid includes the following parameter settings: <pre><code>{P0:8, P1:16, P2:16, P3:1.0}\n{P0:8, P1:16, P2:16, P3:2.0}\n{P0:8, P1:32, P2:16, P3:1.0}\n{P0:8, P1:32, P2:16, P3:2.0}\n{P0:16, P1:16, P2:16, P3:1.0}\n{P0:16, P1:16, P2:16, P3:2.0}\n{P0:16, P1:32, P2:16, P3:1.0}\n{P0:16, P1:32, P2:16, P3:2.0}\n</code></pre></p> <p>Accera provides an easy way to add all the functions that correspond to the parameter grid at once: <pre><code>parameters = create_parameter_grid(parameter_choices={P0:[8,16], P1:[16,32], P2:[16], P3:[1.0,2.0])\npackage.add(nest, args=(A, B, C), base_name=\"matmul\", parameters)\n</code></pre> In this case, <code>package.add</code> generates a function eight times, once for each parameter combination in the grid.  Other than <code>nest</code>, <code>package.add</code> can alternatively accept a <code>Schedule</code> (if we are performing schedule transformations), or a <code>Plan</code> (if we are setting target-specific options). All eight functions share the same base name. However, Accera automatically adds a unique suffix to each function name to prevent duplication. This pattern allows optional filtering by inspecting the generated parameter values list before calling <code>package.add</code>.</p> <p>You can define a lambda or function to filter out combinations from the parameter grid. The arguments to the filter are the values of a parameter combination, and it should return <code>True</code> if the combination should be included, and <code>False</code> otherwise: <pre><code>parameters = create_parameter_grid(parameter_choices={P0:[8,16], P1:[16,32], P2:[16], P3:[1.0,2.0]}, filter_func=lambda p0, p1, p2, p3: p2 &lt; p1 and 4 * (p0 * p3 + p1 * p2 + p1 * p3 + p2 * p3) / 1024 &lt; 256)\n</code></pre></p> <p>To limit the size of the parameter grid (and therefore the number of functions generated) to at most 5: <pre><code>parameters = create_parameter_grid(parameter_choices={P0:[8,16], P1:[16,32], P2:[16], P3:[1.0,2.0]}, sample=5)\n</code></pre></p> <p>If the parameter is a loop order which is a list or tuple of indices, <code>create_parameter_grid</code> can generate all the permutations of loop order. Furthermore, you can pass in a filter function to filter out invalid loop orders: <pre><code>parameters = create_parameter_grid({P0:(i, j, k, ii, jj, kk)}, filter_func = lambda *p : schedule.is_valid_loop_order(p[0][0]))\n</code></pre></p> <p><code>Schedule.is_valid_loop_order()</code> is a pre-defined filter function that determines if a given loop order is valid for that schedule.</p> <p>Note that the order of the list or tuple of indices provided to <code>create_parameter_grid</code> does not matter.</p> <p>To filter parameters with more complicated logic, you can define your own filter function that wraps <code>Schedule.is_valid_loop_order()</code>:</p> <pre><code>def my_filter(parameters_choice):\n    P1, P2, P3, P4, P5, loop_order = parameters_choice\n\n    return P1 &gt; P2 \\\n        and P3 &gt; P4 \\\n        and P1 * P5 &lt; P3 \\\n        and P2 * P5 &lt; P4 \\\n        and schedule.is_valid_loop_order(loop_order)\n\n parameters = acc.create_parameter_grid({\n        P1: [64, 128, 256],\n        P2: [32, 128], \n        P3: [16, 32, 128],\n        P4: [8, 64],\n        P5: [4],\n        loop_order: (i, j, k, ii, jj, kk)\n    }, my_filter)\n</code></pre>"},{"location":"Manual/10%20Packages/","title":"Section 10: Building Packages","text":"<p>The <code>Package</code> class represents a collection of Accera-generated functions. Whenever a package is built, it creates a stand-alone function library that other pieces of software can use. Currently, Accera supports two package formats: HAT and MLIR.</p>"},{"location":"Manual/10%20Packages/#hat-package-format","title":"HAT package format","text":"<p>HAT \"Header Annotated with TOML\" is a format for packaging compiled libraries in the C programming language. HAT implies that a standard C header is styled with useful metadata in the TOML markup language.</p> <p>Consider a nest that holds some loop-nest logic. To build a HAT package containing a function with this logic for the Windows operating system, we write the following lines of code:  <pre><code>package = acc.Package()\npackage.add(nest, args=(A, B), base_name=\"myFunc\")\npackage.build(format=acc.Package.Format.HAT_DYNAMIC, name=\"myPackage\", platform=acc.Package.Platform.WINDOWS)\n</code></pre></p> <p>The result is two files: <code>myPackage.hat</code> and <code>myPackage.dll</code>. The output directory defaults to current working directory. We can change the output directory with <code>output_dir</code> set to a relative or absolute path:</p> <pre><code>package.build(format=acc.Package.Format.HAT_DYNAMIC, name=\"myPackage\", platform=acc.Package.Platform.WINDOWS, output_dir=\"hat_packages\")\n</code></pre>"},{"location":"Manual/10%20Packages/#mlir-package-format","title":"MLIR package format","text":"<p>MLIR format is used for debugging the multiple stages of MLIR lowering, from the Accera DSL all the way to runnable code. <pre><code>package.build(format=acc.Package.Format.MLIR, name=\"myPackage\")\n</code></pre></p>"},{"location":"Manual/10%20Packages/#function-names-in-packages","title":"Function names in packages","text":"<p>We can specify the base name of a function when it is added to a package. The full function name is the base name followed by an automatically generated unique identifier. For example, if the base name is \"myFunc\" then the function name could be \"myFunc_8f24bef5\". If no base name is defined, the automatically-generated unique identifier becomes the function name.</p> <p>The unique identifier ensures that no two functions share the same name. However, invoking the function from the client code becomes cumbersome because the function name changes each time the Accera package is updated and rebuilt. Therefore, the HAT file includes the client code to call the function without the unique identifier. Concretely, if the function signature in C is: <pre><code>void myFunc_8f24bef5(const float* A, float* B);\n</code></pre> then the HAT file also contains the line: <pre><code>void (*myFunc)(const float* A, float* B) = myFunc_8f24bef5;\n</code></pre> The above code makes the abbreviated name <code>myFunc</code> an alias of the full function name <code>myFunc_8f24bef5</code>. If multiple functions share the same base name, the first function in the HAT file gets the alias.</p>"},{"location":"Manual/10%20Packages/#debug-mode","title":"Debug mode","text":"<p>A package can be built with<code>mode=acc.Package.Mode.DEBUG</code>. Doing so creates a special version of each function that validates its own correctness every time the function is called. From the outside, a debugging package looks identical to a standard package. However, each of its functions actually contains two different implementations: the Accera implementation (with all of the fancy scheduling and planning) and the trivial default implementation (without any scheduling or planning). When called, the function runs both implementations and asserts that their outputs are within the predefined tolerance. If the outputs don't match, the function prints error messages to <code>stderr</code>. <pre><code>package.build(format=acc.Package.Format.HAT_DYNAMIC, name=\"myPackage\", mode=acc.Package.Mode.DEBUG, tolerance=1.0e-6)\n</code></pre></p> <p>Not yet implemented: Debug mode is not supported for GPU targets.</p>"},{"location":"Manual/10%20Packages/#adding-descriptions","title":"Adding descriptions","text":"<p>Accera allows us to specify some standard descriptive fields in a package: <pre><code>package.add_description(version=\"1.0\", license=\"https://mit-license.org/\", author=\"Microsoft Research\")\n</code></pre> Additionally, we can add arbitrary metadata to the package description as follows: <pre><code>package.add_description(other={\"title\": \"My Package Title\", \"source\": \"https://github.com/\", \"citations\": [\"https://arxiv.org/2021.12345/\", \"https://arxiv.org/2021.56789/\"]})\n</code></pre></p>"},{"location":"Manual/11%20Plans%20-%20GPU%20Tensorization/","title":"Section 11: Plans - GPU Tensorization","text":"<p>In this section we will look more closely at how we can utilize tensor cores on supported GPUs to accelerate matrix multiplication operations.</p>"},{"location":"Manual/11%20Plans%20-%20GPU%20Tensorization/#related-concepts","title":"Related Concepts","text":"<p>Since tensor cores on the GPU can perform matrix multiplication of some standard shapes, we need to first familiarize ourselves with some of the associated terminology: - MMA shape - the smallest tensorizable matrix multiplication shape. In other words, nest of this shape or its multiple can be executed on tensor cores. Accera supports MMA shapes in the form of MmxNnxKk_Bb which performs matrix multiplication of shape {m, n, k}, i.e., <code>C</code> += <code>A</code> x <code>B</code>, where matrix <code>A</code> is of shape {m, k}, matrix <code>B</code> is of shape {k, n} and the result matrix <code>C</code> is of shape {m, n}. The MMA shape can be specified by setting the <code>mma_shape</code> parameter in the <code>plan.tensorize</code> function call. - Tensor pass - A single tensor pass refers to a single unit of tensor operation. For example, a single pass of the MMA shape <code>M16xN16xK4_B1</code> performs matrix multiplication of shape {16, 16, 4}, whereas 4 passes of the same MMA shape performs a matmul of shape {16, 16, 16} in 4 iterations (passes) where each pass performs a matmul of shape {16, 16, 4}. The number of passes can be controlled by setting the <code>num_total_passes</code> parameter in the <code>plan.tensorize</code> function call.</p>"},{"location":"Manual/11%20Plans%20-%20GPU%20Tensorization/#tuning-parameters","title":"Tuning parameters","text":"<ul> <li>Pass fusing/grouping - A group of passes can be fused together to control allocation of registers required for input data (<code>A</code> and <code>B</code> matrices) and memory I/O density during tensor matmul. This is explained in more detail in the Multi-Pass Tensorized MatMul with Pass Fusion tutorial.</li> <li>Scheduling policy - This parameter can be used to tune register usage for accumulator data (<code>C</code> matrix) for multi-block tensor shapes. This is explained in more detail in Tensor MatMul on GPU: Scheduling Policy experiments tutorial.</li> <li>Prologue/Epilogue Ops - These parameters can be set to perform element-wise ops before and after matmul operations on tensor cores in an optimized way. Examples of this usage is presented in the Tensor MatMul on GPU: Fused Element-wise Operations tutorial.</li> </ul>"},{"location":"Reference/accera/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/accera/#module-functions","title":"Module functions","text":"<ul> <li><code>accera.cast</code> <code>(value, type)</code></li> <li><code>accera.create_dimensions</code> <code>([role])</code></li> <li><code>accera.create_parameters</code> <code>()</code></li> <li><code>accera.create_parameter_grid</code> <code>(parameter_choices[, filter_func, sample, seed])</code></li> <li><code>accera.fuse</code> <code>(schedules[, partial])</code></li> </ul>"},{"location":"Reference/accera/#top-level-enumerations","title":"Top level enumerations","text":"<ul> <li><code>accera.CacheStrategy</code></li> <li><code>accera.ScalarType</code></li> <li><code>accera.MMAFragmentOp</code></li> <li><code>accera.MMASchedulingPolicy</code></li> <li><code>accera.MMAShape</code></li> <li><code>accera.Role</code></li> </ul>"},{"location":"Reference/accera/#classes","title":"Classes","text":""},{"location":"Reference/accera/#class-acceraarray","title":"<code>class accera.Array</code>","text":"<p>A multidimensional array of scalar elements.</p>"},{"location":"Reference/accera/#constructors","title":"Constructors","text":"<ul> <li><code>Array</code> <code>(role[, data, element_type, layout, offset, shape])</code></li> </ul>"},{"location":"Reference/accera/#enumerations","title":"Enumerations","text":"<ul> <li><code>accera.Array.Layout</code></li> </ul>"},{"location":"Reference/accera/#methods","title":"Methods","text":"<ul> <li><code>deferred_layout</code> <code>(layout)</code></li> <li><code>sub_array</code> <code>(offsets, shape[, strides])</code></li> <li><code>slice</code> <code>(sliced_dims, sliced_offsets)</code></li> </ul>"},{"location":"Reference/accera/#class-acceracache","title":"<code>class accera.Cache</code>","text":"<p>A local copy of an <code>Array</code> block.</p>"},{"location":"Reference/accera/#class-acceraindex","title":"<code>class accera.Index</code>","text":"<p>An index representing one of the loops in a <code>Nest</code> or one of the iteration-space dimensions of a <code>Schedule</code> or a <code>Plan</code>.</p>"},{"location":"Reference/accera/#class-acceranest","title":"<code>class accera.Nest</code>","text":"<p>The logic of a loop nest.</p>"},{"location":"Reference/accera/#constructors_1","title":"Constructors","text":"<ul> <li><code>Nest</code> <code>(shape)</code></li> </ul>"},{"location":"Reference/accera/#methods_1","title":"Methods","text":"<ul> <li><code>iteration_logic</code> <code>(logic)</code></li> <li><code>create_plan</code> <code>([target])</code></li> <li><code>create_schedule</code> <code>()</code></li> <li><code>get_indices</code> <code>()</code></li> </ul>"},{"location":"Reference/accera/#class-accerapackage","title":"<code>class accera.Package</code>","text":"<p>Represents a collection of functions that can be built and emitted for use in client code.</p>"},{"location":"Reference/accera/#constructors_2","title":"Constructors","text":"<ul> <li><code>Package</code> <code>()</code></li> </ul>"},{"location":"Reference/accera/#enumerations_1","title":"Enumerations","text":"<ul> <li><code>accera.Package.Format</code></li> <li><code>accera.Package.Mode</code></li> <li><code>accera.Package.Platform</code></li> </ul>"},{"location":"Reference/accera/#methods_2","title":"Methods","text":"<ul> <li><code>add_description</code> <code>([author, license, other, version])</code></li> <li><code>add</code> <code>(args, source[, base_name, parameters])</code></li> <li><code>build</code> <code>(name[, error_path, format, mode, os, tolerance])</code></li> </ul>"},{"location":"Reference/accera/#class-acceraparameter","title":"<code>class accera.Parameter</code>","text":"<p>A placeholder that can be used instead of concrete values when constructing or calling the methods of a <code>Nest</code>, <code>Schedule</code>, or <code>Plan</code>.</p>"},{"location":"Reference/accera/#class-acceraplan","title":"<code>class accera.Plan</code>","text":"<p>A scheduled (ordered) loop nest with target-specific implementation details.</p>"},{"location":"Reference/accera/#methods_3","title":"Methods","text":"<ul> <li><code>cache</code> <code>(source[, index, trigger_index, layout, level, trigger_level, max_elements,  thrifty, location, double_buffer, double_buffer_location, vectorize])</code></li> <li><code>bind</code> <code>(indices, grid)</code></li> <li><code>kernelize</code> <code>(unroll_indices[, vectorize_indices])</code></li> <li><code>parallelize</code> <code>(indices[, pin, policy, max_threads])</code></li> <li><code>tensorize</code> <code>(indices, mma_shape[, use_static_offsets, num_total_passes, num_fused_passes, scheduling_policy])</code></li> <li><code>unroll</code> <code>(index)</code></li> <li><code>vectorize</code> <code>(index)</code></li> </ul>"},{"location":"Reference/accera/#class-accerascalar","title":"<code>class accera.Scalar</code>","text":"<p>A scalar element.</p>"},{"location":"Reference/accera/#constructors_3","title":"Constructors","text":"<ul> <li><code>Scalar</code> <code>([value])</code></li> <li><code>Scalar</code> <code>([value, name, role])</code></li> <li><code>Scalar</code> <code>([element_type, role])</code></li> </ul>"},{"location":"Reference/accera/#class-acceradimension","title":"<code>class accera.Dimension</code>","text":"<p>A specialization of Scalar with <code>element_type</code> as ScalarType.index.</p>"},{"location":"Reference/accera/#constructors_4","title":"Constructors","text":"<ul> <li><code>Dimension</code> <code>([role])</code></li> <li><code>Dimension</code> <code>([name, role])</code></li> <li><code>Dimension</code> <code>([value, name, role])</code></li> </ul>"},{"location":"Reference/accera/#class-acceraschedule","title":"<code>class accera.Schedule</code>","text":"<p>A scheduled (ordered) loop nest with no target-specific implementation details.</p>"},{"location":"Reference/accera/#methods_4","title":"Methods","text":"<ul> <li><code>create_plan</code> <code>([target])</code></li> <li><code>pad</code> <code>(index, size)</code></li> <li><code>reorder</code> <code>(indices)</code></li> <li><code>skew</code> <code>(index, reference_index)</code></li> <li><code>split</code> <code>(index, size)</code></li> <li><code>tile</code> <code>(indices, sizes)</code></li> <li><code>get_indices</code> <code>()</code></li> </ul>"},{"location":"Reference/accera/#class-accerafusedschedule","title":"<code>class accera.FusedSchedule</code>","text":"<p>Child class of <code>class accera.Schedule</code> created as a result of fusing multiple schedules.</p>"},{"location":"Reference/accera/#methods-in-addition-to-the-inherited-functions-from-class-acceraschedule","title":"Methods (in addition to the inherited functions from <code>class accera.Schedule</code>)","text":"<ul> <li><code>get_fusing_index</code> <code>()</code></li> <li><code>get_fused_indices</code> <code>()</code></li> <li><code>get_unfused_indices</code> <code>()</code></li> </ul>"},{"location":"Reference/accera/#class-acceratarget","title":"<code>class accera.Target</code>","text":"<p>A target platform for the cross-compiler.</p>"},{"location":"Reference/accera/#constructors_5","title":"Constructors","text":"<ul> <li><code>Target</code> <code>([architecture, cache_lines, cache_sizes, category, extensions, family, frequency_GHz, model, name, num_cores, num_threads, turbo_frequency_GHz])</code></li> </ul>"},{"location":"Reference/accera/#enumerations_2","title":"Enumerations","text":"<ul> <li><code>accera.Target.Architecture</code></li> <li><code>accera.Target.Category</code></li> <li><code>accera.Target.Models</code></li> </ul>"},{"location":"Reference/safety_analysis/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/safety_analysis/#safety-analysis","title":"Safety Analysis","text":"<p>One of the most important features in Accera is to provide safety guarantees to preserve the underlying logic no matter how we transform the schedule. Not all Accera schedules are safe, but those that are safe are much easier to work with.</p>"},{"location":"Reference/safety_analysis/#order-invariant-schedules","title":"Order-invariant Schedules","text":"<p>Order-invariant schedules are always safe because Accera transformations never remove any iterations. They only change the order of the loop-nest iterations, or add empty iterations in the form of padding when necessary. Recall that a <code>Nest</code> represents a simple nest. A simple nest is assumed to be order-invariant, and therefore any schedule created by a call to <code>create_schedule()</code> is safe.</p>"},{"location":"Reference/safety_analysis/#safety-and-fusing","title":"Safety and Fusing","text":"<p>Fusing is another way to create a schedule (see Section 4 of the Accera manual). Say that we have a sequence of n schedules: <code>schedule0</code>, <code>schedule1</code>, ... and we partially fuse their first m dimensions. Namely: <pre><code>schedule = acc.fuse((schedule0, schedule1, ...), partial=m)\n</code></pre> At this point, <code>schedule</code> is equivalent to sequentially executing the individual schedules for each iteration of the fused dimensions. However, is the fused <code>schedule</code> safe? In other words, does <code>schedule</code> guarantee the preservation of underlying logic, regardless of the applied transformation?</p> <p>The dimensions of <code>schedule</code> fall into three categories:</p> <ul> <li>Fused dimensions: at first, this category contains the next m dimensions of <code>schedule</code>. If any of these dimensions are split, the derived dimensions are also added to this category.</li> <li>Fusing dimensions: at first, this category contains a single dimension, the first dimension of <code>schedule</code>. However, if this dimension is split, its derived dimensions are added to this category.</li> <li>Unfused dimensions: all the remaining dimensions.</li> </ul> <p>Note that the individual schedules being fused may have been created by previous fusing operations. The categories above relate to the role of each dimension in the current fusing operation.</p>"},{"location":"Reference/safety_analysis/#theorem","title":"Theorem","text":"<p>Imagine that we apply a sequence of transformations to <code>schedule</code>, which may derive new dimensions. Derived dimensions belong to the same category as the dimension from which they were derived. Suppose the fusing dimension (and its derived dimensions) precedes all the unfused dimensions. In that case, for any value of the fused dimensions, all the corresponding work from <code>schedule0</code> is executed before any of the corresponding work from <code>schedule1</code>. Similarly, all the corresponding work from <code>schedule1</code> is executed before any of the corresponding work from <code>schedule2</code>; and so on.</p>"},{"location":"Reference/safety_analysis/#proof","title":"Proof","text":"<p>For simplicity, assume that there is only one fusing dimension, <code>f</code>. Also, assume that we've only fused two schedules, <code>schedule0</code> and <code>schedule1</code>. Note that these simplifying assumptions can easily be relaxed.</p> <p>Assume that <code>f</code> precedes all of the unfused dimensions. Therefore, dimensions that precede <code>f</code> are necessarily fused dimensions. Let <code>U</code> be a sequence of concrete values for all the fused dimensions, and let <code>V</code> denote only those values that correspond to dimensions that precede <code>f</code>. The work from <code>schedule0</code> that corresponds to the concrete values in <code>U</code> is contained in the slice (V, 0, *, ..., *). Similarly, the work form <code>schedule1</code> that corresponds to the values in <code>U</code> is contained in (V, 1, *, ..., *). Finally, note that the former slice lexicographically precedes the latter, concluding the proof.</p>"},{"location":"Reference/safety_analysis/#an-example","title":"An example","text":"<p>To make the theorem less abstract, we demonstrate how it applies to a simple example. Assume that we start with two three-dimensional schedules, <code>schedule0</code> and <code>schedule1</code>, and we fuse their first two dimensions: <pre><code>i0, j0, k0 = schedule0.get_indices() # redundant operation, included for clarity\ni1, j1, k1 = schedule1.get_indices() # redundant operation, included for clarity\nschedule = acc.fuse((schedule0, schedule1), partial=2)\ni, j, f, k0, k1 = schedule.get_indices()\n</code></pre> Next, say that we transform <code>schedule</code> by tiling dimensions <code>j</code> and <code>k0</code> to reorder the dimensions as follows: <pre><code>jj, kk0 = schedule.tile({\n    j: 4,\n    k0: 4\n})\nschedule.reorder(j, i, f, k0, k1, kk0, jj)\n</code></pre> Dimensions <code>i</code>, <code>j</code>, and <code>jj</code> are fused dimensions, while <code>k0</code>, <code>kk0</code>, and <code>k1</code> are unfused dimensions. Note that the fusing dimension <code>f</code> precedes all of the unfused dimensions, satisfying the theorem's condition. Next, choose concrete values for the fused dimensions, say, <code>i=4</code>, <code>j=3</code>, and <code>jj=2</code>. The work from <code>schedule0</code> that corresponds to these values is contained in the slice (3, 4, 0, *, *, *, *). Similarly, the work from <code>schedule1</code> that corresponds to these values is contained in the slice (3, 4, 1, *, *, *, *). The former slice lexicographically precedes the latter and is therefore executed first.</p>"},{"location":"Reference/safety_analysis/#safety","title":"Safety","text":"<p>The theorem holds for any schedule, but it does not imply that every schedule is safe. Additional effort is required to prove whether a specific schedule is safe. When performing a <code>fuse</code> operation, we must examine the specific circumstances and consider whether the theorem provides a sufficient condition for safety.</p>"},{"location":"Reference/classes/Array/Array/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Array/Array/#acceraarrayrole-data-element_type-layout-offset-shape","title":"<code>accera.Array(role[, data, element_type, layout, offset, shape])</code>","text":"<p>Constructs an array.</p>"},{"location":"Reference/classes/Array/Array/#arguments","title":"Arguments","text":"argument description type/default <code>role</code> The role of the array determines if the array scope is internal or external, if the array is mutable or immutable, and if the array memory is dynamically allocated. <code>accera.Role</code> <code>data</code> The contents of a constant array. Required for <code>accera.Role.CONST</code> arrays but should not be specified for other roles. Python buffer or <code>numpy.ndarray</code>. <code>element_type</code> The array element type. <code>accera.ScalarType</code>, default: <code>accera.ScalarType.float32</code>. <code>layout</code> The affine memory map. <code>accera.Array.Layout</code>, or tuple of (integers or <code>accera.Dimension</code>), default: <code>accera.Array.Layout.FIRST_MAJOR</code>. <code>offset</code> The offset of the affine memory map integer (positive, zero, or negative), default: 0. <code>shape</code> The array shape. Required for roles other than <code>accera.Role.CONST</code>, should not be specified for <code>accera.Role.CONST</code>. tuple of (integers or <code>accera.Dimension</code>)."},{"location":"Reference/classes/Array/Array/#examples","title":"Examples","text":"<p>Construct an input array: <pre><code>import accera as acc\nA = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(10, 20))  # the default layout is acc.Array.Layout.FIRST_MAJOR\n</code></pre></p> <p>Construct an input array with an explicit standard layout: <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(10, 20), layout=acc.Array.Layout.LAST_MAJOR)\n</code></pre></p> <p>Construct an input array with an explicit affine memory map: <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(10, 20), layout=(1, 10))\n</code></pre></p> <p>Construct an input array with an infinite (undefined) major dimension: <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(10, acc.inf), layout=acc.Array.Layout.LAST_MAJOR)\n</code></pre></p> <p>Construct a input array with both runtime and compile-time dimension sizes: <pre><code>M = acc.create_dimensions()\nA = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, 20))\n</code></pre></p> <p>Construct an input/output array: <pre><code>A = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(10, 20))\n</code></pre></p> <p>Construct an input/output array with runtime input dimension sizes: <pre><code>M, N = acc.create_dimensions()\nA = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n</code></pre></p> <p>Construct an output array with runtime output dimension sizes: <pre><code>M, N = acc.create_dimensions(role=acc.Role.OUTPUT)\nA = acc.Array(role=acc.Role.OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n</code></pre></p> <p>Construct an output array with explicit affine memory map: <pre><code>M, N = acc.create_dimensions(role=acc.Role.OUTPUT)\nA = acc.Array(role=acc.Role.OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N), layout=(1, M))\n</code></pre></p> <p>Construct a constant array: <pre><code>D = np.random.rand(10, 16)\nA = acc.Array(role=acc.Role.CONST, data=D)\n</code></pre></p> <p>Construct a constant array with an explicit element type and layout, which does not necessarily match the input data: <pre><code>D = np.random.rand(10, 16)\nA = acc.Array(role=acc.Role.CONST, element_type=acc.ScalarType.float32, layout=acc.Array.Layout.LAST_MAJOR, data=D)\n</code></pre></p> <p>Construct a temporary array: <pre><code>A = acc.Array(role=acc.Role.TEMP, element_type=acc.ScalarType.float32, shape=(10, 20), layout=acc.Array.Layout.LAST_MAJOR)\n</code></pre></p>"},{"location":"Reference/classes/Array/Layout/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Array/Layout/#acceraarraylayout","title":"<code>accera.Array.Layout</code>","text":"type description <code>accera.Array.Layout.FIRST_MAJOR</code> Specifies a memory layout where the first major axis is in contiguous memory. For example, in a matrix, this corresponds to \"row-major\". <code>accera.Array.Layout.LAST_MAJOR</code> Specifies a memory layout where the last major axis is in contiguous memory. For example, in a matrix, this corresponds to \"column-major\". <code>accera.Array.Layout.DEFERRED</code> Defer specifying the memory layout for an <code>Role.CONST</code> array until a cache is created."},{"location":"Reference/classes/Array/deferred_layout/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Array/deferred_layout/#acceraarraydeferred_layoutcache","title":"<code>accera.Array.deferred_layout(cache)</code>","text":"<p>Specifies the layout for a <code>Role.CONST</code> array based on a <code>Cache</code>. For more details, see Deferred layout of constant arrays</p>"},{"location":"Reference/classes/Array/deferred_layout/#arguments","title":"Arguments","text":"argument description type/default <code>cache</code> The cache that defines the layout to set. <code>accera.Cache</code>"},{"location":"Reference/classes/Array/deferred_layout/#examples","title":"Examples","text":"<p>Create a constant 16x16 array without specifying a layout. Later on, define its layout based on a cache:</p> <pre><code>import numpy as np\nimport accera as acc\n\nmatrix = np.random.rand(16, 16)\n\n# Create a constant array with a deferred layout\nA = acc.Array(role=acc.Role.CONST, data=matrix, layout=acc.Array.Layout.DEFERRED)\nB = Array(role=Role.INPUT_OUTPUT, element_type=ScalarType.float32, shape=matrix.shape)\n\nnest = Nest(shape=matrix.shape)\ni, j = nest.get_indices()\n\n@nest.iteration_logic\ndef_():\n    B[i, j] += A[i, j]\n\nplan = nest.create_plan()\n\n# create a cache for the constant array\nAA = plan.cache(A, i, layout=acc.Array.Layout.FIRST_MAJOR, thrifty=True)\n\n# update the constant array's layout based on the cache\nA.deferred_layout(cache=AA)\n</code></pre>"},{"location":"Reference/classes/Array/slice/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Array/slice/#acceraarrayslicesliced_dims-sliced_offsets","title":"<code>accera.Array.slice(sliced_dims, sliced_offsets)</code>","text":"<p>Creates a sliced view of reduced rank from an array. The view is created from elements at specified offsets of the sliced dimensions of the original array.</p>"},{"location":"Reference/classes/Array/slice/#arguments","title":"Arguments","text":"argument description type/default <code>sliced_dims</code> The dimension indices of the original array to slice on. <code>Tuple[int]</code> <code>sliced_offsets</code> The offsets of the corresponding dliced dimensions. <code>Tuple[Scalar]</code>"},{"location":"Reference/classes/Array/slice/#examples","title":"Examples","text":"<p>Clear a slice of size 5 from an array of size 5x5 at dimension 0 with offset 2:</p> <pre><code>import numpy as np\nimport accera as acc\n\nN = 5\nslice_dim = 0\nslice_offset = 2\n\nmatrix = np.random.rand(N, N)\nArr = Array(role=Role.INPUT, data=matrix)\n\n# Zero out a slice of size [5] such that the resulting array looks like this:\n# xxxxx\n# xxxxx\n# 00000\n# xxxxx\n# xxxxx\n\nnest = Nest(shape=(N,))\ni, = nest.get_indices()\n\n@nest.iteration_logic\ndef _():\n    SliceArr = Arr.slice([slice_dim], [slice_offset])\n    SliceArr[i] = 0.0\n\nschedule = nest.create_schedule()\n</code></pre>"},{"location":"Reference/classes/Array/sub_array/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Array/sub_array/#acceraarraysub_arrayoffsets-shape-strides","title":"<code>accera.Array.sub_array(offsets, shape[, strides])</code>","text":"<p>Creates a sub-array of a specific shape from an array. The sub-array is created from elements at specified offsets and strides into the original array.</p>"},{"location":"Reference/classes/Array/sub_array/#arguments","title":"Arguments","text":"argument description type/default <code>offsets</code> The offsets into the original array. <code>Tuple[int]</code> <code>shape</code> The size of the sub-array. <code>Tuple[int]</code> <code>strides</code> (Optional) The strides in the original array used to create the sub-array. <code>Tuple[int]</code>"},{"location":"Reference/classes/Array/sub_array/#examples","title":"Examples","text":"<p>Create a sub-array of size 2x3 from an array of size 5x5 at an offset of {1, 1} and a stride of {2, 1}:</p> <pre><code>import numpy as np\nimport accera as acc\n\nN = 5\nsubArrayNumRows = 2\nsubArrayNumCols = 3\n\nmatrix = np.random.rand(N, N)\nArr = Array(role=Role.INPUT, data=matrix)\n\n# Zero out a sub array of size [2, 3] such that the resulting array looks like this:\n# xxxxx\n# x000x\n# xxxxx\n# x000x\n# xxxxx\n\nnest = Nest(shape=(subArrayNumRows, subArrayNumCols))\ni, j = nest.get_indices()\n\n@nest.iteration_logic\ndef _():\n    SubArr = Arr.sub_array([1, 1], [subArrayNumRows, subArrayNumCols], [2, 1])\n    SubArr[i, j] = 0.0\n\nschedule = nest.create_schedule()\n</code></pre>"},{"location":"Reference/classes/Dimension/Dimension/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Dimension/Dimension/#acceradimensionrole-value","title":"<code>accera.Dimension([role, value])</code>","text":"<p>Constructs a runtime dimension size with optional initialization.</p> <p>Note: This constructor is meant for advanced use cases that involve Python generator expressions. For the simplified syntax to create dimensions, see create_dimensions.</p>"},{"location":"Reference/classes/Dimension/Dimension/#arguments","title":"Arguments","text":"argument description type/default <code>role</code> The role of the dimension determines if it is mutable or immutable. <code>accera.Role</code>. default: <code>accera.Role.INPUT</code>. <code>name</code> The name of the dimension variable. Default is an empty string. string <code>value</code> The optional value to initialize the dimension. Only applies to mutable dimensions (<code>accera.Role.OUTPUT</code>) integer or <code>Dimension</code>"},{"location":"Reference/classes/Dimension/Dimension/#returns","title":"Returns","text":"<p><code>Dimension</code></p>"},{"location":"Reference/classes/Dimension/Dimension/#examples","title":"Examples","text":"<p>Construct an output array with runtime dimensions using Python tuple comprehension over an input shape: <pre><code>import accera as acc\n\n# input_shape is a tuple or list of acc.Dimensions or integers\noutput_shape = tuple(acc.Dimension(role=acc.Role.OUTPUT, value=i) for i in input_shape)\nA = acc.Array(role=acc.Role.OUTPUT, element_type=acc.ScalarType.float32, shape=output_shape)\n</code></pre></p>"},{"location":"Reference/classes/FusedSchedule/get_fused_indices/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/FusedSchedule/get_fused_indices/#accerafusedscheduleget_fused_indices","title":"<code>accera.FusedSchedule.get_fused_indices()</code>","text":"<p>Gets the fused indices of a fused schedule.</p>"},{"location":"Reference/classes/FusedSchedule/get_fused_indices/#returns","title":"Returns","text":"<p>Tuple of <code>Index</code></p>"},{"location":"Reference/classes/FusedSchedule/get_fused_indices/#examples","title":"Examples","text":"<pre><code>i, j = fused_schedule.get_fused_indices()\n</code></pre>"},{"location":"Reference/classes/FusedSchedule/get_fusing_index/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/FusedSchedule/get_fusing_index/#accerafusedscheduleget_fusing_index","title":"<code>accera.FusedSchedule.get_fusing_index()</code>","text":"<p>Gets the fusing index of a fused schedule.</p>"},{"location":"Reference/classes/FusedSchedule/get_fusing_index/#returns","title":"Returns","text":"<p>Instance of <code>Index</code></p>"},{"location":"Reference/classes/FusedSchedule/get_fusing_index/#examples","title":"Examples","text":"<pre><code>f = fused_schedule.get_fusing_index()\n</code></pre>"},{"location":"Reference/classes/FusedSchedule/get_unfused_indices/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/FusedSchedule/get_unfused_indices/#accerafusedscheduleget_unfused_indices","title":"<code>accera.FusedSchedule.get_unfused_indices()</code>","text":"<p>Gets the unfused indices of a fused schedule.</p>"},{"location":"Reference/classes/FusedSchedule/get_unfused_indices/#returns","title":"Returns","text":"<p>Tuple of <code>Index</code></p>"},{"location":"Reference/classes/FusedSchedule/get_unfused_indices/#examples","title":"Examples","text":"<pre><code> k, l = fused_schedule.get_unfused_indices()\n</code></pre>"},{"location":"Reference/classes/Nest/Nest/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Nest/Nest/#acceranestshape","title":"<code>accera.Nest(shape)</code>","text":"<p>Creates an affine loop nest.</p>"},{"location":"Reference/classes/Nest/Nest/#arguments","title":"Arguments","text":"argument description type/default <code>shape</code> The shape of the iteration space tuple of positive integers"},{"location":"Reference/classes/Nest/Nest/#examples","title":"Examples","text":"<p>Create a nest with 3 nested for-loops of sizes 16, 10, and 11:</p> <pre><code>nest = acc.Nest(shape=(16, 10, 11))\n</code></pre>"},{"location":"Reference/classes/Nest/create_plan/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Nest/create_plan/#acceranestcreate_plantarget","title":"<code>accera.Nest.create_plan([target])</code>","text":"<p>Creates a plan using the default schedule for the nest.</p>"},{"location":"Reference/classes/Nest/create_plan/#arguments","title":"Arguments","text":"argument description type/default <code>target</code> The target platform. Defaults to <code>acc.Target.HOST</code> <code>Target</code>"},{"location":"Reference/classes/Nest/create_plan/#returns","title":"Returns","text":"<p><code>Plan</code></p>"},{"location":"Reference/classes/Nest/create_plan/#examples","title":"Examples","text":"<p>Create a plan for the host computer, using the default schedule for a nest:</p> <pre><code>plan = nest.create_plan()\n</code></pre> <p>Create a plan for an Intel Core 7th Generation, using the default schedule for a nest:</p> <pre><code>corei9 = acc.Target(\"Intel 7900X\", num_threads=44)\nplan = nest.create_plan(corei9)\n</code></pre>"},{"location":"Reference/classes/Nest/create_schedule/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Nest/create_schedule/#acceranestcreate_schedule","title":"<code>accera.Nest.create_schedule()</code>","text":"<p>Create a default schedule for a nest.</p>"},{"location":"Reference/classes/Nest/create_schedule/#returns","title":"Returns","text":"<p><code>Schedule</code></p>"},{"location":"Reference/classes/Nest/create_schedule/#examples","title":"Examples","text":"<pre><code>schedule = nest.create_schedule()\n</code></pre>"},{"location":"Reference/classes/Nest/get_indices/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Nest/get_indices/#acceranestget_indices","title":"<code>accera.Nest.get_indices()</code>","text":"<p>Gets the iteration space dimensions for a nest.</p>"},{"location":"Reference/classes/Nest/get_indices/#returns","title":"Returns","text":"<p>Tuple of <code>Index</code></p>"},{"location":"Reference/classes/Nest/get_indices/#examples","title":"Examples","text":"<p>Get the iteration space dimensions for a 3-dimensional nest:</p> <pre><code>i, j, k = nest.get_indices()\n</code></pre>"},{"location":"Reference/classes/Nest/iteration_logic/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Nest/iteration_logic/#acceranestiteration_logiclogic","title":"<code>accera.Nest.iteration_logic(logic)</code>","text":"<p>Adds an iteration logic function to a <code>Nest</code>.</p>"},{"location":"Reference/classes/Nest/iteration_logic/#arguments","title":"Arguments","text":"argument description type/default <code>logic</code> Python function that represents the logic to run in the innermost loop of the nest."},{"location":"Reference/classes/Nest/iteration_logic/#examples","title":"Examples","text":"<p>The preferred syntax uses Python decorators, as follows: <pre><code>import accera as acc\n\nA = acc.Array(role=acc.role.INPUT, shape=(16, 64))\nB = acc.Array(role=acc.role.INPUT, shape=(64, 32))\nC = acc.Array(role=acc.role.INPUT_OUTPUT, shape=(16, 32))\n\nnest = acc.Nest(shape=(16, 32, 64))\ni, j, k = nest.get_indices()\n\n@nest.iteration_logic\ndef _():\n    C[i,j] += A[i,k] * B[k,j]\n</code></pre></p> <p>The alternative syntax avoids decorators and instead defines the logic in a function: <pre><code>import accera as acc\n\nA = acc.Array(role=acc.role.INPUT, shape=(16, 64))\nB = acc.Array(role=acc.role.INPUT, shape=(64, 32))\nC = acc.Array(role=acc.role.INPUT_OUTPUT, shape=(16, 32))\n\nnest = acc.Nest(shape=(16, 32, 64))\ni, j, k = nest.get_indices()\n\ndef logic_fn():\n    C[i, j] += A[i, k] * B[k, j]\n\nnest.iteration_logic(logic_fn)\n</code></pre></p>"},{"location":"Reference/classes/Package/Format/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Package/Format/#accerapackageformat","title":"<code>accera.Package.Format</code>","text":"type description <code>accera.Package.Format.HAT_DYNAMIC</code> HAT package format, dynamically linked. <code>accera.Package.Format.HAT_STATIC</code> HAT package format, statically linked. <code>accera.Package.Format.MLIR_DYNAMIC</code> MLIR (debugging) package format, dynamically linked. <code>accera.Package.Format.MLIR_STATIC</code> MLIR (debugging) package format, statically linked. <p>When cross-compiling, use either <code>accera.Package.Format.HAT_STATIC</code> or <code>accera.Package.Format.MLIR_STATIC</code>.</p>"},{"location":"Reference/classes/Package/Mode/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Package/Mode/#accerapackagemode","title":"<code>accera.Package.Mode</code>","text":"type description <code>accera.Package.Mode.DEBUG</code> Debug mode (automatically tests logical equivalence). <code>accera.Package.Mode.RELEASE</code> Release (maximally optimized)."},{"location":"Reference/classes/Package/Package/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Package/Package/#accerapackagepackage","title":"<code>accera.Package.Package()</code>","text":"<p>A package of functions that can be built and linked with client code.</p>"},{"location":"Reference/classes/Package/Package/#examples","title":"Examples","text":"<p>Create a package:</p> <pre><code>package = acc.Package()\n</code></pre>"},{"location":"Reference/classes/Package/Platform/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Package/Platform/#accerapackageplatform","title":"<code>accera.Package.Platform</code>","text":"type description <code>accera.Package.Platform.HOST</code> The host computer's platform <code>accera.Package.Platform.WINDOWS</code> The Windows platform <code>accera.Package.Platform.LINUX</code> The Linux platform <code>accera.Package.Platform.MACOS</code> The MacOS platform <code>accera.Package.Platform.ANDRIOD</code> The Android platform <code>accera.Package.Platform.IOS</code> The iOS platform <code>accera.Package.Platform.RASPBIAN</code> The Raspbian platform"},{"location":"Reference/classes/Package/add/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Package/add/#accerapackageaddsource-args-base_name-parameters","title":"<code>accera.Package.add(source, args[, base_name, parameters])</code>","text":"<p>Adds one or more functions to the package.</p>"},{"location":"Reference/classes/Package/add/#arguments","title":"Arguments","text":"argument description type <code>source</code> The source which defines the function's implementation. <code>Nest</code> or <code>Schedule</code> or <code>Plan</code> <code>args</code> The order of external-scope arrays, scalars, and dimensions used in the function signature. tuple of <code>Array</code>, <code>Scalar</code>, or <code>Dim</code> <code>base_name</code> A base name for the function. The full name for the function will be the base name followed by an automatically-generated unique identifier. string <code>parameters</code> A value for each parameter if the function's implementation is parameterized. See Parameters. A list of dictionaries can also be provided, in which case, multiple functions are generated. <code>Parameter</code> to value dictionary or a list of <code>Parameter</code> to value dictionaries."},{"location":"Reference/classes/Package/add/#examples","title":"Examples","text":"<p>Adding a function defined by an <code>Plan</code>:</p> <pre><code>package.add(plan, args=(A, B, C), base_name=\"simple_matmul\")\n</code></pre> <p>Convenience syntax to add a function defined by a <code>Schedule</code>. A default <code>Plan</code> will be created automatically:</p> <pre><code>package.add(schedule, args=(A, B, C), base_name=\"simple_matmul\")\n</code></pre> <p>Convenience syntax to add a function defined by a <code>Nest</code>. A default <code>Schedule</code> and <code>Plan</code> will be created internally:</p> <pre><code>package.add(nest, args=(A, B, C), base_name=\"simple_matmul\")\n</code></pre> <p>Adding a function with concrete values specified for its parameters (<code>P0</code>, <code>P1</code>, <code>P2</code>, <code>P3</code>).</p> <pre><code>package.add(nest, args=(A, B, C), parameters={P0:16, P1:16, P2:16, P3:1}, base_name=\"matmul_16_16_16_1\")\n</code></pre> <p>Adding a function with runtime dimension sizes <code>M</code>, <code>N</code>, <code>K</code> and arrays <code>A</code>, <code>B</code>, and <code>C</code>:</p> <pre><code>package.add(nest, args=(M, N, K, A, B, C), base_name=\"matmul_M_N_K\")\n</code></pre>"},{"location":"Reference/classes/Package/add_description/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Package/add_description/#accerapackageadd_descriptionauthor-license-other-version","title":"<code>accera.Package.add_description([author, license, other, version])</code>","text":"<p>Adds descriptive metadata to the HAT package.</p>"},{"location":"Reference/classes/Package/add_description/#arguments","title":"Arguments","text":"argument description type/default <code>author</code> Name of the individual or group that authored the package. string <code>license</code> The internet URL of the license used to release the package. string <code>other</code> User-specific descriptive metadata dictionary <code>version</code> The package version. string"},{"location":"Reference/classes/Package/add_description/#examples","title":"Examples","text":"<p>Adds the standard version, license, and author description fields to the package: <pre><code>package.add_description(version\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b=\"1.0\", license=\"https://mit-license.org/\", author=\"Microsoft Research\")\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre></p> <p>Adds arbitrary user-defined metadata to describe the package: <pre><code>package.add_description(other={\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\"title\": \"My Package Title\", \"source\": \"https://github.com/\", \"citations\": [\"https://arxiv.org/2021.12345/\", \"https://arxiv.org/2021.56789/\"]}\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b)\n</code></pre></p>"},{"location":"Reference/classes/Package/build/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Package/build/#accerapackagebuildname-format-mode-platform-tolerance-output_dir","title":"<code>accera.Package.build(name[, format, mode, platform, tolerance, output_dir])</code>","text":"<p>Builds a HAT package.</p>"},{"location":"Reference/classes/Package/build/#arguments","title":"Arguments","text":"argument description type/default <code>name</code> The package name. string <code>format</code> The format of the package. <code>accera.Package.Format</code>, defaults to <code>Package.Format.HAT_STATIC</code> <code>mode</code> The package mode, such as whether it is optimized or used for debugging. <code>robopy.Package.Mode</code>, defaults to <code>Package.Mode.Release</code> <code>platform</code> The platform where the package runs. <code>accera.Package.Platform</code> <code>tolerance</code> The tolerance for correctness checking when <code>mode = Package.Mode.Debug</code>. float, defaults to 1e-5 <code>output_dir</code> The path to an output directory. Defaults to the current directory if unspecified. string"},{"location":"Reference/classes/Package/build/#examples","title":"Examples","text":"<p>Build a Dynamically-linked HAT package called <code>myPackage</code> containing <code>func1</code> for the host platform in the current directory:</p> <pre><code>package = acc.Package()\npackage.add(plan, base_name=\"func1\")\npackage.build(format=acc.Package.Format.HAT_DYNAMIC, name=\"myPackage\")\n</code></pre> <p>Build a statically-linked HAT package called <code>myPackage</code> containing <code>func1</code> for the host platform in the <code>hat_packages</code> subdirectory:</p> <pre><code>package = acc.Package()\npackage.add(plan, base_name=\"func1\")\npackage.build(format=acc.Package.Format.HAT_STATIC, name=\"myPackage\", output_dir=\"hat_packages\")\n</code></pre> <p>Build a statically-linked <code>myPackage</code> with additional intermediate MLIR files for debugging purposes. To build a dynamically-linked package, use <code>acc.Package.Format.MLIR_DYNAMIC</code>:</p> <pre><code>package = acc.Package()\npackage.add(plan, base_name=\"func1\")\npackage.build(format=acc.Package.Format.MLIR_STATIC, name=\"myPackage\")\n</code></pre> <p>Build a package with error checking for <code>func1</code>, outputing error messages to <code>stderr</code> if the default implementation and the Accera implementation do not match within a tolerance of <code>1.0e-6</code>:</p> <pre><code>package = acc.Package()\npackage.add(plan, base_name=\"func1\")\npackage.build(format=acc.Package.Format.HAT_DYNAMIC, name=\"myPackage\", mode=acc.Package.Mode.DEBUG, tolerance=1.0e-6)\n</code></pre> <p>Cross-compile a statically-linked HAT package called <code>myPackage</code> containing <code>func1</code> for the Raspberry Pi 3. Note that dynamically-linked HAT packages are not supported for cross-compilation:</p> <pre><code>pi3 = Target(\"Raspberry Pi 3B\", category=Target.Category.CPU)\nplan = schedule.create_plan(target=pi3)\npackage = acc.Package()\npackage.add(plan, base_name=\"func1\")\npackage.build(format=acc.Package.Format.HAT_STATIC, name=\"myPackagePi3\", platform=acc.Package.Platform.RASPBIAN)\n</code></pre>"},{"location":"Reference/classes/Plan/bind/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Plan/bind/#acceraplanbindmapping","title":"<code>accera.Plan.bind(mapping)</code>","text":"<p>Only available for targets that can execute a grid of work (such as GPUs). The <code>bind</code> function binds dimensions of the iteration space to axes of the target-specific grid (such as <code>v100.GridUnit.BLOCK_X</code>, <code>v100.GridUnit.THREAD_X</code> or <code>v100.GridUnit.WARP_X</code> on an Nvidia GPU).</p>"},{"location":"Reference/classes/Plan/bind/#arguments","title":"Arguments","text":"argument description type/default <code>mapping</code> Mapping of indices to GPU thread or block identifiers. dict of <code>Index</code> to target-specific identifiers"},{"location":"Reference/classes/Plan/bind/#examples","title":"Examples","text":"<p>Mark the <code>i</code>, <code>j</code>, and <code>k</code> indices to execute on NVidia V100's <code>BLOCK_X</code>, <code>THREAD_X</code>, and <code>THREAD_Y</code> grid axes, respectively.</p> <pre><code>v100 = acc.Target(Target.Model.NVIDIA_V100)\nplan.bind({\n    i: v100.GridUnit.BLOCK_X,\n    j: v100.GridUnit.THREAD_X,\n    k: v100.GridUnit.THREAD_Y\n})\n</code></pre> <p>In some cases, e.g. with tensorization where it might be non-trivial to assign threads to their respective data, it might be simpler to bind iteration space indices to warps (Nvidia) or waves (AMD) in the x and y dimensions rather than threads. This also abstracts the computation at a level higher than individual threads where, instead of each thread performing calculation independently we consider a group of threads (warp) working to solve a bigger computational problem collaboratively (as we often see in warp synchronous primitives like the CUDA's WMMA api). For example,</p> <pre><code>v100 = acc.Target(Target.Model.NVIDIA_V100)\nplan.bind({\n    i: v100.GridUnit.BLOCK_X,\n    j: v100.GridUnit.BLOCK_Y,\n    ii: v100.GridUnit.WARP_Y,\n    jj: v100.GridUnit.WARP_X\n})\n</code></pre> <p>in this case, we assign a warp/wave of threads to each unique combination of the (<code>ii</code>, <code>jj</code>) in the iteration space. The spatial arrangement of the warps to their data is defined by the ranges assigned to these individual indices. For example, if both <code>ii</code> and <code>jj</code> are ranges [0, 32) with step size of 16, we will have a total of 4 warps (2 in the x-dimension and 2 in the y-dimension) covering a 32x32 data region.</p>"},{"location":"Reference/classes/Plan/cache/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Plan/cache/#acceraplancachesource-index-trigger_index-layout-level-trigger_level-max_elements-element_type-strategy-thrifty-location-double_buffer-double_buffer_location-vectorize","title":"<code>accera.Plan.cache(source[, index, trigger_index, layout, level, trigger_level, max_elements, element_type, strategy, thrifty, location, double_buffer, double_buffer_location, vectorize])</code>","text":"<p>Adds a caching strategy to a plan.</p>"},{"location":"Reference/classes/Plan/cache/#arguments","title":"Arguments","text":"argument description type <code>source</code> The array or cache from which this cache is copied. <code>Array</code> or <code>Cache</code>. <code>index</code> The index used to determine the cache level. Specify one and only one of <code>index</code>, <code>level</code>, <code>max_elements</code>. <code>Index</code>. <code>trigger_index</code> The index used to determine what level to fill the cache at. <code>trigger_index</code> can't come after <code>index</code> in the schedule order and will default to <code>index</code> if not specified. Specify at most one of <code>trigger_index</code> or <code>trigger_level</code>. <code>Index</code>. <code>layout</code> The affine memory map, if different from the source. <code>accera.Layout</code>. <code>level</code> The key-slice level to cache (the number of wildcard dimensions in a key-slice). Specify one and only one of <code>index</code>, <code>level</code>, <code>max_elements</code>. positive integer. <code>trigger_level</code> The key-slice level to fill the cache at. <code>trigger_level</code> can't be smaller than <code>level</code>, and will default to <code>level</code> if not specified. Specify at most one of <code>trigger_index</code> or <code>trigger_level</code>. positive integer <code>max_elements</code> The maximum elements to include in the cached region. Specify one and only one of <code>index</code>, <code>level</code>, <code>max_elements</code>. positive integer <code>element_type</code> The element type to use in the cache. Defaults to the element type of the cached array <code>ScalarType</code> <code>strategy</code> The thread to data mapping pattern to use when collaboratively caching by multiple threads. Defaults to AUTO which will resolve to the strategy best suited for the current target environment. <code>CacheStrategy</code> <code>thrifty</code> Use thrifty caching (copy data into a cache only if the cached data differs from the original active block). <code>bool</code> <code>location</code> The type of memory used to store the cache. <code>MemorySpace</code> <code>double_buffer</code> Whether to make this cache a double-buffering cache. Only valid on INPUT and CONST arrays. <code>bool</code> <code>double_buffer_location</code> Which memory space to put the double buffer temp array in. Requires that double_buffer is set to True. Defaults to <code>AUTO</code>. <code>MemorySpace</code> or <code>AUTO</code> <code>vectorize</code> Whether to vectorize the cache operations. Defaults to <code>AUTO</code>, which will behave like <code>vectorize=True</code> if the loop-nest has any vectorized loop via <code>plan.vectorize(index)</code> or <code>vectorize=False</code> if the loop-nest has no vectorized loops. <code>bool</code> <p><code>AUTO</code> will configure the double buffering location based on the following: <code>location</code> | <code>double_buffer</code> | <code>double_buffer_location</code> = <code>AUTO</code> --- | --- | --- <code>MemorySpace.SHARED</code> | <code>True</code> | <code>MemorySpace.PRIVATE</code> <code>!MemorySpace.SHARED</code> | <code>True</code> | Same value as <code>location</code></p>"},{"location":"Reference/classes/Plan/cache/#returns","title":"Returns","text":"<p>A <code>Cache</code> handle that represents the created cache.</p>"},{"location":"Reference/classes/Plan/cache/#examples","title":"Examples","text":"<p>Create a cache of array <code>A</code> at level 2. <pre><code>AA = plan.cache(A, level=2)\n</code></pre></p> <p>Create a cache of array <code>A</code> with the <code>Array.Layout.FIRST_MAJOR</code> layout: <pre><code>AA = plan.cache(A, level=2, layout=acc.Array.Layout.FIRST_MAJOR)\n</code></pre></p> <p>Create a cache of array <code>A</code> for dimension <code>j</code>: <pre><code>AA = plan.cache(A, index=j)\n</code></pre></p> <p>Create a cache of array <code>A</code> for the largest active block that does not exceed 1024 elements: <pre><code>AA = plan.cache(A, max_elements=1024)\n</code></pre></p> <p>Create a level 2 cache of array <code>A</code> from its level 4 cache: <pre><code>AA = plan.cache(A, level=4)\nAAA = plan.cache(AA, level=2)\n</code></pre></p> <p>Not yet implemented: Create a cache of array <code>A</code> at index <code>i</code> in GPU shared memory: <pre><code>v100 = Target(Target.Model.NVIDIA_V100)\nAA = plan.cache(A, i, location=v100.MemorySpace.SHARED)\n</code></pre></p>"},{"location":"Reference/classes/Plan/kernelize/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Plan/kernelize/#acceraplankernelizeunroll_indices-vectorize_indices","title":"<code>accera.Plan.kernelize(unroll_indices[, vectorize_indices])</code>","text":"<p>A convenience method for a sequence of <code>unroll</code> instructions followed by a possible sequence of <code>vectorize</code> instructions.</p>"},{"location":"Reference/classes/Plan/kernelize/#arguments","title":"Arguments","text":"argument description type/default <code>unroll_indices</code> The iteration-space dimensions to unroll tuple of <code>accera.Index</code>. <code>vectorize_indices</code> The optional iteration-space dimensions to vectorize <code>accera.Index</code> or tuple of <code>accera.Index</code>."},{"location":"Reference/classes/Plan/kernelize/#examples","title":"Examples","text":"<p>Unroll <code>i</code> and <code>k</code>, and then vectorize <code>j</code>:</p> <pre><code>schedule.reorder(i, k, j)\nplan = schedule.create_plan()\nplan.kernelize(unroll_indices=(i, k), vectorize_indices=j)\n</code></pre> <p>Another example is to Unroll <code>i</code> and then vectorize <code>j</code> and <code>k</code>:</p> <pre><code>schedule.reorder(i, j, k)\nplan = schedule.create_plan()\nplan.kernelize(unroll_indices=(i,), vectorize_indices=(j, k))\n</code></pre>"},{"location":"Reference/classes/Plan/parallelize/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Plan/parallelize/#acceraplanparallelizeindices-pin-policy-max_threads","title":"<code>accera.Plan.parallelize(indices[, pin, policy, max_threads])</code>","text":"<p>Executes one or more loops in parallel on multiple cores or processors.</p> <p>Only available for targets with multiple cores or processors.</p>"},{"location":"Reference/classes/Plan/parallelize/#arguments","title":"Arguments","text":"argument description type/default <code>indices</code> The iteration-space dimensions to run in parallel. To assign multiple threads to an index, first split that index, then parallelize its split indices.  Unsplit indices will be assigned one thread each, split indices will be assigned threads based on the number of split blocks. This is limited by the number of threads supported by the target. tuple of <code>accera.Index</code> <code>pin</code> Pin the computation to a subset of cores or processors. tuple of target-specific identifiers <code>policy</code> The scheduling policy to apply (\"dynamic\" or \"static\"). string. Defaults to \"static\". <code>max_threads</code> The maximum number of threads to use when distributing the workload. The actual number of threads used is the lowest value among (a) <code>max_threads</code>, (b) the number of threads supported by the target and (c) the number of iterations in the domain as specified by <code>indices</code>. int. Defaults to None."},{"location":"Reference/classes/Plan/parallelize/#examples","title":"Examples","text":""},{"location":"Reference/classes/Plan/parallelize/#parallelize-the-i-j-and-k-dimensions-using-default-number-of-threads","title":"Parallelize the <code>i</code>, <code>j</code>, and <code>k</code> dimensions using default number of threads:","text":"<pre><code>nest = Nest(shape=(2, 3, 4))\ni, j, k = nest.get_indices()\nplan.parallelize(indices=(i, j, k)) # This will use 2 x 3 x 4 = 24 threads\n</code></pre>"},{"location":"Reference/classes/Plan/parallelize/#parallelize-the-i-dimension-after-splitting-using-default-number-of-threads","title":"Parallelize the <code>i</code> dimension after splitting using default number of threads:","text":"<pre><code>nest = Nest(shape=(20,))\nschedule = nest.create_schedule()\ni = schedule.get_indices()\nii = schedule.split(i, 4)\nplan.parallelize(indices=i) # This will use 20 / 4 = 5 threads\n</code></pre>"},{"location":"Reference/classes/Plan/parallelize/#parallelize-the-i-j-and-k-dimensions-using-thread-limit","title":"Parallelize the <code>i</code>, <code>j</code>, and <code>k</code> dimensions using thread limit:","text":"<pre><code>nest = Nest(shape=(2, 3, 4))\ni, j, k = nest.get_indices()\nplan.parallelize(indices=(i, j, k), max_threads=4) # This will use 4 threads\n</code></pre>"},{"location":"Reference/classes/Plan/parallelize/#parallelize-the-i-dimension-with-thread-limit-set-higher-than-the-number-of-iterations","title":"Parallelize the <code>i</code> dimension with thread limit set higher than the number of iterations:","text":"<pre><code>nest = Nest(shape=(2, 3, 4))\ni, j, k = nest.get_indices()\nplan.parallelize(indices=i, max_threads=4) # This will use 2 threads since 'i' has only 2 iterations\n</code></pre> <p>Not yet implemented: Parallelize the <code>i</code>, <code>j</code>, and <code>k</code> dimensions by pinning them to specific cores on an Intel Xeon E5:</p> <pre><code>plan.parallelize(indices=(i, j, k), pin=(xeonE5.cores[0], xeonE5.cores[1], xeonE5.cores[2]))\n</code></pre> <p>Apply a dynamic scheduling policy, which uses a queue to partition the work across multiple cores:</p> <pre><code>plan.parallelize(indices=(i, j, k), policy=\"dynamic\")\n</code></pre>"},{"location":"Reference/classes/Plan/tensorize/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Plan/tensorize/#acceraplantensorizeindices-mma_shape-use_static_offsets-num_total_passes-num_fused_passes-scheduling_policy-prologue_op-epilogue_op","title":"<code>accera.Plan.tensorize(indices, mma_shape [, use_static_offsets, num_total_passes, num_fused_passes, scheduling_policy, prologue_op, epilogue_op])</code>","text":"<p>Only available for targets with native matrix multiplication instruction (tensor core) support. Marks the dimensions of the iteration-space for tensorization. Only perfectly nested loops of the following form can be tensorized:</p> <pre><code>for i in range(M):\n    for k in range(N):\n        for j in range(K):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre>"},{"location":"Reference/classes/Plan/tensorize/#arguments","title":"Arguments","text":"argument description type/default <code>indices</code> The 3-dimensional iteration space to tensorize. 3-D tuple of <code>accera.Index</code> <code>mma_shape</code> The type of MMA operation to use. <code>accera.MMAShape</code> <code>use_static_offsets</code> This is an optimization flag, which when enabled will use precomputed offset maps stored in device constant memory. Defaults to <code>False</code>. bool <code>num_total_passes</code> This controls the total number of passes to run. Defaults to 1. positive integer <code>num_fused_passes</code> This controls the number of passes for which register allocation is done, higher the value more the number of registers that are allocated. Defaults to <code>None</code> which will fuse all the passes as specified by <code>num_total_passes</code>. positive integer <code>scheduling_policy</code> For multi-block MMA operations, this controls whether matrix multiplication is done block-by-block or pass-by-pass (affects register usage). Default value is <code>accera.MMASchedulingPolicy.PASS_ORDER</code> <code>accera.MMASchedulingPolicy</code> <code>prologue_op</code> The element-wise operation to apply on matrix fragment data as a part of initialization (pre-tensorization). Default value is <code>accera.MMAFragmentOp.NONE</code> <code>accera.MMAFragmentOp</code> <code>epilogue_op</code> The element-wise operation to apply on matrix fragment data as a part of the final store (post-tensorization). Default value is <code>accera.MMAFragmentOp.NONE</code> <code>accera.MMAFragmentOp</code> <p>The different values of the enum <code>MMAShape</code> are explained here: <code>accera.MMAShape</code></p> <p>The different values of the enum <code>MMASchedulingPolicy</code> (applicable only for AMD targets supporting MFMA ops, such as <code>accera.Target.Model.AMD_MI100</code>) are mentioned here: <code>accera.MMASchedulingPolicy</code></p> <p>The different values of the enum <code>MMAFragmentOp</code> are explained here: <code>accera.MMAFragmentOp</code></p>"},{"location":"Reference/classes/Plan/tensorize/#examples","title":"Examples","text":"<p>Mark the dimensions <code>ii</code>, <code>jj</code>, and <code>kk</code> for tensorization execution:</p> <pre><code>plan.tensorize(indices=(ii,jj,kk))\n</code></pre>"},{"location":"Reference/classes/Plan/unroll/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Plan/unroll/#acceraplanunrollindex","title":"<code>accera.Plan.unroll(index)</code>","text":"<p>Marks a dimension of the iteration-space for unrolling.</p>"},{"location":"Reference/classes/Plan/unroll/#arguments","title":"Arguments","text":"argument description type/default <code>index</code> The index to unroll. <code>Index</code>"},{"location":"Reference/classes/Plan/unroll/#examples","title":"Examples","text":"<p>Mark the <code>i</code> dimension for unrolling:</p> <pre><code>plan.unroll(index=i)\n</code></pre>"},{"location":"Reference/classes/Plan/vectorize/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Plan/vectorize/#acceraplanvectorizeindex","title":"<code>accera.Plan.vectorize(index)</code>","text":"<p>Only available for targets that have SIMD registers and support vector instructions. Marks a dimension of the iteration-space for vectorization.</p>"},{"location":"Reference/classes/Plan/vectorize/#arguments","title":"Arguments","text":"argument description type/default <code>index</code> The index to vectorize. <code>Index</code>"},{"location":"Reference/classes/Plan/vectorize/#examples","title":"Examples","text":"<p>Mark the dimension <code>ii</code> for vectorized execution:</p> <pre><code>plan.vectorize(index=ii)\n</code></pre>"},{"location":"Reference/classes/Scalar/Scalar/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Scalar/Scalar/#accerascalarelement_type-value","title":"<code>accera.Scalar([element_type, value])</code>","text":"<p>Constructs a scalar that holds a number.</p>"},{"location":"Reference/classes/Scalar/Scalar/#arguments","title":"Arguments","text":"argument description type/default <code>element_type</code> The element type. <code>accera.ScalarType</code>, default: <code>accera.ScalarType.float32</code>. <code>value</code> An optional value. A number."},{"location":"Reference/classes/Scalar/Scalar/#examples","title":"Examples","text":"<p>Construct a float32 scalar: <pre><code>import accera as acc\n\nX = acc.Scalar()\n</code></pre></p> <p>Construct a float32 scalar and initialize it: <pre><code>Pi = acc.Scalar(value=3.14)\n</code></pre></p> <p>Construct integer scalars and perform arithmetic operations on them: <pre><code>X = acc.Scalar(element_type=acc.ScalarType.int32)\nY = acc.Scalar(element_type=acc.ScalarType.int32)\nY.value = x + 2\n</code></pre></p>"},{"location":"Reference/classes/Schedule/create_plan/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/create_plan/#acceraschedulecreate_plantarget","title":"<code>accera.Schedule.create_plan([target])</code>","text":"<p>Creates a plan for running this schedule.</p>"},{"location":"Reference/classes/Schedule/create_plan/#arguments","title":"Arguments","text":"argument description type/default <code>target</code> The target platform. Defaults to <code>acc.Target.HOST</code> <code>Target</code>"},{"location":"Reference/classes/Schedule/create_plan/#returns","title":"Returns","text":"<p><code>Plan</code></p>"},{"location":"Reference/classes/Schedule/create_plan/#examples","title":"Examples","text":"<p>TODO</p>"},{"location":"Reference/classes/Schedule/get_indices/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/get_indices/#accerascheduleget_indices","title":"<code>accera.Schedule.get_indices()</code>","text":"<p>Gets the iteration space dimensions for a schedule.</p>"},{"location":"Reference/classes/Schedule/get_indices/#returns","title":"Returns","text":"<p>Tuple of <code>Index</code></p>"},{"location":"Reference/classes/Schedule/get_indices/#examples","title":"Examples","text":"<p>Get the iteration space dimensions for a 3-dimensional nest:</p> <pre><code>i, j, k = schedule.get_indices()\n</code></pre>"},{"location":"Reference/classes/Schedule/is_valid_loop_order/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/is_valid_loop_order/#accerascheduleis_valid_loop_orderorder","title":"<code>accera.Schedule.is_valid_loop_order(*order)</code>","text":"<p>The <code>is_valid_loop_order</code> function determines if an order of indices is valid. For a description of valid schedule orders, refer to reorder.</p>"},{"location":"Reference/classes/Schedule/is_valid_loop_order/#arguments","title":"Arguments","text":"argument description type/default <code>*order</code> The order of indices to check for validity variable <code>Index</code> arguments"},{"location":"Reference/classes/Schedule/is_valid_loop_order/#examples","title":"Examples","text":"<p>Checks if an order is valid:</p> <pre><code>print(schedule.is_valid_loop_order(k, i, j))\n</code></pre> <p>Uses this function as part of a parameter filter to determine which permutations of loop order parameters are valid:</p> <pre><code>P1, P2, P2, P4, P5, loop_order = acc.create_parameters()\nschedule.reorder(order=loop_order)\n\ndef my_filter(parameters_choice):\n    P1, P2, P3, P4, P5, loop_order = parameters_choice\n\n    return P1 &gt; P2 \\\n        and P3 &gt; P4 \\\n        and P1 * P5 &lt; P3 \\\n        and P2 * P5 &lt; P4 \\\n        and schedule.is_valid_loop_order(loop_order)\n\n parameters = acc.create_parameter_grid({\n        P1: [64, 128, 256],\n        P2: [32, 128], \n        P3: [16, 32, 128],\n        P4: [8, 64],\n        P5: [4],\n        loop_order: (i, j, k, ii, jj, kk)\n    }, my_filter)\n</code></pre>"},{"location":"Reference/classes/Schedule/pad/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/pad/#acceraschedulepadindex-size","title":"<code>accera.Schedule.pad(index, size)</code>","text":"<p>Pads the beginning of a specified dimension of the iteration-space with empty (no-op) elements.</p>"},{"location":"Reference/classes/Schedule/pad/#arguments","title":"Arguments","text":"argument description type/default <code>index</code> The dimension to pad <code>Index</code> <code>size</code> The number of elements to pad non-negative integer"},{"location":"Reference/classes/Schedule/pad/#examples","title":"Examples","text":"<p>Pads the beginning of dimension <code>i</code> with 10 empty elements</p> <pre><code>schedule.pad(i, 10)\n</code></pre>"},{"location":"Reference/classes/Schedule/reorder/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/reorder/#acceraschedulereorderorder-args","title":"<code>accera.Schedule.reorder(order, *args)</code>","text":"<p>The <code>reorder</code> transformation sets the order of the indices in the schedule.</p> <p>These orders are not allowed: 1. The outer dimension created by a <code>split</code> transformation must always precede the corresponding inner dimension. 2. The fusing dimension created by a <code>fuse</code> operation must always precede any unfused dimensions.</p>"},{"location":"Reference/classes/Schedule/reorder/#arguments","title":"Arguments","text":"argument description type/default <code>order</code> Either the order of indices to set or the outermost index if using variable arguments tuple of <code>Index</code> or <code>Index</code>. <code>*args</code> Optional variable arguments containing subsequent indices to set variable <code>Index</code> arguments"},{"location":"Reference/classes/Schedule/reorder/#examples","title":"Examples","text":"<p>Reorder a schedule by moving the <code>k</code> dimension to the outermost loop:</p> <pre><code>schedule.reorder(k, i, j)\n</code></pre> <p>Using a tuple to reorder a schedule. This overloaded form is better suited for parameters:</p> <pre><code>schedule.reorder(order=(k, i, j))\n</code></pre>"},{"location":"Reference/classes/Schedule/skew/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/skew/#accerascheduleskewindex-reference_index-unroll_loops_smaller_than","title":"<code>accera.Schedule.skew(index, reference_index [, unroll_loops_smaller_than])</code>","text":"<p>Transforms a dimension with respect to a reference dimension into a parallelogram by padding with empty elements.</p>"},{"location":"Reference/classes/Schedule/skew/#arguments","title":"Arguments","text":"argument description type/default <code>index</code> The dimension to skew <code>Index</code> <code>reference_index</code> The reference dimension <code>Index</code> <code>unroll_loops_smaller_than</code> Unroll loops that are smaller than this range (non-inclusive) non-negative integer"},{"location":"Reference/classes/Schedule/skew/#examples","title":"Examples","text":"<p>Skew dimension <code>i</code> with respect to dimension <code>j</code>:</p> <pre><code>schedule.skew(i, j)\n</code></pre> <p>Skew dimension <code>j</code> with respect to dimension <code>i</code>, and unroll if the resulting loops are smaller than 3:</p> <pre><code>schedule.skew(j, i, unroll_loops_smaller_than=3)\n</code></pre>"},{"location":"Reference/classes/Schedule/split/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/split/#acceraschedulesplitindex-size","title":"<code>accera.Schedule.split(index, size)</code>","text":"<p>The <code>split</code> transformation takes a dimension <code>i</code> and a <code>size</code>, modifies <code>i</code>, and creates a new dimension <code>ii</code>.</p> <p>Assume that the original size of dimension <code>i</code> was n: The <code>split</code> transformation splits dimension <code>i</code> into ceil(n/size) parts of size <code>size</code>, arranges each of those parts along dimension <code>ii</code>, and stacks the ceil(n/size) parts along dimension <code>i</code>.</p> <p>If the split size does not divide the dimension size, empty elements are added such that the split size does divide the dimension size.</p>"},{"location":"Reference/classes/Schedule/split/#arguments","title":"Arguments","text":"argument description type/default <code>index</code> The dimension to split <code>Index</code> <code>size</code> The split size non-negative integer"},{"location":"Reference/classes/Schedule/split/#returns","title":"Returns","text":"<p><code>Index</code> for the new inner dimension</p>"},{"location":"Reference/classes/Schedule/split/#examples","title":"Examples","text":"<p>Split the <code>i</code> dimension by 5, creating a new dimension <code>ii</code>:</p> <pre><code>ii = schedule.split(j, 5)\n</code></pre>"},{"location":"Reference/classes/Schedule/tile/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Schedule/tile/#accerascheduletileshape","title":"<code>accera.Schedule.tile(shape)</code>","text":"<p>The <code>tile</code> transformation is a convenience syntax that takes a tuple of indices and a tuple of sizes, and splits each index by the corresponding size. The indices involved in the split are then ordered such that all the outer indices precede all of their respective inner indices.</p>"},{"location":"Reference/classes/Schedule/tile/#arguments","title":"Arguments","text":"argument description type/default <code>shape</code> Mapping of indices to tile sizes dict of <code>Index</code> and non-negative integers"},{"location":"Reference/classes/Schedule/tile/#returns","title":"Returns","text":"<p>Tuple of <code>Index</code> representing the new inner dimensions.</p>"},{"location":"Reference/classes/Schedule/tile/#examples","title":"Examples","text":"<p>Tile the <code>i</code>, <code>j</code>, and <code>k</code> dimensions by 8, 2, and 3, respectively.</p> <pre><code>ii, jj, kk = schedule.tile({\n    i: 8,\n    j: 2,\n    k: 3\n})\n</code></pre>"},{"location":"Reference/classes/Target/Architecture/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Target/Architecture/#acceratargetarchitecture","title":"<code>accera.Target.Architecture</code>","text":"<p>Defines the supported target architectures.</p> type description <code>accera.Target.Architecture.HOST</code> The host computer's architecture <code>accera.Target.Architecture.ARM</code> The ARM architecture <code>accera.Target.Architecture.AARCH64</code> The 64-bit ARM architecture <code>accera.Target.Architecture.X86</code> The 32-bit x86 architecture <code>accera.Target.Architecture.X86_64</code> The 64-bit x86 architecture <p>TODO: AARCH64?</p>"},{"location":"Reference/classes/Target/Category/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Target/Category/#acceratargetcategory","title":"<code>accera.Target.Category</code>","text":"<p>Defines the target processor category.</p> type description <code>accera.Target.Category.CPU</code> <code>accera.Target.Category.GPU</code>"},{"location":"Reference/classes/Target/Model/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Target/Model/#acceratargetmodel","title":"<code>accera.Target.Model</code>","text":"<p>Defines constants for some well-known CPU models.</p> type description <code>accera.Target.Model.AMD_1200</code> AMD 1200 <code>accera.Target.Model.AMD_1300X</code> AMD 1300X <code>accera.Target.Model.AMD_1400</code> AMD 1400 <code>accera.Target.Model.AMD_1500X</code> AMD 1500X <code>accera.Target.Model.AMD_1600</code> AMD 1600 <code>accera.Target.Model.AMD_1600X</code> AMD 1600X <code>accera.Target.Model.AMD_1700</code> AMD 1700 <code>accera.Target.Model.AMD_1700X</code> AMD 1700X <code>accera.Target.Model.AMD_1800X</code> AMD 1800X <code>accera.Target.Model.AMD_1900X</code> AMD 1900X <code>accera.Target.Model.AMD_1920X</code> AMD 1920X <code>accera.Target.Model.AMD_1950X</code> AMD 1950X <code>accera.Target.Model.AMD_200GE</code> AMD 200GE <code>accera.Target.Model.AMD_2200G</code> AMD 2200G <code>accera.Target.Model.AMD_2200GE</code> AMD 2200GE <code>accera.Target.Model.AMD_2200U</code> AMD 2200U <code>accera.Target.Model.AMD_220GE</code> AMD 220GE <code>accera.Target.Model.AMD_2300U</code> AMD 2300U <code>accera.Target.Model.AMD_2300X</code> AMD 2300X <code>accera.Target.Model.AMD_2400G</code> AMD 2400G <code>accera.Target.Model.AMD_2400GE</code> AMD 2400GE <code>accera.Target.Model.AMD_240GE</code> AMD 240GE <code>accera.Target.Model.AMD_2500U</code> AMD 2500U <code>accera.Target.Model.AMD_2500X</code> AMD 2500X <code>accera.Target.Model.AMD_2600</code> AMD 2600 <code>accera.Target.Model.AMD_2600E</code> AMD 2600E <code>accera.Target.Model.AMD_2600H</code> AMD 2600H <code>accera.Target.Model.AMD_2600X</code> AMD 2600X <code>accera.Target.Model.AMD_2700</code> AMD 2700 <code>accera.Target.Model.AMD_2700E</code> AMD 2700E <code>accera.Target.Model.AMD_2700U</code> AMD 2700U <code>accera.Target.Model.AMD_2700X</code> AMD 2700X <code>accera.Target.Model.AMD_2700X_GOLD_EDITION</code> AMD 2700X Gold Edition <code>accera.Target.Model.AMD_2800H</code> AMD 2800H <code>accera.Target.Model.AMD_2920X</code> AMD 2920X <code>accera.Target.Model.AMD_2950X</code> AMD 2950X <code>accera.Target.Model.AMD_2970WX</code> AMD 2970WX <code>accera.Target.Model.AMD_2990WX</code> AMD 2990WX <code>accera.Target.Model.AMD_3000G</code> AMD 3000G <code>accera.Target.Model.AMD_300U</code> AMD 300U <code>accera.Target.Model.AMD_3050U</code> AMD 3050U <code>accera.Target.Model.AMD_3101</code> AMD 3101 <code>accera.Target.Model.AMD_3150U</code> AMD 3150U <code>accera.Target.Model.AMD_3151</code> AMD 3151 <code>accera.Target.Model.AMD_3200G</code> AMD 3200G <code>accera.Target.Model.AMD_3200U</code> AMD 3200U <code>accera.Target.Model.AMD_3201</code> AMD 3201 <code>accera.Target.Model.AMD_3250U</code> AMD 3250U <code>accera.Target.Model.AMD_3251</code> AMD 3251 <code>accera.Target.Model.AMD_3255</code> AMD 3255 <code>accera.Target.Model.AMD_3300U</code> AMD 3300U <code>accera.Target.Model.AMD_3301</code> AMD 3301 <code>accera.Target.Model.AMD_3351</code> AMD 3351 <code>accera.Target.Model.AMD_3400G</code> AMD 3400G <code>accera.Target.Model.AMD_3401</code> AMD 3401 <code>accera.Target.Model.AMD_3451</code> AMD 3451 <code>accera.Target.Model.AMD_3500</code> AMD 3500 <code>accera.Target.Model.AMD_3500U</code> AMD 3500U <code>accera.Target.Model.AMD_3500X</code> AMD 3500X <code>accera.Target.Model.AMD_3550H</code> AMD 3550H <code>accera.Target.Model.AMD_3580U</code> AMD 3580U <code>accera.Target.Model.AMD_3600</code> AMD 3600 <code>accera.Target.Model.AMD_3600X</code> AMD 3600X <code>accera.Target.Model.AMD_3600XT</code> AMD 3600XT <code>accera.Target.Model.AMD_3700U</code> AMD 3700U <code>accera.Target.Model.AMD_3700X</code> AMD 3700X <code>accera.Target.Model.AMD_3750H</code> AMD 3750H <code>accera.Target.Model.AMD_3780U</code> AMD 3780U <code>accera.Target.Model.AMD_3800X</code> AMD 3800X <code>accera.Target.Model.AMD_3800XT</code> AMD 3800XT <code>accera.Target.Model.AMD_3900</code> AMD 3900 <code>accera.Target.Model.AMD_3900X</code> AMD 3900X <code>accera.Target.Model.AMD_3900XT</code> AMD 3900XT <code>accera.Target.Model.AMD_3950X</code> AMD 3950X <code>accera.Target.Model.AMD_3960X</code> AMD 3960X <code>accera.Target.Model.AMD_3970X</code> AMD 3970X <code>accera.Target.Model.AMD_3980X</code> AMD 3980X <code>accera.Target.Model.AMD_3990X</code> AMD 3990X <code>accera.Target.Model.AMD_4300G</code> AMD 4300G <code>accera.Target.Model.AMD_4300GE</code> AMD 4300GE <code>accera.Target.Model.AMD_4300U</code> AMD 4300U <code>accera.Target.Model.AMD_4500U</code> AMD 4500U <code>accera.Target.Model.AMD_4600G</code> AMD 4600G <code>accera.Target.Model.AMD_4600GE</code> AMD 4600GE <code>accera.Target.Model.AMD_4600H</code> AMD 4600H <code>accera.Target.Model.AMD_4600HS</code> AMD 4600HS <code>accera.Target.Model.AMD_4600U</code> AMD 4600U <code>accera.Target.Model.AMD_4680U</code> AMD 4680U <code>accera.Target.Model.AMD_4700G</code> AMD 4700G <code>accera.Target.Model.AMD_4700GE</code> AMD 4700GE <code>accera.Target.Model.AMD_4700U</code> AMD 4700U <code>accera.Target.Model.AMD_4800H</code> AMD 4800H <code>accera.Target.Model.AMD_4800HS</code> AMD 4800HS <code>accera.Target.Model.AMD_4800U</code> AMD 4800U <code>accera.Target.Model.AMD_4900H</code> AMD 4900H <code>accera.Target.Model.AMD_4900HS</code> AMD 4900HS <code>accera.Target.Model.AMD_4980U</code> AMD 4980U <code>accera.Target.Model.AMD_5300G</code> AMD 5300G <code>accera.Target.Model.AMD_5300GE</code> AMD 5300GE <code>accera.Target.Model.AMD_5300U</code> AMD 5300U <code>accera.Target.Model.AMD_5400U</code> AMD 5400U <code>accera.Target.Model.AMD_5500U</code> AMD 5500U <code>accera.Target.Model.AMD_5600G</code> AMD 5600G <code>accera.Target.Model.AMD_5600GE</code> AMD 5600GE <code>accera.Target.Model.AMD_5600H</code> AMD 5600H <code>accera.Target.Model.AMD_5600HS</code> AMD 5600HS <code>accera.Target.Model.AMD_5600U</code> AMD 5600U <code>accera.Target.Model.AMD_5600X</code> AMD 5600X <code>accera.Target.Model.AMD_5700G</code> AMD 5700G <code>accera.Target.Model.AMD_5700GE</code> AMD 5700GE <code>accera.Target.Model.AMD_5700U</code> AMD 5700U <code>accera.Target.Model.AMD_5800</code> AMD 5800 <code>accera.Target.Model.AMD_5800H</code> AMD 5800H <code>accera.Target.Model.AMD_5800HS</code> AMD 5800HS <code>accera.Target.Model.AMD_5800U</code> AMD 5800U <code>accera.Target.Model.AMD_5800X</code> AMD 5800X <code>accera.Target.Model.AMD_5900</code> AMD 5900 <code>accera.Target.Model.AMD_5900HS</code> AMD 5900HS <code>accera.Target.Model.AMD_5900HX</code> AMD 5900HX <code>accera.Target.Model.AMD_5900X</code> AMD 5900X <code>accera.Target.Model.AMD_5950X</code> AMD 5950X <code>accera.Target.Model.AMD_5980HS</code> AMD 5980HS <code>accera.Target.Model.AMD_5980HX</code> AMD 5980HX <code>accera.Target.Model.AMD_7232P</code> AMD 7232P <code>accera.Target.Model.AMD_7251</code> AMD 7251 <code>accera.Target.Model.AMD_7252</code> AMD 7252 <code>accera.Target.Model.AMD_7261</code> AMD 7261 <code>accera.Target.Model.AMD_7262</code> AMD 7262 <code>accera.Target.Model.AMD_7272</code> AMD 7272 <code>accera.Target.Model.AMD_7281</code> AMD 7281 <code>accera.Target.Model.AMD_7282</code> AMD 7282 <code>accera.Target.Model.AMD_72F3</code> AMD 72F3 <code>accera.Target.Model.AMD_7301</code> AMD 7301 <code>accera.Target.Model.AMD_7302</code> AMD 7302 <code>accera.Target.Model.AMD_7302P</code> AMD 7302P <code>accera.Target.Model.AMD_7313</code> AMD 7313 <code>accera.Target.Model.AMD_7313P</code> AMD 7313P <code>accera.Target.Model.AMD_7343</code> AMD 7343 <code>accera.Target.Model.AMD_7351</code> AMD 7351 <code>accera.Target.Model.AMD_7351P</code> AMD 7351P <code>accera.Target.Model.AMD_7352</code> AMD 7352 <code>accera.Target.Model.AMD_7371</code> AMD 7371 <code>accera.Target.Model.AMD_73F3</code> AMD 73F3 <code>accera.Target.Model.AMD_7401</code> AMD 7401 <code>accera.Target.Model.AMD_7401P</code> AMD 7401P <code>accera.Target.Model.AMD_7402</code> AMD 7402 <code>accera.Target.Model.AMD_7402P</code> AMD 7402P <code>accera.Target.Model.AMD_7413</code> AMD 7413 <code>accera.Target.Model.AMD_7443</code> AMD 7443 <code>accera.Target.Model.AMD_7443P</code> AMD 7443P <code>accera.Target.Model.AMD_7451</code> AMD 7451 <code>accera.Target.Model.AMD_7452</code> AMD 7452 <code>accera.Target.Model.AMD_7453</code> AMD 7453 <code>accera.Target.Model.AMD_74F3</code> AMD 74F3 <code>accera.Target.Model.AMD_7501</code> AMD 7501 <code>accera.Target.Model.AMD_7502</code> AMD 7502 <code>accera.Target.Model.AMD_7502P</code> AMD 7502P <code>accera.Target.Model.AMD_7513</code> AMD 7513 <code>accera.Target.Model.AMD_7532</code> AMD 7532 <code>accera.Target.Model.AMD_7542</code> AMD 7542 <code>accera.Target.Model.AMD_7543</code> AMD 7543 <code>accera.Target.Model.AMD_7543P</code> AMD 7543P <code>accera.Target.Model.AMD_7551</code> AMD 7551 <code>accera.Target.Model.AMD_7551P</code> AMD 7551P <code>accera.Target.Model.AMD_7552</code> AMD 7552 <code>accera.Target.Model.AMD_75F3</code> AMD 75F3 <code>accera.Target.Model.AMD_7601</code> AMD 7601 <code>accera.Target.Model.AMD_7642</code> AMD 7642 <code>accera.Target.Model.AMD_7643</code> AMD 7643 <code>accera.Target.Model.AMD_7662</code> AMD 7662 <code>accera.Target.Model.AMD_7663</code> AMD 7663 <code>accera.Target.Model.AMD_7702</code> AMD 7702 <code>accera.Target.Model.AMD_7702P</code> AMD 7702P <code>accera.Target.Model.AMD_7713</code> AMD 7713 <code>accera.Target.Model.AMD_7713P</code> AMD 7713P <code>accera.Target.Model.AMD_7742</code> AMD 7742 <code>accera.Target.Model.AMD_7763</code> AMD 7763 <code>accera.Target.Model.AMD_7F32</code> AMD 7F32 <code>accera.Target.Model.AMD_7F52</code> AMD 7F52 <code>accera.Target.Model.AMD_7F72</code> AMD 7F72 <code>accera.Target.Model.AMD_7H12</code> AMD 7H12 <code>accera.Target.Model.AMD_7V12</code> AMD 7V12 <code>accera.Target.Model.AMD_FIREFLIGHT</code> AMD FireFlight <code>accera.Target.Model.AMD_PRO_1200</code> AMD PRO 1200 <code>accera.Target.Model.AMD_PRO_1300</code> AMD PRO 1300 <code>accera.Target.Model.AMD_PRO_1500</code> AMD PRO 1500 <code>accera.Target.Model.AMD_PRO_1600</code> AMD PRO 1600 <code>accera.Target.Model.AMD_PRO_1700</code> AMD PRO 1700 <code>accera.Target.Model.AMD_PRO_1700X</code> AMD PRO 1700X <code>accera.Target.Model.AMD_PRO_200GE</code> AMD PRO 200GE <code>accera.Target.Model.AMD_PRO_2200G</code> AMD PRO 2200G <code>accera.Target.Model.AMD_PRO_2200GE</code> AMD PRO 2200GE <code>accera.Target.Model.AMD_PRO_2300U</code> AMD PRO 2300U <code>accera.Target.Model.AMD_PRO_2400G</code> AMD PRO 2400G <code>accera.Target.Model.AMD_PRO_2400GE</code> AMD PRO 2400GE <code>accera.Target.Model.AMD_PRO_2500U</code> AMD PRO 2500U <code>accera.Target.Model.AMD_PRO_2600</code> AMD PRO 2600 <code>accera.Target.Model.AMD_PRO_2700</code> AMD PRO 2700 <code>accera.Target.Model.AMD_PRO_2700U</code> AMD PRO 2700U <code>accera.Target.Model.AMD_PRO_2700X</code> AMD PRO 2700X <code>accera.Target.Model.AMD_PRO_300GE</code> AMD PRO 300GE <code>accera.Target.Model.AMD_PRO_300U</code> AMD PRO 300U <code>accera.Target.Model.AMD_PRO_3200G</code> AMD PRO 3200G <code>accera.Target.Model.AMD_PRO_3200GE</code> AMD PRO 3200GE <code>accera.Target.Model.AMD_PRO_3300U</code> AMD PRO 3300U <code>accera.Target.Model.AMD_PRO_3400G</code> AMD PRO 3400G <code>accera.Target.Model.AMD_PRO_3400GE</code> AMD PRO 3400GE <code>accera.Target.Model.AMD_PRO_3500U</code> AMD PRO 3500U <code>accera.Target.Model.AMD_PRO_3600</code> AMD PRO 3600 <code>accera.Target.Model.AMD_PRO_3700</code> AMD PRO 3700 <code>accera.Target.Model.AMD_PRO_3700U</code> AMD PRO 3700U <code>accera.Target.Model.AMD_PRO_3900</code> AMD PRO 3900 <code>accera.Target.Model.AMD_PRO_4350G</code> AMD PRO 4350G <code>accera.Target.Model.AMD_PRO_4350GE</code> AMD PRO 4350GE <code>accera.Target.Model.AMD_PRO_4450U</code> AMD PRO 4450U <code>accera.Target.Model.AMD_PRO_4650G</code> AMD PRO 4650G <code>accera.Target.Model.AMD_PRO_4650GE</code> AMD PRO 4650GE <code>accera.Target.Model.AMD_PRO_4650U</code> AMD PRO 4650U <code>accera.Target.Model.AMD_PRO_4750G</code> AMD PRO 4750G <code>accera.Target.Model.AMD_PRO_4750GE</code> AMD PRO 4750GE <code>accera.Target.Model.AMD_PRO_4750U</code> AMD PRO 4750U <code>accera.Target.Model.AMD_PRO_5350G</code> AMD PRO 5350G <code>accera.Target.Model.AMD_PRO_5350GE</code> AMD PRO 5350GE <code>accera.Target.Model.AMD_PRO_5450U</code> AMD PRO 5450U <code>accera.Target.Model.AMD_PRO_5650G</code> AMD PRO 5650G <code>accera.Target.Model.AMD_PRO_5650GE</code> AMD PRO 5650GE <code>accera.Target.Model.AMD_PRO_5650U</code> AMD PRO 5650U <code>accera.Target.Model.AMD_PRO_5750G</code> AMD PRO 5750G <code>accera.Target.Model.AMD_PRO_5750GE</code> AMD PRO 5750GE <code>accera.Target.Model.AMD_PRO_5850U</code> AMD PRO 5850U <code>accera.Target.Model.AMD_R1102G</code> AMD R1102G <code>accera.Target.Model.AMD_R1305G</code> AMD R1305G <code>accera.Target.Model.AMD_R1505G</code> AMD R1505G <code>accera.Target.Model.AMD_R1606G</code> AMD R1606G <code>accera.Target.Model.AMD_V1202B</code> AMD V1202B <code>accera.Target.Model.AMD_V1404I</code> AMD V1404I <code>accera.Target.Model.AMD_V1500B</code> AMD V1500B <code>accera.Target.Model.AMD_V1605B</code> AMD V1605B <code>accera.Target.Model.AMD_V1756B</code> AMD V1756B <code>accera.Target.Model.AMD_V1780B</code> AMD V1780B <code>accera.Target.Model.AMD_V1807B</code> AMD V1807B <code>accera.Target.Model.AMD_V2516</code> AMD V2516 <code>accera.Target.Model.AMD_V2546</code> AMD V2546 <code>accera.Target.Model.AMD_V2718</code> AMD V2718 <code>accera.Target.Model.AMD_V2748</code> AMD V2748 <code>accera.Target.Model.ARM_CORTEX_M4</code> ARM Cortex-M4 <code>accera.Target.Model.ARM_CORTEX_M4F</code> ARM Cortex-M4F <code>accera.Target.Model.APPLE_M1_MAX</code> Apple M1 Max <code>accera.Target.Model.INTEL_1000G1</code> Intel 1000G1 <code>accera.Target.Model.INTEL_1000G4</code> Intel 1000G4 <code>accera.Target.Model.INTEL_1005G1</code> Intel 1005G1 <code>accera.Target.Model.INTEL_10100</code> Intel 10100 <code>accera.Target.Model.INTEL_10100F</code> Intel 10100F <code>accera.Target.Model.INTEL_10100T</code> Intel 10100T <code>accera.Target.Model.INTEL_10300</code> Intel 10300 <code>accera.Target.Model.INTEL_10300T</code> Intel 10300T <code>accera.Target.Model.INTEL_1030G4</code> Intel 1030G4 <code>accera.Target.Model.INTEL_1030G7</code> Intel 1030G7 <code>accera.Target.Model.INTEL_10320</code> Intel 10320 <code>accera.Target.Model.INTEL_1035G1</code> Intel 1035G1 <code>accera.Target.Model.INTEL_1035G4</code> Intel 1035G4 <code>accera.Target.Model.INTEL_1035G7</code> Intel 1035G7 <code>accera.Target.Model.INTEL_10400</code> Intel 10400 <code>accera.Target.Model.INTEL_10400F</code> Intel 10400F <code>accera.Target.Model.INTEL_10400T</code> Intel 10400T <code>accera.Target.Model.INTEL_10500</code> Intel 10500 <code>accera.Target.Model.INTEL_10500T</code> Intel 10500T <code>accera.Target.Model.INTEL_10600</code> Intel 10600 <code>accera.Target.Model.INTEL_10600K</code> Intel 10600K <code>accera.Target.Model.INTEL_10600KF</code> Intel 10600KF <code>accera.Target.Model.INTEL_10600T</code> Intel 10600T <code>accera.Target.Model.INTEL_1060G7</code> Intel 1060G7 <code>accera.Target.Model.INTEL_1065G7</code> Intel 1065G7 <code>accera.Target.Model.INTEL_1068G7</code> Intel 1068G7 <code>accera.Target.Model.INTEL_10700</code> Intel 10700 <code>accera.Target.Model.INTEL_10700F</code> Intel 10700F <code>accera.Target.Model.INTEL_10700K</code> Intel 10700K <code>accera.Target.Model.INTEL_10700KF</code> Intel 10700KF <code>accera.Target.Model.INTEL_10700T</code> Intel 10700T <code>accera.Target.Model.INTEL_10850K</code> Intel 10850K <code>accera.Target.Model.INTEL_10900</code> Intel 10900 <code>accera.Target.Model.INTEL_10900F</code> Intel 10900F <code>accera.Target.Model.INTEL_10900K</code> Intel 10900K <code>accera.Target.Model.INTEL_10900KF</code> Intel 10900KF <code>accera.Target.Model.INTEL_10900T</code> Intel 10900T <code>accera.Target.Model.INTEL_10910</code> Intel 10910 <code>accera.Target.Model.INTEL_11100B</code> Intel 11100B <code>accera.Target.Model.INTEL_1115G7</code> Intel 1115G7 <code>accera.Target.Model.INTEL_1125G7</code> Intel 1125G7 <code>accera.Target.Model.INTEL_1135G7</code> Intel 1135G7 <code>accera.Target.Model.INTEL_11400</code> Intel 11400 <code>accera.Target.Model.INTEL_11400F</code> Intel 11400F <code>accera.Target.Model.INTEL_11400T</code> Intel 11400T <code>accera.Target.Model.INTEL_1145G7</code> Intel 1145G7 <code>accera.Target.Model.INTEL_11500</code> Intel 11500 <code>accera.Target.Model.INTEL_11500B</code> Intel 11500B <code>accera.Target.Model.INTEL_11500T</code> Intel 11500T <code>accera.Target.Model.INTEL_1155G7</code> Intel 1155G7 <code>accera.Target.Model.INTEL_11600</code> Intel 11600 <code>accera.Target.Model.INTEL_11600K</code> Intel 11600K <code>accera.Target.Model.INTEL_11600KF</code> Intel 11600KF <code>accera.Target.Model.INTEL_11600T</code> Intel 11600T <code>accera.Target.Model.INTEL_1165G7</code> Intel 1165G7 <code>accera.Target.Model.INTEL_11700</code> Intel 11700 <code>accera.Target.Model.INTEL_11700B</code> Intel 11700B <code>accera.Target.Model.INTEL_11700F</code> Intel 11700F <code>accera.Target.Model.INTEL_11700K</code> Intel 11700K <code>accera.Target.Model.INTEL_11700KF</code> Intel 11700KF <code>accera.Target.Model.INTEL_11700T</code> Intel 11700T <code>accera.Target.Model.INTEL_11850H</code> Intel 11850H <code>accera.Target.Model.INTEL_1185G7</code> Intel 1185G7 <code>accera.Target.Model.INTEL_11900</code> Intel 11900 <code>accera.Target.Model.INTEL_11900F</code> Intel 11900F <code>accera.Target.Model.INTEL_11900K</code> Intel 11900K <code>accera.Target.Model.INTEL_11900KB</code> Intel 11900KB <code>accera.Target.Model.INTEL_11900KF</code> Intel 11900KF <code>accera.Target.Model.INTEL_11900T</code> Intel 11900T <code>accera.Target.Model.INTEL_1195G7</code> Intel 1195G7 <code>accera.Target.Model.INTEL_2104G</code> Intel 2104G <code>accera.Target.Model.INTEL_2124</code> Intel 2124 <code>accera.Target.Model.INTEL_2124G</code> Intel 2124G <code>accera.Target.Model.INTEL_2126G</code> Intel 2126G <code>accera.Target.Model.INTEL_2134</code> Intel 2134 <code>accera.Target.Model.INTEL_2136</code> Intel 2136 <code>accera.Target.Model.INTEL_2144G</code> Intel 2144G <code>accera.Target.Model.INTEL_2146G</code> Intel 2146G <code>accera.Target.Model.INTEL_2174G</code> Intel 2174G <code>accera.Target.Model.INTEL_2176G</code> Intel 2176G <code>accera.Target.Model.INTEL_2186G</code> Intel 2186G <code>accera.Target.Model.INTEL_2314</code> Intel 2314 <code>accera.Target.Model.INTEL_2324G</code> Intel 2324G <code>accera.Target.Model.INTEL_2334</code> Intel 2334 <code>accera.Target.Model.INTEL_2336</code> Intel 2336 <code>accera.Target.Model.INTEL_2356G</code> Intel 2356G <code>accera.Target.Model.INTEL_2374G</code> Intel 2374G <code>accera.Target.Model.INTEL_2378</code> Intel 2378 <code>accera.Target.Model.INTEL_2378G</code> Intel 2378G <code>accera.Target.Model.INTEL_2386G</code> Intel 2386G <code>accera.Target.Model.INTEL_2388G</code> Intel 2388G <code>accera.Target.Model.INTEL_3204</code> Intel 3204 <code>accera.Target.Model.INTEL_4108</code> Intel 4108 <code>accera.Target.Model.INTEL_4109T</code> Intel 4109T <code>accera.Target.Model.INTEL_4110</code> Intel 4110 <code>accera.Target.Model.INTEL_4112</code> Intel 4112 <code>accera.Target.Model.INTEL_4114</code> Intel 4114 <code>accera.Target.Model.INTEL_4208</code> Intel 4208 <code>accera.Target.Model.INTEL_4209T</code> Intel 4209T <code>accera.Target.Model.INTEL_4210</code> Intel 4210 <code>accera.Target.Model.INTEL_4210R</code> Intel 4210R <code>accera.Target.Model.INTEL_4214</code> Intel 4214 <code>accera.Target.Model.INTEL_4214R</code> Intel 4214R <code>accera.Target.Model.INTEL_4214Y</code> Intel 4214Y <code>accera.Target.Model.INTEL_4215</code> Intel 4215 <code>accera.Target.Model.INTEL_4215R</code> Intel 4215R <code>accera.Target.Model.INTEL_4216</code> Intel 4216 <code>accera.Target.Model.INTEL_5215</code> Intel 5215 <code>accera.Target.Model.INTEL_5215L</code> Intel 5215L <code>accera.Target.Model.INTEL_5215M</code> Intel 5215M <code>accera.Target.Model.INTEL_5217</code> Intel 5217 <code>accera.Target.Model.INTEL_5218</code> Intel 5218 <code>accera.Target.Model.INTEL_5218B</code> Intel 5218B <code>accera.Target.Model.INTEL_5218N</code> Intel 5218N <code>accera.Target.Model.INTEL_5218R</code> Intel 5218R <code>accera.Target.Model.INTEL_5218T</code> Intel 5218T <code>accera.Target.Model.INTEL_5220</code> Intel 5220 <code>accera.Target.Model.INTEL_5220R</code> Intel 5220R <code>accera.Target.Model.INTEL_5220S</code> Intel 5220S <code>accera.Target.Model.INTEL_5220T</code> Intel 5220T <code>accera.Target.Model.INTEL_5222</code> Intel 5222 <code>accera.Target.Model.INTEL_6035</code> Intel 6035 <code>accera.Target.Model.INTEL_6098P</code> Intel 6098P <code>accera.Target.Model.INTEL_6100</code> Intel 6100 <code>accera.Target.Model.INTEL_6100T</code> Intel 6100T <code>accera.Target.Model.INTEL_6209U</code> Intel 6209U <code>accera.Target.Model.INTEL_6210U</code> Intel 6210U <code>accera.Target.Model.INTEL_6212U</code> Intel 6212U <code>accera.Target.Model.INTEL_6222V</code> Intel 6222V <code>accera.Target.Model.INTEL_6226</code> Intel 6226 <code>accera.Target.Model.INTEL_6226R</code> Intel 6226R <code>accera.Target.Model.INTEL_6230</code> Intel 6230 <code>accera.Target.Model.INTEL_6230N</code> Intel 6230N <code>accera.Target.Model.INTEL_6230R</code> Intel 6230R <code>accera.Target.Model.INTEL_6230T</code> Intel 6230T <code>accera.Target.Model.INTEL_6234</code> Intel 6234 <code>accera.Target.Model.INTEL_6238</code> Intel 6238 <code>accera.Target.Model.INTEL_6238L</code> Intel 6238L <code>accera.Target.Model.INTEL_6238M</code> Intel 6238M <code>accera.Target.Model.INTEL_6238R</code> Intel 6238R <code>accera.Target.Model.INTEL_6238T</code> Intel 6238T <code>accera.Target.Model.INTEL_6240</code> Intel 6240 <code>accera.Target.Model.INTEL_6240L</code> Intel 6240L <code>accera.Target.Model.INTEL_6240M</code> Intel 6240M <code>accera.Target.Model.INTEL_6240R</code> Intel 6240R <code>accera.Target.Model.INTEL_6240Y</code> Intel 6240Y <code>accera.Target.Model.INTEL_6242</code> Intel 6242 <code>accera.Target.Model.INTEL_6242R</code> Intel 6242R <code>accera.Target.Model.INTEL_6244</code> Intel 6244 <code>accera.Target.Model.INTEL_6246</code> Intel 6246 <code>accera.Target.Model.INTEL_6246R</code> Intel 6246R <code>accera.Target.Model.INTEL_6248</code> Intel 6248 <code>accera.Target.Model.INTEL_6248R</code> Intel 6248R <code>accera.Target.Model.INTEL_6252</code> Intel 6252 <code>accera.Target.Model.INTEL_6252N</code> Intel 6252N <code>accera.Target.Model.INTEL_6254</code> Intel 6254 <code>accera.Target.Model.INTEL_6258R</code> Intel 6258R <code>accera.Target.Model.INTEL_6262V</code> Intel 6262V <code>accera.Target.Model.INTEL_6300</code> Intel 6300 <code>accera.Target.Model.INTEL_6300T</code> Intel 6300T <code>accera.Target.Model.INTEL_6320</code> Intel 6320 <code>accera.Target.Model.INTEL_6400</code> Intel 6400 <code>accera.Target.Model.INTEL_6400T</code> Intel 6400T <code>accera.Target.Model.INTEL_6402P</code> Intel 6402P <code>accera.Target.Model.INTEL_6500</code> Intel 6500 <code>accera.Target.Model.INTEL_6500T</code> Intel 6500T <code>accera.Target.Model.INTEL_6585R</code> Intel 6585R <code>accera.Target.Model.INTEL_6600</code> Intel 6600 <code>accera.Target.Model.INTEL_6600K</code> Intel 6600K <code>accera.Target.Model.INTEL_6600T</code> Intel 6600T <code>accera.Target.Model.INTEL_6685R</code> Intel 6685R <code>accera.Target.Model.INTEL_6700</code> Intel 6700 <code>accera.Target.Model.INTEL_6700K</code> Intel 6700K <code>accera.Target.Model.INTEL_6700T</code> Intel 6700T <code>accera.Target.Model.INTEL_6785R</code> Intel 6785R <code>accera.Target.Model.INTEL_6820HQ</code> Intel 6820HQ <code>accera.Target.Model.INTEL_7100</code> Intel 7100 <code>accera.Target.Model.INTEL_7100T</code> Intel 7100T <code>accera.Target.Model.INTEL_7101E</code> Intel 7101E <code>accera.Target.Model.INTEL_7101TE</code> Intel 7101TE <code>accera.Target.Model.INTEL_7300</code> Intel 7300 <code>accera.Target.Model.INTEL_7300T</code> Intel 7300T <code>accera.Target.Model.INTEL_7320</code> Intel 7320 <code>accera.Target.Model.INTEL_7350K</code> Intel 7350K <code>accera.Target.Model.INTEL_7400</code> Intel 7400 <code>accera.Target.Model.INTEL_7400T</code> Intel 7400T <code>accera.Target.Model.INTEL_7500</code> Intel 7500 <code>accera.Target.Model.INTEL_7500T</code> Intel 7500T <code>accera.Target.Model.INTEL_7505</code> Intel 7505 <code>accera.Target.Model.INTEL_7600</code> Intel 7600 <code>accera.Target.Model.INTEL_7600K</code> Intel 7600K <code>accera.Target.Model.INTEL_7600T</code> Intel 7600T <code>accera.Target.Model.INTEL_7640X</code> Intel 7640X <code>accera.Target.Model.INTEL_7700</code> Intel 7700 <code>accera.Target.Model.INTEL_7700K</code> Intel 7700K <code>accera.Target.Model.INTEL_7700T</code> Intel 7700T <code>accera.Target.Model.INTEL_7740X</code> Intel 7740X <code>accera.Target.Model.INTEL_7800X</code> Intel 7800X <code>accera.Target.Model.INTEL_7820X</code> Intel 7820X <code>accera.Target.Model.INTEL_7900X</code> Intel 7900X <code>accera.Target.Model.INTEL_7920X</code> Intel 7920X <code>accera.Target.Model.INTEL_7940X</code> Intel 7940X <code>accera.Target.Model.INTEL_7960X</code> Intel 7960X <code>accera.Target.Model.INTEL_7980XE</code> Intel 7980XE <code>accera.Target.Model.INTEL_8086K</code> Intel 8086K <code>accera.Target.Model.INTEL_8100</code> Intel 8100 <code>accera.Target.Model.INTEL_8100F</code> Intel 8100F <code>accera.Target.Model.INTEL_8100T</code> Intel 8100T <code>accera.Target.Model.INTEL_8253</code> Intel 8253 <code>accera.Target.Model.INTEL_8256</code> Intel 8256 <code>accera.Target.Model.INTEL_8260</code> Intel 8260 <code>accera.Target.Model.INTEL_8260L</code> Intel 8260L <code>accera.Target.Model.INTEL_8260M</code> Intel 8260M <code>accera.Target.Model.INTEL_8260Y</code> Intel 8260Y <code>accera.Target.Model.INTEL_8268</code> Intel 8268 <code>accera.Target.Model.INTEL_8270</code> Intel 8270 <code>accera.Target.Model.INTEL_8272CL</code> Intel 8272CL <code>accera.Target.Model.INTEL_8273CL</code> Intel 8273CL <code>accera.Target.Model.INTEL_8276</code> Intel 8276 <code>accera.Target.Model.INTEL_8276L</code> Intel 8276L <code>accera.Target.Model.INTEL_8276M</code> Intel 8276M <code>accera.Target.Model.INTEL_8280</code> Intel 8280 <code>accera.Target.Model.INTEL_8280L</code> Intel 8280L <code>accera.Target.Model.INTEL_8280M</code> Intel 8280M <code>accera.Target.Model.INTEL_8284</code> Intel 8284 <code>accera.Target.Model.INTEL_8300</code> Intel 8300 <code>accera.Target.Model.INTEL_8300T</code> Intel 8300T <code>accera.Target.Model.INTEL_8350K</code> Intel 8350K <code>accera.Target.Model.INTEL_8351N</code> Intel 8351N <code>accera.Target.Model.INTEL_8352S</code> Intel 8352S <code>accera.Target.Model.INTEL_8352V</code> Intel 8352V <code>accera.Target.Model.INTEL_8352Y</code> Intel 8352Y <code>accera.Target.Model.INTEL_8358</code> Intel 8358 <code>accera.Target.Model.INTEL_8358P</code> Intel 8358P <code>accera.Target.Model.INTEL_8360Y</code> Intel 8360Y <code>accera.Target.Model.INTEL_8362</code> Intel 8362 <code>accera.Target.Model.INTEL_8368</code> Intel 8368 <code>accera.Target.Model.INTEL_8368Q</code> Intel 8368Q <code>accera.Target.Model.INTEL_8380</code> Intel 8380 <code>accera.Target.Model.INTEL_8400</code> Intel 8400 <code>accera.Target.Model.INTEL_8400T</code> Intel 8400T <code>accera.Target.Model.INTEL_8500</code> Intel 8500 <code>accera.Target.Model.INTEL_8500T</code> Intel 8500T <code>accera.Target.Model.INTEL_8550U</code> Intel 8550U <code>accera.Target.Model.INTEL_8600</code> Intel 8600 <code>accera.Target.Model.INTEL_8600K</code> Intel 8600K <code>accera.Target.Model.INTEL_8600T</code> Intel 8600T <code>accera.Target.Model.INTEL_8650U</code> Intel 8650U <code>accera.Target.Model.INTEL_8700</code> Intel 8700 <code>accera.Target.Model.INTEL_8700K</code> Intel 8700K <code>accera.Target.Model.INTEL_8700T</code> Intel 8700T <code>accera.Target.Model.INTEL_9221</code> Intel 9221 <code>accera.Target.Model.INTEL_9222</code> Intel 9222 <code>accera.Target.Model.INTEL_9242</code> Intel 9242 <code>accera.Target.Model.INTEL_9282</code> Intel 9282 <code>accera.Target.Model.INTEL_9800X</code> Intel 9800X <code>accera.Target.Model.INTEL_9820X</code> Intel 9820X <code>accera.Target.Model.INTEL_9900X</code> Intel 9900X <code>accera.Target.Model.INTEL_9920X</code> Intel 9920X <code>accera.Target.Model.INTEL_9940X</code> Intel 9940X <code>accera.Target.Model.INTEL_9960X</code> Intel 9960X <code>accera.Target.Model.INTEL_9980XE</code> Intel 9980XE <code>accera.Target.Model.INTEL_9990XE</code> Intel 9990XE <code>accera.Target.Model.INTEL_E3_1220_V6</code> Intel E3-1220 v6 <code>accera.Target.Model.INTEL_E3_1225_V6</code> Intel E3-1225 v6 <code>accera.Target.Model.INTEL_E3_1230_V6</code> Intel E3-1230 v6 <code>accera.Target.Model.INTEL_E3_1240_V6</code> Intel E3-1240 v6 <code>accera.Target.Model.INTEL_E3_1245_V6</code> Intel E3-1245 v6 <code>accera.Target.Model.INTEL_E3_1270_V6</code> Intel E3-1270 v6 <code>accera.Target.Model.INTEL_E3_1275_V6</code> Intel E3-1275 v6 <code>accera.Target.Model.INTEL_E3_1280_V6</code> Intel E3-1280 v6 <code>accera.Target.Model.INTEL_E3_1285_V6</code> Intel E3-1285 v6 <code>accera.Target.Model.INTEL_E5_1607_V2</code> Intel E5-1607 v2 <code>accera.Target.Model.INTEL_E5_1620_V2</code> Intel E5-1620 v2 <code>accera.Target.Model.INTEL_E5_1650_V2</code> Intel E5-1650 v2 <code>accera.Target.Model.INTEL_E5_1650_V3</code> Intel E5-1650 v3 <code>accera.Target.Model.INTEL_E5_1660_V2</code> Intel E5-1660 v2 <code>accera.Target.Model.INTEL_E5_1660_V3</code> Intel E5-1660 v3 <code>accera.Target.Model.INTEL_E5_1680_V2</code> Intel E5-1680 v2 <code>accera.Target.Model.INTEL_E5_1680_V3</code> Intel E5-1680 v3 <code>accera.Target.Model.INTEL_E5_2620_V3</code> Intel E5-2620 v3 <code>accera.Target.Model.INTEL_E5_2673_V4</code> Intel E5-2673 v4 <code>accera.Target.Model.INTEL_G3900</code> Intel G3900 <code>accera.Target.Model.INTEL_G3900T</code> Intel G3900T <code>accera.Target.Model.INTEL_G3900TE</code> Intel G3900TE <code>accera.Target.Model.INTEL_G3920</code> Intel G3920 <code>accera.Target.Model.INTEL_G4400</code> Intel G4400 <code>accera.Target.Model.INTEL_G4400T</code> Intel G4400T <code>accera.Target.Model.INTEL_G4400TE</code> Intel G4400TE <code>accera.Target.Model.INTEL_G4500</code> Intel G4500 <code>accera.Target.Model.INTEL_G4500T</code> Intel G4500T <code>accera.Target.Model.INTEL_G4520</code> Intel G4520 <code>accera.Target.Model.INTEL_W_1250</code> Intel W-1250 <code>accera.Target.Model.INTEL_W_1250P</code> Intel W-1250P <code>accera.Target.Model.INTEL_W_1270</code> Intel W-1270 <code>accera.Target.Model.INTEL_W_1270P</code> Intel W-1270P <code>accera.Target.Model.INTEL_W_1290</code> Intel W-1290 <code>accera.Target.Model.INTEL_W_1290P</code> Intel W-1290P <code>accera.Target.Model.INTEL_W_1290T</code> Intel W-1290T <code>accera.Target.Model.INTEL_W_1350</code> Intel W-1350 <code>accera.Target.Model.INTEL_W_1350P</code> Intel W-1350P <code>accera.Target.Model.INTEL_W_1370</code> Intel W-1370 <code>accera.Target.Model.INTEL_W_1370P</code> Intel W-1370P <code>accera.Target.Model.INTEL_W_1390</code> Intel W-1390 <code>accera.Target.Model.INTEL_W_1390P</code> Intel W-1390P <code>accera.Target.Model.INTEL_W_1390T</code> Intel W-1390T <code>accera.Target.Model.INTEL_W_2102</code> Intel W-2102 <code>accera.Target.Model.INTEL_W_2104</code> Intel W-2104 <code>accera.Target.Model.INTEL_W_2123</code> Intel W-2123 <code>accera.Target.Model.INTEL_W_2125</code> Intel W-2125 <code>accera.Target.Model.INTEL_W_2133</code> Intel W-2133 <code>accera.Target.Model.INTEL_W_2135</code> Intel W-2135 <code>accera.Target.Model.INTEL_W_2140B</code> Intel W-2140B <code>accera.Target.Model.INTEL_W_2150B</code> Intel W-2150B <code>accera.Target.Model.INTEL_W_3175X</code> Intel W-3175X <code>accera.Target.Model.INTEL_W_3223</code> Intel W-3223 <code>accera.Target.Model.INTEL_W_3225</code> Intel W-3225 <code>accera.Target.Model.INTEL_W_3235</code> Intel W-3235 <code>accera.Target.Model.INTEL_W_3245</code> Intel W-3245 <code>accera.Target.Model.INTEL_W_3245M</code> Intel W-3245M <code>accera.Target.Model.INTEL_W_3265</code> Intel W-3265 <code>accera.Target.Model.INTEL_W_3265M</code> Intel W-3265M <code>accera.Target.Model.INTEL_W_3275</code> Intel W-3275 <code>accera.Target.Model.INTEL_W_3275M</code> Intel W-3275M <code>accera.Target.Model.RASPBERRY_PI_3B</code> Raspberry Pi 3B <code>accera.Target.Model.RASPBERRY_PI_4B</code> Raspberry Pi 4B <code>accera.Target.Model.RASPBERRY_PI_ZERO</code> Raspberry Pi Zero <p>The enum also defines constants for some well-known GPU models.</p> type description <code>accera.Target.Model.AMD_MI100</code> AMD MI100 <code>accera.Target.Model.AMD_MI200</code> AMD MI200 <code>accera.Target.Model.AMD_MI50</code> AMD MI50 <code>accera.Target.Model.AMD_RADEON7</code> AMD Radeon7 <code>accera.Target.Model.NVIDIA_A100</code> NVidia A100 <code>accera.Target.Model.NVIDIA_P100</code> NVidia P100 <code>accera.Target.Model.NVIDIA_RTX_A6000</code> NVidia RTX A6000 <code>accera.Target.Model.NVIDIA_V100</code> NVidia V100"},{"location":"Reference/classes/Target/Runtime/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Target/Runtime/#acceratargetruntime","title":"<code>accera.Target.Runtime</code>","text":"<p>The runtime for code generation and/or compilation.</p> type description <code>accera.Target.Runtime.CUDA</code> The NVidia CUDA runtime. <code>accera.Target.Runtime.ROCM</code> The AMD ROCm runtime. <code>accera.Target.Runtime.VULKAN</code> The Vulkan runtime. <code>accera.Target.Runtime.OPENMP</code> The OpenMP runtime."},{"location":"Reference/classes/Target/Target/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/classes/Target/Target/#acceratargetarchitecture-cache_lines-cache_sizes-category-extensions-family-frequency_ghz-known_name-model-name-num_cores-num_threads-runtime-tensor_core_info-turbo_frequency_ghz-vector_bytes-vector_registers","title":"<code>accera.Target([architecture, cache_lines, cache_sizes, category, extensions, family, frequency_GHz, known_name, model, name, num_cores, num_threads, runtime, tensor_core_info, turbo_frequency_GHz, vector_bytes, vector_registers)</code>","text":"<p>Defines the capabilities of a target processor.</p>"},{"location":"Reference/classes/Target/Target/#arguments","title":"Arguments","text":"argument description type/default <code>architecture</code> The processor architecture accera.Target.Architecture <code>cache_lines</code> Cache lines (kilobytes) list of positive integers <code>cache_sizes</code> Cache sizes (bytes) list of positive integers <code>category</code> The processor category accera.Target.Category <code>extensions</code> Supported processor extensions list of extension codes <code>family</code> The processor family string <code>frequency_GHz</code> The processor frequency (GHz) positive number <code>known_name</code> A name of a device known to Accera string | accera.Target.Model / \"HOST\" <code>model</code> The processor model accera.Target.Model <code>name</code> The processor name string <code>num_cores</code> Number of cores positive integer <code>num_threads</code> Number of threads positive integer <code>runtime</code> The runtime accera.Target.Runtime <code>tensor_core_info</code> The tensor core capabilities, such as the supported input type, output type, and shapes accera.Targets.TensorCoreInformation <code>turbo_frequency_GHz</code> Turbo frequency (GHz) positive number <code>vector_bytes</code> Bytes per vector register positive number <code>vector_registers</code> total number of SIMD registers positive number"},{"location":"Reference/classes/Target/Target/#known-device-names","title":"Known device names","text":"<p>Accera provides a pre-defined list of known targets through the <code>accera.Target.Models</code> enumeration.</p> <p>These known targets provide typical hardware settings and may not fit your specific hardware characteristics exactly. If your target matches closely with (but not exactly to) one of these targets, you can always start with a known target and update the properties accordingly.</p> <p>If your target is your host machine, Accera will first try to find your host machine's CPU in the list of known devices then use its corresponding capabilities. If none is found, we recommend that you inspect the closest matching device in <code>accera.Target.Models</code> enumeration in order to generate optimal code. If there is no closely matching device for you host machine, we suggest you to look at the following section to define a cpu target in Accera.</p>"},{"location":"Reference/classes/Target/Target/#examples","title":"Examples","text":"<p>Let's have a look at some examples to understand how to define a CPU target in Accera.</p> <p>Create a custom CPU target: <pre><code>cpu_target = acc.Target(name=\"Custom processor\", category=acc.Target.Category.CPU, architecture=acc.Target.Architecture.X86_64, num_cores=10)\n</code></pre></p> <p>We further create a known CPU target and can selectively override fields.</p> <pre><code>gen10 = acc.Target(\n                known_name=\"Intel 7940X\",\n                category=acc.Target.Category.CPU,\n                extensions=[\"SSE4.1\", \"SSE4.2\", \"AVX2\"])\n</code></pre> <p>In this example, we created a target device of a known CPU but overrode the extensions to remove AVX512 support.</p> <p>You can use this example as a starting point to define any other Intel Core Processor. Their specifications are listed in the table above.</p> <p>Craete a pre-defined GPU target representing an NVidia Tesla v100 processor:</p> <pre><code>v100 = acc.Target(model=acc.Target.Model.NVIDIA_TESLA_V100)\n</code></pre> <p>Here is another example to create a custom GPU target:</p> <pre><code>gpu_target = acc.Target(name=\"Custom GPU processor\", category=acc.Target.Category.GPU, default_block_size=16)\n</code></pre>"},{"location":"Reference/classes/Target/Target/#additional-notes-on-instruction-set-extensions","title":"Additional Notes on Instruction Set Extensions","text":"<p>It is important to identify the number of vector registers and vector bytes of each SIMD register. These values may help you determine if you are leveraging the vector units of the underlying hardware to its best capabilities.</p>"},{"location":"Reference/classes/Target/Target/#avx","title":"AVX","text":"<p>Advanced Vector Extensions (AVX) promotes legacy 128-bit SIMD instructions that operate on XMM registers to use a vector-extension (VEX) prefix and operate on 256-bit YMM registers.</p> <p>Intel AVX introduced support for 256-bit wide SIMD registers (YMM0-YMM7 in operating modes that are 32-bit or less, YMM0-YMM15 in 64-bit mode). For Accera, 64-bit mode is the default. a target. The lower 128-bits of the YMM registers are aliased to the respective 128-bit XMM registers. In Intel AVX, there are 256-bit wide vector registers, 16 XMM registers, and 16 YMM registers to support an extension of 128-bits.</p>"},{"location":"Reference/classes/Target/Target/#avx512","title":"AVX512","text":"<p>AVX-512 is a further extension offering 32 ZMM registers, and each SIMD register is 512 bits (64 bytes) wide.</p>"},{"location":"Reference/classes/Target/Target/#sse4-extension","title":"SSE4 Extension","text":"<p>There are 16 XMM registers (XMM0 to XMM15), each 128-bit wide. In 64-bit mode, eight additional XMM registers are accessible. Registers XMM8-XMM15 are accessed by using REX prefixes.</p>"},{"location":"Reference/enumerations/CacheStrategy/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/enumerations/CacheStrategy/#acceracachestrategy","title":"<code>accera.CacheStrategy</code>","text":"type description <code>accera.CacheStrategy.BLOCKED</code> Every thread copies a contiguous block of memory based on their thread index. e.g. If 100 elements are cached by 10 threads, thread 0 copies elements [0, 10), thread 1 copies elements [10, 20) and so on. <code>accera.CacheStrategy.STRIPED</code> Every thread copies a part of their contribution in a round-robin fashion. e.g. In the previous example, thread 0 will now copy elements [0, 2), [20, 22), [40, 42), [60, 62) and [80, 82), thread 1 will copy [2, 4), [22, 24), [42, 44), [62, 64) and [82, 84) and so on. The minimum number of contiguous elements that each thread copies is governed by the vectorization parameter, which in this example is 2. <p>The effects of different caching strategies can be noticed as performance characteristics arising out of overhead caused by bank conflicts, memory coalescing etc.</p>"},{"location":"Reference/enumerations/MMAFragmentOp/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/enumerations/MMAFragmentOp/#accerammafragmentop","title":"<code>accera.MMAFragmentOp</code>","text":"type description Mathematical formula <code>accera.MMAFragmentOp.NONE</code> No-op which does not modify the fragment data. <code>f(x) = x</code> <code>accera.MMAFragmentOp.ReLU</code> Rectified linear unit activation function (details). <code>f(x) = max(0, x)</code> <code>accera.MMAFragmentOp.ReLU_NoConditional</code> Rectified linear unit activation function which does not generate divergent code. <code>f(x) = x * bool(x &gt; 0)</code> <code>accera.MMAFragmentOp.SET</code> Sets the data to scalar constant, C. <code>f(x) = C</code> <code>accera.MMAFragmentOp.SCALE</code> Multiplies the data by a scalar constant, C. <code>f(x) = C.f(x)</code>"},{"location":"Reference/enumerations/MMASchedulingPolicy/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/enumerations/MMASchedulingPolicy/#accerammaschedulingpolicy","title":"<code>accera.MMASchedulingPolicy</code>","text":"type description <code>accera.MMASchedulingPolicy.PASS_ORDER</code> Process pass groups (fused passes) sequentially, within each pass group compute all the MFMA blocks. This allocates Accmulator registers required for all the blocks, however it only allocates input (A, B) registers which are only required for the current pass group. <code>accera.MMASchedulingPolicy.BLOCK_ORDER</code> Process MFMA blocks sequentially, for each block iterate over all the passes. This allocates Accumulator registers required for only 1 block and input (A, B) registers required for the entire pass group currently being processed. In this mode, input data for the same pass group is loaded into registers multiple times, once per block."},{"location":"Reference/enumerations/MMAShape/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/enumerations/MMAShape/#accerammashape","title":"<code>accera.MMAShape</code>","text":"<p>The following table shows the matrix multiplication parameters associated with the different enum values, for different data types for a single pass. So for example a single pass of the <code>M32xN32xK2_B1</code> operation would take input matrices of dimensions [32x2] (A) and [2x32] (B) to produce a matrix multiplication result of dimensions [32x32] (C). These operations can then be composed together to perform matrix multiplication of larger matrices.</p> <p>More information about the corresponding Matrix Arithmetic Instructions (MAI) can be found here.</p> Supported MMA shapes and their compatible types for AMD targets accera.MMAShape MFMA Instruction M, N, K Input Type (ScalarType) Output Type (ScalarType) Compute Type (C++) M64xN64xK1_B4 V_MFMA_F32_16x16x1F32 64, 64, 1 float32 float32 float M64xN64xK1_B2 V_MFMA_F32_32x32x1F32 M32xN32xK2_B1 V_MFMA_F32_32x32x2F32 32, 32, 2 M16xN16xK4_B1 V_MFMA_F32_16x16x4F32 16, 16, 4 M64xN64xK2_B4 V_MFMA_F32_16X16X2BF16 64, 64, 2 bfloat16 bfloat16/float32 M64xN64xK2_B2 V_MFMA_F32_32X32X2BF16 bfloat16/float32 M32xN32xK4_B1 V_MFMA_F32_32X32X4BF16 32, 32, 4 bfloat16/float32 M16xN16xK8_B1 V_MFMA_F32_16X16X8BF16 16, 16, 8 bfloat16/float32 M64xN64xK4_B4 V_MFMA_F32_16x16x4F16 64, 64, 4 float16 float16/32 V_MFMA_I32_16X16X4I8 int8 int8/16/32 int M64xN64xK4_B2 V_MFMA_F32_32x32x4F16 float16 float16/32 float V_MFMA_I32_32X32X4I8 int8 int8/16/32 int M32xN32xK8_B1 V_MFMA_F32_32x32x8F16 32, 32, 8 float16 float16/32 float V_MFMA_I32_32X32X8I8 int8 int8/16/32 int M16xN16xK16_B1 V_MFMA_F32_16x16x16F16 16, 16, 16 float16 float16/32 float V_MFMA_I32_16X16X16I8 int8 int8/16/32 int Supported MMA shapes and their compatible types for Nvidia targets accera.MMAShape M, N, K Input Type (ScalarType) Output Type (ScalarType) Compute Type (C++) M16xN16xK8_B1 16, 16, 8 float32 float32 tf32* M16xN16xK16_B1 16, 16, 16 float16 float16/32 float bfloat16 float32 u/int8 int32 int M32xN8xK16_B1 32, 8, 16 float16 float16/32 float bfloat16 float32 u/int8 int32 int M8xN32xK16_B1 8, 32, 16 float16 float16/32 float bfloat16 float32 u/int8 int32 int <p>*TensorFloat-32 is a floating-point type introduced in the Nvidia Ampere architecture for accelerating FP32 performance. Information about this can be found here and in more detail in the architecture whitepaper. In this mode, multiplication is performed in TF32 precision and accumulation happens in FP32 precision.</p>"},{"location":"Reference/enumerations/Role/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/enumerations/Role/#accerarole","title":"<code>accera.Role</code>","text":"type description <code>accera.Role.CONST</code> A constant array (immutable internally scoped) whose contents are known at compile-time. <code>accera.Role.INPUT</code> An input array (immutable external-scope). <code>accera.Role.INPUT_OUTPUT</code> An input/output array (mutable external-scope). <code>accera.Role.OUTPUT</code> An output array (mutable external-scope) which is allocated at runtime. <code>accera.Role.TEMP</code> A temporary array (mutable internal-scope)."},{"location":"Reference/enumerations/ScalarType/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/enumerations/ScalarType/#accerascalartype","title":"<code>accera.ScalarType</code>","text":"type description <code>accera.ScalarType.bool</code> boolean <code>accera.ScalarType.float16</code> 16-bit floating point number <code>accera.ScalarType.float32</code> 32-bit floating point number <code>accera.ScalarType.float64</code> 64-bit floating point number <code>accera.ScalarType.bfloat16</code> 16-bit Brain floating point number <code>accera.ScalarType.int8</code> 8-bit signed integer <code>accera.ScalarType.int16</code> 16-bit signed integer <code>accera.ScalarType.int32</code> 32-bit signed integer <code>accera.ScalarType.int64</code> 64-bit signed integer <code>accera.ScalarType.uint8</code> 8-bit unsigned integer <code>accera.ScalarType.uint16</code> 16-bit unsigned integer <code>accera.ScalarType.uint32</code> 32-bit unsigned integer <code>accera.ScalarType.uint64</code> 64-bit unsigned integer"},{"location":"Reference/functions/cast/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/functions/cast/#acceracastvalue-type","title":"<code>accera.cast(value, type)</code>","text":"<p>The <code>cast</code> operation converts a value from one <code>acc.ScalarType</code> to another.</p> <p>Accera performs implicit casting between most types. Therefore, this operation should only be used to override the implicit casting behavior documented in Section 2.</p> <p>Limitation: casting constants may result in truncation.</p>"},{"location":"Reference/functions/cast/#arguments","title":"Arguments","text":"argument description type/default <code>value</code> The value to cast <code>type</code> The destination type <code>acc.ScalarType</code>"},{"location":"Reference/functions/cast/#returns","title":"Returns","text":"<p>The result after casting</p>"},{"location":"Reference/functions/cast/#examples","title":"Examples","text":"<p>Casting from float32 to int16:</p> <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(10, 20))\nB = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.int16, shape=(10, 20))\n\nnest = acc.Nest(10, 20)\ni, j = nest.get_indices()\n\n@nest.iteration_logic:\ndef _():\n    B[i, j] = acc.cast(A[i, j], acc.ScalarType.int16) # explicit cast to int16\n    ...\n</code></pre> <p>In comparison, casting from int16 to float32 is implicit, which means the <code>cast</code> operation can be omitted:</p> <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.int16, shape=(10, 20))\nB = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(10, 20))\n\nnest = acc.Nest(10, 20)\ni, j = nest.get_indices()\n\n@nest.iteration_logic:\ndef _():\n    B[i, j] = A[i, j] # implicit cast to float32\n    ...\n</code></pre> <p>Casting a constant to int8:</p> <pre><code>A = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.int8, shape=(10, 20))\n\nnest = acc.Nest(10, 20)\ni, j = nest.get_indices()\n\n@nest.iteration_logic:\ndef _():\n    A[i, j] = acc.cast(10, acc.ScalarType.int8)\n    ...\n</code></pre>"},{"location":"Reference/functions/create_dimensions/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/functions/create_dimensions/#acceracreate_dimensionsrole","title":"<code>accera.create_dimensions([role])</code>","text":"<p>Creates placeholder dimensions of the specified role. These typically represent runtime <code>Array</code> and <code>Nest</code> dimensions.</p> <p>There are two roles for runtime dimensions:</p> <ul> <li><code>accera.Role.INPUT</code> - immutable dimension that is provided by an input parameter to an Accera function</li> <li><code>accera.Role.OUTPUT</code> - mutable dimension that is set within an Accera function</li> </ul> <p>A third type of dimension, the compile-time dimension, is not covered here because it is just a constant.</p>"},{"location":"Reference/functions/create_dimensions/#arguments","title":"Arguments","text":"argument description type/default <code>role</code> The role of the dimension determines if it is mutable or immutable. <code>accera.Role</code>. default: <code>accera.Role.INPUT</code>. Must be set to <code>accera.Role.OUTPUT</code> if intended for an <code>accera.Role.OUTPUT</code> <code>Array</code>."},{"location":"Reference/functions/create_dimensions/#returns","title":"Returns","text":"<p>Tuple of <code>Dimension</code></p>"},{"location":"Reference/functions/create_dimensions/#examples","title":"Examples","text":"<p>Construct an input array with runtime input dimensions: <pre><code>import accera as acc\nM, K = acc.create_dimensions()\nA = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, K))\n</code></pre></p> <p>Construct a input/output array using a combination of runtime and compile-time dimensions, respectively: <pre><code>M = acc.create_dimensions()\nA = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, 20))\n</code></pre></p> <p>Adding a function for an input/output array with runtime input dimensions: <pre><code>M, N = acc.create_dimensions()\nA = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n\nnest = acc.Nest(M, N)\n...\n\npackage = acc.Package()\npackage.add(nest, args=(A, M, N), base_name=\"myFunc\")\n</code></pre></p> <p>Construct a output array with runtime (mutable) output dimensions. <pre><code>M, N = acc.create_dimensions(role=acc.Role.OUTPUT)\nA = acc.Array(role=acc.Role.OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n</code></pre></p> <p>Assign the value of a runtime input dimension to a runtime output dimension: <pre><code>M = acc.create_dimensions()\nN = acc.create_dimensions(role=acc.Role.OUTPUT)\n\nN.value = M\n</code></pre></p> <p>Assign the value of a runtime input dimension to a runtime output dimension: <pre><code>M = acc.create_dimensions()\nN = acc.create_dimensions(role=acc.Role.OUTPUT)\n\nN.value = M\n</code></pre></p> <p>Assign an integer value to a runtime output dimension: <pre><code>N = acc.create_dimensions(role=acc.Role.OUTPUT)\nN.value = 100\n</code></pre></p> <p>Assign a value to a runtime output dimension using an expression of runtime input dimensions: <pre><code>M, K = acc.create_dimensions()\nN = acc.create_dimensions(role=acc.Role.OUTPUT)\n\nN.value = M + K + 1\n</code></pre></p>"},{"location":"Reference/functions/create_parameter_grid/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/functions/create_parameter_grid/#acceracreate_parameter_gridparameter_choices-filter_func-sample-seed","title":"<code>accera.create_parameter_grid(parameter_choices, [filter_func, sample, seed])</code>","text":"<p>Create a parameter grid from a dictionary that maps each parameter to its possible values.</p>"},{"location":"Reference/functions/create_parameter_grid/#arguments","title":"Arguments","text":"argument description type/default <code>parameter_choices</code> A dictionary that maps each parameter to its possible values dictionary <code>filter_func</code> A callable to filter parameter_choices which returns a bool to indicate whether a given parameter combination should be included in the grid Callable <code>sample</code> A number to limit the size of the parameter grid. The grid is randomly sampled. integer <code>seed</code> The seed value for random sampling. integer"},{"location":"Reference/functions/create_parameter_grid/#returns","title":"Returns","text":"<p>List of dictionary</p>"},{"location":"Reference/functions/create_parameter_grid/#examples","title":"Examples","text":"<p>Create a parameter grid from a dictionary that maps each parameter to its possible values:</p> <pre><code>parameters = acc.create_parameter_grid(parameter_choices={P0:[8,16], P1:[16,32], P2:[16], P3:[1.0,2.0]})\npackage.add(nest, args=(A, B, C), base_name=\"matmul\", parameters)\n</code></pre> <p>Define a lambda or function to filter out combinations from the parameter grid. The arguments to the filter are the values of a parameter combination. The filter function should return True if the combination should be included, and False otherwise:</p> <pre><code>parameters = acc.create_parameter_grid(parameter_choices={P0:[8,16], P1:[16,32], P2:[16], P3:[1.0,2.0]}, filter_func=lambda p0, p1, p2, p3: p2 &lt; p1 and 4 * (p0 * p3 + p1 * p2 + p1 * p3 + p2 * p3) / 1024 &lt; 256)\n</code></pre> <p>Parameter grids can result in a large number of possible combinations. We can limit the number of combinations by random sampling:</p> <pre><code>parameters = acc.create_parameter_grid(parameter_choices={P0:[8,16], P1:[16,32], P2:[16], P3:[1.0,2.0]}, sample=5)\n</code></pre>"},{"location":"Reference/functions/create_parameters/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/functions/create_parameters/#acceracreate_parameters","title":"<code>accera.create_parameters()</code>","text":"<p>Creates placeholder parameters.</p>"},{"location":"Reference/functions/create_parameters/#returns","title":"Returns","text":"<p>Tuple of <code>Parameter</code></p>"},{"location":"Reference/functions/create_parameters/#examples","title":"Examples","text":"<p>Create 3 parameters <code>m</code>, <code>n</code>, <code>k</code>. Use them to parameterize the nest shape:</p> <pre><code>m, n, k = acc.create_parameters()\nnest = acc.Nest(shape=(m, n, k))\n</code></pre>"},{"location":"Reference/functions/fuse/","title":"Accera v1.2 Reference","text":""},{"location":"Reference/functions/fuse/#accerafuseschedules-args-partial","title":"<code>accera.fuse(schedules[, *args, partial])</code>","text":"<p>The <code>fuse</code> operation combines multiple iteration spaces into a single \"fused\" iteration space. The fused iteration space represents the union of the work in the original spaces.</p> <p>In cases where it doesn't make sense to fuse all of the iteration space dimensions, we can choose to fuse a prefix of the dimensions and leave the rest unfused.</p>"},{"location":"Reference/functions/fuse/#arguments","title":"Arguments","text":"argument description type/default <code>schedules</code> If performing partial fusing, this is a tuple of the schedules to fuse. If performing full fusing, this contains the first schedule to fuse, while <code>args</code> will contain the subsequent schedules. <code>*args</code> Optional variable arguments containing subsequent schedules to fuse variable <code>Schedule</code> arguments <code>partial</code> The number of dimensions to fuse. If not specified, all dimensions will be fused non-negative integer"},{"location":"Reference/functions/fuse/#returns","title":"Returns","text":"<p>Instance of <code>FusedSchedule</code></p>"},{"location":"Reference/functions/fuse/#examples","title":"Examples","text":"<p>Full fusing of same-shaped iteration spaces:</p> <pre><code># Fuse all dimensions of schedule0 and schedule1\nschedule = acc.fuse(schedule0, schedule1)\n</code></pre> <p>Partial iteration space fusing:</p> <pre><code># Fuse the first two dimensions of schedule0 and schedule1\nschedule = acc.fuse((schedule0, schedule1), partial=2)\n</code></pre>"},{"location":"Tutorials/","title":"Accera Tutorials","text":"Tutorial Description Hello Matrix Multiplication Start here if you are completely new to Accera and would like to learn more about the workflow Optimized Matrix Multiplication Once you understand the basics, we'll look at how to optimize matrix multiplication for a specific hardware target Cross Compilation for Raspberry Pi 3 After you know how to generate code for the host target, we'll look at how to generate code for other targets [GPU] Hello Matrix Multiplication We'll look at how to apply the basic concepts for GPU targets [GPU] Tensorized Matrix Multiplication Explains the basic usage of Tensor cores on GPU [GPU] Multi-Pass Tensorized MatMul with Pass Fusion Shows how pass fusion can be used to control register usage of input data [GPU] Tensorized MatMul with Caching Explores shared memory and register caching techniques on GPU [GPU] Tensorized MatMul with Element-wise Op fusion Enhanced Matmul with element-wise pre/post matmul OP fusion [GPU] Multi-Block Tensorized MatMul with different Scheduling Policies Explains tradeoffs between register usage and memory I/O, and their performance impact"},{"location":"Tutorials/Hello_MatMul/","title":"Hello MatMul","text":""},{"location":"Tutorials/Hello_MatMul/#hello-matmul","title":"Hello MatMul","text":"<p>By the end of this tutorial, you will learn how to:</p> <ul> <li>Implement a simple Matrix Multiplication (MatMul) function using Accera's Domain Specific Language (DSL)</li> <li>Produce a HAT package containing the MatMul function</li> <li>Call the function from C or C++ code</li> </ul>"},{"location":"Tutorials/Hello_MatMul/#prerequisites","title":"Prerequisites","text":"<ul> <li>This tutorial assumes you already have Accera installed. If not, you can find the instructions in here</li> <li>You should also be familiar with writing Python and C++</li> </ul>"},{"location":"Tutorials/Hello_MatMul/#a-naive-matmul-algorithm","title":"A naive MatMul algorithm","text":"<p>Let's consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as:</p> <pre><code>C += A @ B\n</code></pre> <p>A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could like:</p> <pre><code># A.shape = (M, K), B.shape = (K, N), C.shape = (M, N)\n\nfor i in range(M):\n    for j in range(N):\n        for k in range(K):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre>"},{"location":"Tutorials/Hello_MatMul/#accera-python-dsl","title":"Accera Python DSL","text":"<p>We will now walk through a naive Matrix Multiplication (MatMul) using Accera.</p> <p>Create an empty file called <code>hello_matmul_generator.py</code>. First we'll import Accera's module.</p> <pre><code>import accera as acc\n</code></pre> <p>Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N.</p> <pre><code># Define our matrix sizes\nM = 128\nN = 256\nK = 256\n</code></pre> <p>Write a Python function that receives arrays <code>A</code>, <code>B</code> and <code>C</code>. These are our input and input/output matrices.</p> <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, K))\nB = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(K, N))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n</code></pre> <p>Here, we will use the <code>Nest</code> class to define our 3-layered nested for loop. The range indices are <code>M</code>, <code>N</code>, and <code>K</code>, with the outermost loop (<code>M</code>) listed first. We can get the loop nest indices in order to perform the computation.</p> <pre><code># Define the loop nest\nnest = acc.Nest(shape=(M, N, K))\n\n# Get the loop nest indices\ni, j, k = nest.get_indices()\n</code></pre> <p>Next we define the logic of each iteration of the loop nest: <pre><code># Define the loop nest logic\n@nest.iteration_logic\ndef _():\n    C[i, j] += A[i, k] * B[k, j]\n</code></pre></p> <p>We have finished defining the logic of MatMul, and let's define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest:</p> <pre><code>sched = nest.create_schedule()\n</code></pre> <p>At this point, <code>sched</code> represents the default schedule for our algorithm. We can also perform some basic transformations on this schedule. For example, the following lines of code will split the <code>k</code> index in blocks of 4 (so <code>k</code>, <code>k+4</code>, <code>k+8</code>, and so on).</p> <pre><code># Split the k loop into blocks of 4, effectively doing this\n# (assuming K is divisible by 4):\n#\n# for i in range(M):\n#    for j in range(N):\n#        # Split k into two loops\n#        for k in range(0, K, 4):\n#            for kk in range(4):\n#                C[i, j] += A[i, k + kk] * B[k + kk, j]\n#\n# If k is not divisible by 4, Accera will take care of the boundary\n# case for you.\nkk = sched.split(k, 4)\n</code></pre> <p>The split index is now <code>k</code> and <code>kk</code>.</p> <p>The next step is to create a plan from the schedule. For instance, we can use this plan to unroll the innermost loop.</p> <pre><code>plan = sched.create_plan()\n# Unroll kk, effectively doing this\n# (assuming K is divisible by 4):\n#\n# for i in range(M):\n#    for j in range(N):\n#        for k in range(0, K, 4):\n#            # Unrolled kk\n#            C[i, j] += A[i, k + 0] * B[k + 0, j]\n#            C[i, j] += A[i, k + 1] * B[k + 1, j]\n#            C[i, j] += A[i, k + 2] * B[k + 2, j]\n#            C[i, j] += A[i, k + 3] * B[k + 3, j]\n#\n# If k is not divisible by 4, Accera will take care of the boundary\n# case for you.\nplan.unroll(kk)\n</code></pre> <p>Use the plan to add a callable function named <code>hello_matmul_pi3_py</code> to a HAT package.</p> <pre><code># Create a package and add a function to the package based on the plan\npackage = acc.Package()\npackage.add(plan, args=(A, B, C), base_name=\"hello_matmul_py\")\n</code></pre> <p>Finally, we build the HAT package: <pre><code># Build the HAT package\npackage.build(name=\"hello_matmul\")\n</code></pre></p> <p>By now, you should have all the code necessary to generate your first Accera MatMul function. You can also find the complete Python script here.</p>"},{"location":"Tutorials/Hello_MatMul/#generate-hat-package","title":"Generate HAT package","text":"<p>Next, we run the generator script to produce a HAT package.</p>"},{"location":"Tutorials/Hello_MatMul/#windowsmacos","title":"Windows/MacOS","text":"<pre><code>python hello_matmul_generator.py\n</code></pre>"},{"location":"Tutorials/Hello_MatMul/#ubuntu","title":"Ubuntu","text":"<pre><code>python3 hello_matmul_generator.py\n</code></pre> <p>After this runs, you should see a header file <code>hello_matmul.hat</code> and some object files (such as <code>hello_matmul.obj</code> or <code>hello_matmul.o</code>). The <code>.hat</code> file format is described here. In Accera, we call these files the \"HAT package\".</p>"},{"location":"Tutorials/Hello_MatMul/#runner-code","title":"Runner code","text":"<p>We will now walk through how to call our MatMul implementation from the HAT package.</p> <p>Create a file called <code>hello_matmul_runner.cpp</code> with the code below. You can also find it here.</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;algorithm&gt;\n\n// Include the HAT file that declares our MatMul function\n#include \"hello_matmul.hat\"\n\n#define M 128\n#define N 256\n#define K 256\n\nint main(int argc, const char** argv)\n{\n// Prepare our matrices\nfloat A[M*K];\nfloat B[K*N];\nfloat C[M*N];\n\n// Fill with data\nstd::fill_n(A, M*K, 2.0f);\nstd::fill_n(B, K*N, 3.0f);\nstd::fill_n(C, M*N, 0.42f);\n\nprintf(\"Calling MatMul M=%d, K=%d, N=%d\\n\", M, K, N);\nhello_matmul_py(A, B, C);\n\nprintf(\"Result (first few elements): \");\nfor (int i = 0; i &lt; 10; ++i)\n{\nprintf(\"%f \", C[i]);\n}\nprintf(\"\\n\");\nreturn 0;\n}\n</code></pre> <p>The code above creates the <code>A</code>, <code>B</code>, and <code>C</code> matrices, and calls the function <code>hello_matmul_py</code> to perform MatMul.</p> <p>Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as <code>hello_matmul_generator.py</code> and the generated <code>*.hat</code> and object files.</p>"},{"location":"Tutorials/Hello_MatMul/#build-and-run","title":"Build and run","text":""},{"location":"Tutorials/Hello_MatMul/#windows","title":"Windows","text":"<p>We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\":</p> <pre><code>cl.exe hello_matmul_runner.cpp *.lib\nhello_matmul_runner.exe\n</code></pre>"},{"location":"Tutorials/Hello_MatMul/#macos","title":"MacOS","text":"<pre><code>clang hello_matmul_runner.cpp *.a -o hello_matmul_runner\n./hello_matmul_runner\n</code></pre>"},{"location":"Tutorials/Hello_MatMul/#ubuntu_1","title":"Ubuntu","text":"<pre><code>gcc hello_matmul_runner.cpp *.a -o hello_matmul_runner\n./hello_matmul_runner\n</code></pre> <p>The output should look like:</p> <pre><code>Calling MatMul M=128, K=256, N=256\nResult (first few elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922\n</code></pre> <p>You can now experiment with the generated MatMul function with your own inputs.</p>"},{"location":"Tutorials/Hello_MatMul/#optimized-matmul-algorithm","title":"Optimized MatMul algorithm","text":"<p>The above example illustrates a naive algorithm. To see what a more optimized version could like like, see the Optimized MatMul tutorial.</p>"},{"location":"Tutorials/Optimized_MatMul/","title":"Optimized MatMul","text":""},{"location":"Tutorials/Optimized_MatMul/#optimized-matmul","title":"Optimized MatMul","text":"<p>Optimizing MatMul is highly dependent on the target platform. The code in the example below is optimized specifically for an Intel Xeon E5-2673 v3 CPU. However, it should work equally well on CPUs with similar hardware characteristics, such as an AMD Epyc 7551.</p> <p>By the end of this tutorial, you will learn how to: * Implement a performant Matrix Multiplication (MatMul) function targetting AVX2 FMA3 CPUs like Intel Haswell or the AMD Epyc families. * Produce a HAT package containing the optimized MatMul function. * Call the function from C or C++ code.</p>"},{"location":"Tutorials/Optimized_MatMul/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have Accera installed. If not, you can find the instructions in here.</li> <li>You are familiar with writing Python and C++.</li> <li>You know about SIMD instructions and registers.</li> <li>You have completed the Hello_MatMul tutorial.</li> </ul>"},{"location":"Tutorials/Optimized_MatMul/#review-the-naive-matmul-algorithm","title":"Review: the naive MatMul algorithm","text":"<p>As in the Hello_MatMul tutorial, we'll consider the example of multiplying matrices A and B and adding the result into matrix C. In NumPy syntax, this can be expressed as:</p> <pre><code>C += A @ B\n</code></pre> <p>A naive algorithm for matrix multiplication typically contains 3 nested for-loops. Expressed in Python, this will look like:</p> <pre><code>for i in range(M):\n    for j in range(N):\n        for k in range(K):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre>"},{"location":"Tutorials/Optimized_MatMul/#accera-python-dsl","title":"Accera Python DSL","text":"<p>We will walk through how to specify an optimized Matrix Multiplication (MatMul) using Accera. This tutorial assumes the following:</p> <ul> <li>Specific matrix sizes. Inputs A and B are 784 x 128 and 128 x 512 matrices, respectively. The output C is a 784 x 512 matrix. These can represent a mid-level layer in a Resnet-50 model. The A matrix contains the activation values from the previous layer, and the B matrix contains the weights of the neural network layer.</li> <li>Row-major layout of the matrix elements.</li> <li>The target hardware is capable of AVX2 FMA3 instructions, such as the Intel Xeon E5-2673 v3 or the AMD Epyc 7551.</li> </ul> <p>Create an empty file called <code>optimized_matmul_generator.py</code>. Import dependent modules: <pre><code>import accera as acc\n</code></pre></p> <p>Define some matrix sizes, where A's shape is M by K, B's is K by N, and C's, M by N. <pre><code># Define our matrix sizes\nM = 784\nN = 512\nK = 128\n</code></pre></p> <p>Declare arrays <code>A</code>, <code>B</code>, and <code>C</code>. These are our input and input/output matrices and hold 32-bit floating-point elements. <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, K))\nB = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(K, N))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n</code></pre></p> <p>Use the <code>Nest</code> class to define a 3-layered nested for-loop and get the indices: <pre><code># Define the loop nest\nnest = acc.Nest(shape=(M, N, K))\n\n# Get the loop nest indices\ni, j, k = nest.get_indices()\n</code></pre></p> <p>Next, we define the logic for every iteration of the loop nest: <pre><code># Define the loop nest logic\n@nest.iteration_logic\ndef _():\n    C[i, j] += A[i, k] * B[k, j]\n</code></pre></p> <p>We have finished defining the logic of MatMul. Let's now define the schedule which controls execution logic. To do this, we first create the schedule from the nest: <pre><code>schedule = nest.create_schedule()\n</code></pre></p> <p>We will transform the iteration space and change the plan according to some predefined constants to execute this efficiently on our chosen hardware target. Values of these constants come either from hardware target characteristics and the shapes of the arrays or can be found through auto-tuning. These will be explained in detail in a subsequent tutorial. For now, define: <pre><code>tile_size_i = 6\ntile_size_j = 256\ntile_size_k = 128\ninner_dim_unroll = 4\nnum_rows_in_kernel = 6\n</code></pre></p> <p>We create a CPU target that defines constants for the SIMD vector sizes and the number of vector execution units to use the hardware characteristics. <pre><code>target = acc.Target(category=acc.Target.Category.CPU)\n</code></pre></p> <p>Transform the iteration space to specify the tiling behavior: <pre><code>ii = schedule.split(i, tile_size_i)\njj = schedule.split(j, tile_size_j)\nkk = schedule.split(k, tile_size_k)\n</code></pre></p> <p>Next, let's split the iteration space to match the kernel characteristics: <pre><code>kkk = schedule.split(kk, inner_dim_unroll)\niii = schedule.split(ii, num_rows_in_kernel)\njjj = schedule.split(jj, (target.vector_bytes // 4) * 2) # There are 2 vfma execution units, each holding (target.vector_bytes // 4) 32-bit float elements\njjjj = schedule.split(jjj, target.vector_bytes // 4) # Each SIMD register holds (target.vector_bytes // 4) 32-bit float elements\n</code></pre></p> <p>Accera will handle the encountered boundary conditions for each of these splits and do appropriate optimizations such as loop un-switching to ensure that efficient code gets generated in those cases.</p> <p>Set the order to traverse the iteration space. We start with the outer indices that control the tiling, then move to the innermost indices that are used in the kernel: <pre><code>schedule.reorder(j, k, i, jj, kk, ii, kkk, iii, jjj, jjjj)\n</code></pre></p> <p>Create a plan from the schedule and the current target. The plan allows us to control specific execution behavior on the hardware target, such as vectorization and caching, which are essential for high performance: <pre><code>plan = schedule.create_plan(target)\n</code></pre></p> <p>Add caching. We use an input cache for array <code>B</code> that exceeds our threshold. The <code>B</code> cache will be packed according to the access pattern specified by the schedule. We use an input/output cache for array <code>C</code>. See caching for more information: <pre><code># Cache the B array by prefetching and packing the memory footprint along slices of the jj dimension.\nplan.cache(B, jj)\n# Cache the C array along slices of jj dimension. Since the C array is the output, its footprint is\n# the size of the kernel. If the kernel is small enough, Accera will use registers for this\n# accumulation before writing these values back to C.\nplan.cache(C, jj)\n</code></pre></p> <p>Kernelize the inner dimensions, which applies unroll and vectorize transformations allowing use of SIMD registers: <pre><code>plan.kernelize(unroll_indices=[jjj, iii, kkk], vectorize_indices=jjjj)\n</code></pre></p> <p>Use the plan to add a function named <code>optimized_matmul_py</code> to a HAT package. <pre><code># Create a package and add a function to the package based on the plan\npackage = acc.Package()\npackage.add(plan, args=(A, B, C), base_name=\"optimized_matmul_py\")\n</code></pre></p> <p>Finally, we build the HAT package: <pre><code># Build a statically-linked HAT package to be consumed by the C++ runner\npackage.build(name=\"optimized_matmul\", format=acc.Package.Format.HAT_STATIC)\n</code></pre></p> <p>By now, you should have all the code necessary to generate an optimized Accera MatMul function. You can find the complete Python script here.</p>"},{"location":"Tutorials/Optimized_MatMul/#generate-hat-package","title":"Generate HAT package","text":"<p>Next, we run the generator script to produce a HAT package.</p>"},{"location":"Tutorials/Optimized_MatMul/#windowsmacos","title":"Windows/MacOS","text":"<pre><code>python optimized_matmul_generator.py\n</code></pre>"},{"location":"Tutorials/Optimized_MatMul/#ubuntu","title":"Ubuntu","text":"<pre><code>python3 optimized_matmul_generator.py\n</code></pre> <p>The generator script produces a HAT file (<code>optimized_matmul.hat</code>). Examine this file, and you will see that it contains the exported function with the following meta-data:</p> <pre><code>[functions.optimized_matmul_py_4a6286d9]\nname = 'optimized_matmul_py_4a6286d9'\ndescription = ''\ncalling_convention = \"cdecl\"\narguments = [\n{name = '', description = '', logical_type = \"affine_array\", declared_type = 'float*', element_type = 'float', usage = \"input_output\", shape = [ 784, 128 ], affine_map = [ 128, 1 ], affine_offset = 0},\n{name = '', description = '', logical_type = \"affine_array\", declared_type = 'float*', element_type = 'float', usage = \"input_output\", shape = [ 128, 512 ], affine_map = [ 512, 1 ], affine_offset = 0},\n{name = '', description = '', logical_type = \"affine_array\", declared_type = 'float*', element_type = 'float', usage = \"input_output\", shape = [ 784, 512 ], affine_map = [ 512, 1 ], affine_offset = 0}\n]\nreturn = {name = '', description = '', logical_type = \"void\", declared_type = 'void', element_type = 'void', usage = \"output\"}\n</code></pre> <p>The C declaration from the header is: <pre><code>void optimized_matmul_py_4a6286d9(float*, float*, float*);\n</code></pre></p> <p>Accera automatically appends a unique identifier to the function implementation, such as <code>optimized_matmul_py_4a6286d9</code> to support auto-tuning. This name is re-generated every time the HAT package is rebuilt. To make it easier for client code to use the function, Accera also provides a fixed-name alias, <code>optimized_matmul_py</code>, for the same function.</p> <p>To see how Accera generates code for the iteration space transformations and the plan, you can change the <code>format=HAT</code> to <code>format=MLIR</code>, which will output MLIR for each lowering phase. Stepping through the progression of lowerings, you can see how Accera moves from simple representation of the Accera DSL, to the final optimized assembly.</p> <p>Compare this to previous tutorial, whose naive DSL is given here, and final assembly can be viewed here.</p>"},{"location":"Tutorials/Optimized_MatMul/#runner-code","title":"Runner code","text":"<p>Let's see how to call our MatMul implementation from the HAT package.</p> <p>Create a file called <code>optimized_matmul_runner.cpp</code> with the code below. You can also find it here.</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;algorithm&gt;\n\n// Include the HAT file that declares our MatMul function\n#include \"optimized_matmul.hat\"\n\n#define M 784\n#define N 512\n#define K 128\n\nint main(int argc, const char** argv)\n{\n// Prepare our matrices (using the heap for large matrices)\nfloat* A = new float[M*K];\nfloat* B = new float[K*N];\nfloat* C = new float[M*N];\n\n// Fill with data\nstd::fill_n(A, M*K, 2.0f);\nstd::fill_n(B, K*N, 3.0f);\nstd::fill_n(C, M*N, 0.42f);\n\nprintf(\"Calling MatMul M=%d, K=%d, N=%d\\n\", M, K, N);\noptimized_matmul_py(A, B, C);\n\nprintf(\"Result (first 10 elements): \");\nfor (int i = 0; i &lt; 10; ++i)\n{\nprintf(\"%f \", C[i]);\n}\nprintf(\"\\n\");\n\ndelete[] A;\ndelete[] B;\ndelete[] C;\nreturn 0;\n}\n</code></pre> <p>The above code creates the matrices <code>A</code>, <code>B</code>, and <code>C</code> and calls the function <code>optimized_matmul_py</code> to perform MatMul.</p> <p>Now that we have the code, let's compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as <code>optimized_matmul_generator.py</code> and the generated <code>*.hat</code> and object files.</p>"},{"location":"Tutorials/Optimized_MatMul/#build-and-run","title":"Build and run","text":""},{"location":"Tutorials/Optimized_MatMul/#windows","title":"Windows","text":"<p>We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\":</p> <pre><code>cl.exe optimized_matmul_runner.cpp *.lib\noptimized_matmul_runner.exe\n</code></pre>"},{"location":"Tutorials/Optimized_MatMul/#macos","title":"MacOS","text":"<pre><code>clang++ optimized_matmul_runner.cpp *.a -o optimized_matmul_runner\n./optimized_matmul_runner\n</code></pre>"},{"location":"Tutorials/Optimized_MatMul/#ubuntu_1","title":"Ubuntu","text":"<pre><code>g++ optimized_matmul_runner.cpp *.a -o optimized_matmul_runner\n./optimized_matmul_runner\n</code></pre> <p>The output should look like:</p> <pre><code>Calling MatMul M=784, K=128, N=512\nResult (first 10 elements): 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983\n</code></pre> <p>You can now experiment with the generated MatMul function with your own inputs.</p>"},{"location":"Tutorials/Pi3_Cross_Compilation/","title":"Cross Compiling for the Raspberry Pi 3","text":"<p>By the end of this tutorial, you will learn how to:</p> <ul> <li>Cross compile a simple Matrix Multiplication (MatMul) function for execution on a Raspberry Pi 3.</li> <li>Produce a HAT package containing the MatMul function that can be called on the Pi 3 target.</li> <li>Call the function on a Raspberry Pi 3 from C/C++ code.</li> </ul>"},{"location":"Tutorials/Pi3_Cross_Compilation/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have Accera installed. If not, you can find the instructions in here.</li> <li>Be familiar with writing Python and C++ code.</li> <li>Have access to a Raspberry Pi 3 device.</li> </ul>"},{"location":"Tutorials/Pi3_Cross_Compilation/#a-naive-matmul-algorithm","title":"A naive MatMul algorithm","text":"<p>Consider the example of multiplying matrices A and B and adding the result into matrix C. In NumPy syntax, this can be expressed as:</p> <pre><code>C += A @ B\n</code></pre> <p>A naive algorithm for matrix multiplication typically contains 3 nested for-loops. In Python, this can be expressed as:</p> <pre><code># A.shape = (M, K), B.shape = (K, N), C.shape = (M, N)\n\nfor i in range(M):\n    for j in range(N):\n        for k in range(K):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre>"},{"location":"Tutorials/Pi3_Cross_Compilation/#accera-python-dsl","title":"Accera Python DSL","text":"<p>Let's walk through a na\u00efve Matrix Multiplication (MatMul) using Accera. Instead of using the default target, i.e., the host machine, we specify a target representing a Raspberry Pi 3 to cross-compile the host for a different target.</p> <p>Create an empty file called <code>hello_matmul_pi3_generator.py</code>. First, we import Accera's module:</p> <pre><code>import accera as acc\n</code></pre> <p>Define some matrix sizes, where A's shape is M by K, B's is K by N, and C's, M by N. </p> <pre><code># Define our matrix sizes\nM = 128\nN = 256\nK = 256\n</code></pre> <p>Write a Python function that receives <code>A</code>, <code>B</code>, and <code>C</code> arrays. These are our input and input/output matrices.</p> <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, K))\nB = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(K, N))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n</code></pre> <p>We now use the <code>Nest</code> class to define our 3-layered nested for-loop. The range indices are <code>M</code>, <code>N</code>, and <code>K</code>, with the outermost loop (<code>M</code>) listed first. We can get the loop nest indices to perform the computation.</p> <pre><code># Define the loop nest\nnest = acc.Nest(shape=(M, N, K))\n\n# Get the loop nest indices\ni, j, k = nest.get_indices()\n</code></pre> <p>Next, we define the logic for every iteration of the loop nest: <pre><code># Define the loop nest logic\n@nest.iteration_logic\ndef _():\n    C[i, j] += A[i, k] * B[k, j]\n</code></pre></p> <p>We have finished defining the logic of MatMul. Let's now define the schedule which controls the execution of logic. For this, we first create the schedule from the nest:</p> <pre><code>sched = nest.create_schedule()\n</code></pre> <p>At this point, <code>sched</code> represents the default schedule for our algorithm. We can also perform some basic transformations on this schedule. For example, the following lines of code split the <code>k</code> index into blocks of 4 ( <code>k</code>, <code>k+4</code>, <code>k+8</code>, and so on).</p> <pre><code># Split the k loop into blocks of 4, effectively doing this\n# (assuming K is divisible by 4):\n#\n# for i in range(M):\n#    for j in range(N):\n#        # Split k into two loops\n#        for k in range(0, K, 4):\n#            for kk in range(4):\n#                C[i, j] += A[i, k + kk] * B[k + kk, j]\n#\n# If k is not divisible by 4, Accera will take care of the boundary\n# case for you.\nkk = sched.split(k, 4)\n</code></pre> <p>The split index is now <code>k</code> and <code>kk</code>.</p> <p>The next step is to create a plan from the schedule. For instance, we can use this plan to unroll the innermost loop.</p> <pre><code># Create a plan, specify the target to be a Raspberry Pi 3\npi3 = acc.Target(acc.Target.Model.RASPBERRY_PI_3B)\nplan = sched.create_plan(pi3)\n\n# Unroll kk, effectively doing this\n# (assuming K is divisible by 4):\n#\n# for i in range(M):\n#    for j in range(N):\n#        for k in range(0, K, 4):\n#            # Unrolled kk\n#            C[i, j] += A[i, k + 0] * B[k + 0, j]\n#            C[i, j] += A[i, k + 1] * B[k + 1, j]\n#            C[i, j] += A[i, k + 2] * B[k + 2, j]\n#            C[i, j] += A[i, k + 3] * B[k + 3, j]\n#\n# If k is not divisible by 4, Accera will take care of the boundary\n# case for you.\nplan.unroll(kk)\n</code></pre> <p>Use the plan to add a callable function named <code>hello_matmul_pi3_py</code> to a HAT package.</p> <pre><code># Create a package and add a function to the package based on the plan\npackage = acc.Package()\npackage.add(plan, args=(A, B, C), base_name=\"hello_matmul_pi3_py\")\n</code></pre> <p>Finally, we build the statically-linked HAT package for the Raspbian platform: <pre><code># Build the HAT package\npackage.build(name=\"hello_matmul_pi3\", format=acc.Package.Format.HAT_STATIC, platform=acc.Package.Platform.RASPBIAN)\n</code></pre> After following the above steps, you should now have all the code necessary to generate your Accera MatMul function that can be called on a Raspberry Pi 3 target. You can find the complete Python script here.</p>"},{"location":"Tutorials/Pi3_Cross_Compilation/#generate-hat-package","title":"Generate HAT package","text":"<p>Next, we run the generator script to produce a HAT package for the Raspberry Pi 3 target.</p>"},{"location":"Tutorials/Pi3_Cross_Compilation/#windowsmacos","title":"Windows/MacOS","text":"<pre><code>python hello_matmul_pi3_generator.py\n</code></pre>"},{"location":"Tutorials/Pi3_Cross_Compilation/#ubuntu","title":"Ubuntu","text":"<pre><code>python3 hello_matmul_pi3_generator.py\n</code></pre> <p>After we run the script, there should be a header file <code>hello_matmul_pi3.hat</code> and an object file <code>hello_matmul_pi3.o</code> in the ELF format. The <code>.hat</code> file format is described here. Collectively, we call the <code>.hat file</code> and <code>object file</code> a \"HAT package\".</p>"},{"location":"Tutorials/Pi3_Cross_Compilation/#runner-code","title":"Runner code","text":"<p>Let's now see how we can call our MatMul implementation from the HAT package on the Raspberry Pi 3.</p> <p>Create a file called <code>hello_matmul_pi3_runner.cpp</code> with the code below. You can find it here.</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;algorithm&gt;\n\n// Include the HAT file that declares our MatMul function\n#include \"hello_matmul_p3.HAT\"\n\n#define M 128\n#define N 256\n#define K 256\n\nint main(int argc, const char** argv)\n{\n// Prepare our matrices\nfloat A[M*K];\nfloat B[K*N];\nfloat C[M*N];\n\n// Fill with data\nstd::fill_n(A, M*K, 2.0f);\nstd::fill_n(B, K*N, 3.0f);\nstd::fill_n(C, M*N, 0.42f);\n\nprintf(\"Calling MatMul M=%d, K=%d, N=%d\\n\", M, K, N);\nhello_matmul_py(A, B, C);\n\nprintf(\"Result (first few elements): \");\nfor (int i = 0; i &lt; 10; ++i)\n{\nprintf(\"%f \", C[i]);\n}\nprintf(\"\\n\");\nreturn 0;\n}\n</code></pre> <p>The above code creates the <code>A</code>, <code>B</code>, and <code>C</code> matrices and calls the function <code>hello_matmul_pi3_py</code> to perform MatMul.</p> <p>Now that we have written the code, we compile and link it with the HAT package to create an executable file. Save this file to your working directory, in the exact location as <code>hello_matmul_pi3_generator.py,</code> and the generated <code>*.hat</code> and <code>*.o</code> files.</p>"},{"location":"Tutorials/Pi3_Cross_Compilation/#build-and-run","title":"Build and run","text":""},{"location":"Tutorials/Pi3_Cross_Compilation/#on-the-raspberry-pi-3-device","title":"On the Raspberry Pi 3 device","text":"<p>For this step, you'll be working with your Raspberry Pi device. If your Pi device is accessible over the network, copy <code>hello_matmul_pi3_runner.cpp</code>, <code>hello_matmul_pi3.hat</code>, and <code>hello_matmul_pi3.o</code> using the Unix scp tool or the Windows WinSCP tool here., otherwise use a USB thumb drive to transfer files manually. You do not need to copy the other generated files and folders.</p> <p>You also need gcc. Although it is often installed by default on Raspberry Pi 3 systems, type this for confirmation:</p> <pre><code>sudo apt-get install -y gcc\n</code></pre> <p>This has been verified with \"Raspbian GNU/Linux 9 (stretch)\" and gcc&lt;4:6.3.0-4&gt; and should work with subsequent versions. Now, you can run the following commands to build and run.</p> <pre><code>gcc hello_matmul_pi3_runner.cpp hello_matmul_pi3.o -o hello_matmul_pi3_runner\n./hello_matmul_pi3_runner\n</code></pre> <p>The output should look like:</p> <pre><code>Calling MatMul M=128, K=256, N=256\nResult (first few elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922\n</code></pre> <p>You can now experiment with the generated MatMul function with your own inputs. To try different inputs, you can modify <code>hello_matmul_pi3_runner.cpp</code> on the Raspberry Pi 3 and recompile it with the existing HAT package.</p>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/","title":"Hello MatMul on GPU","text":"<p>In this tutorial, you will learn how to implement a simple Matrix Multiplication (MatMul) function for execution on a GPU. We will use the Accera's Domain Specific Language (DSL) to produce a HAT package containing the MatMul function that can be called from the host to launch the MatMul function on the GPU.</p>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have Accera installed. If not, you can find the instructions in here.</li> <li>Be familiar with writing Python and C++ code.</li> <li>Be familiar with basic GPU programming and concepts.</li> <li>You have completed the Hello_MatMul tutorial.</li> </ul>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#review-the-naive-matmul-algorithm","title":"Review: the naive MatMul algorithm","text":"<p>As in the Hello_MatMul tutorial, we'll consider the example of multiplying matrices A and B and adding the result into matrix C. In NumPy syntax, this can be expressed as:</p> <pre><code>C += A @ B\n</code></pre> <p>A naive algorithm for matrix multiplication typically contains 3 nested for-loops. Expressed in Python, this can look like:</p> <pre><code>for i in range(M):\n    for j in range(N):\n        for k in range(K):\n            C[i, j] += A[i, k] * B[k, j]\n</code></pre>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#accera-python-dsl","title":"Accera Python DSL","text":"<p>We will now walk through a basic Matrix Multiplication (MatMul) using Accera. Additionally, we will direct Accera to execute this MatMul function on the default GPU.</p> <p>Create an empty file called <code>hello_matmul_gpu_generator.py</code>. Import dependent modules:</p> <pre><code>import accera as acc\n</code></pre> <p>Define some matrix sizes, where A's shape is M by K, B's is K by N, and C's, M by N.</p> <pre><code># Define our matrix sizes\nM = 2048\nN = 1024\nK = 2048\n</code></pre> <p>Declare arrays <code>A</code>, <code>B</code>, and <code>C</code>. These are our input and input/output matrices and hold 32-bit floating-point elements.</p> <pre><code>A = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(M, K))\nB = acc.Array(role=acc.Role.INPUT, element_type=acc.ScalarType.float32, shape=(K, N))\nC = acc.Array(role=acc.Role.INPUT_OUTPUT, element_type=acc.ScalarType.float32, shape=(M, N))\n</code></pre> <p>Use the <code>Nest</code> class to define our 3-layered nested for-loop and get the indices: <pre><code># Define the loop nest\nnest = acc.Nest(shape=(M, N, K))\n\n# Get the loop nest indices\ni, j, k = nest.get_indices()\n</code></pre></p> <p>Next, we define the logic for every iteration of the loop nest: <pre><code># Define the loop nest logic\n@nest.iteration_logic\ndef _():\n    C[i, j] += A[i, k] * B[k, j]\n</code></pre></p> <p>We have finished defining the logic of MatMul. Notice how, up to this point, it is identical to what we did for the CPU example. Let's now define the schedule to control the execution logic. To do this, we first create the schedule from the nest:</p> <pre><code>schedule = nest.create_schedule()\n</code></pre> <p>We will transform the iteration space and change the plan according to some predefined constants to execute this efficiently on our chosen hardware target. The values of these constants can come either from hardware target characteristics and the shapes of the arrays or can be found through auto-tuning. These will be explained in detail in a subsequent tutorial. For now, define:</p> <pre><code>block_x = 32\nblock_y = 32\n</code></pre> <p>Transform the iteration space to specify the thread block behavior. See (GPU blocks)[TODO:markdown...] section to learning more about optimizing block sizes on GPU: <pre><code>ii = schedule.split(i, block_x)\njj = schedule.split(j, block_y)\n</code></pre></p> <p>Set the order to traverse the iteration space. Note that the precise order of execution on GPU targets will be unknown due to the parallel nature of the hardware. Nevertheless, setting the order here is important since the coarse grain parallelization (e.g., grid) should precede the more fine-grained (e.g., warps/wavefronts): <pre><code>schedule.reorder(i, j, ii, jj, k)\n</code></pre></p> <p>Create a plan from the schedule. The plan allows us to control specific execution behavior on the hardware target (AMD MI100 in this example). The same schedule can be retargetted for a different platform like an NVIDIA GPU (acc.Target.Model.NVIDIA_RTX_A6000): <pre><code>target = acc.Target(acc.Target.Model.AMD_MI100)\nplan = schedule.create_plan(target)\n</code></pre></p> <p>Bind dimensions of the schedule to execution units on the GPU. Use the outer dimensions i, j to be the block indices y,x in the grid, and the ii and jj dimensions to be the thread indices y,x in the block: <pre><code>plan.bind({\n    i: target.GridUnit.BLOCK_Y,\n    j: target.GridUnit.BLOCK_X,\n    ii: target.GridUnit.THREAD_Y,\n    jj: target.GridUnit.THREAD_X\n})\n</code></pre></p> <p>Use the plan to add a callable function named <code>hello_matmul_gpu</code> to a HAT package.</p> <pre><code># Create a package and add a function to the package based on the plan\npackage = acc.Package()\npackage.add(plan, args=(A, B, C), base_name=\"hello_matmul_gpu\")\n</code></pre> <p>Finally, we build the HAT package, using the HAT_SOURCE format to produce code for the GPU: <pre><code>package.build(name=\"hello_matmul_gpu\", format=acc.Package.Format.HAT_SOURCE)\n</code></pre></p> <p>By now, you have all the code necessary to generate an Accera MatMul function that runs on the GPU. You can find the complete Python script here.</p>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#generate-hat-package","title":"Generate HAT package","text":"<p>Next, we run the generator script to produce a HAT package.</p> <pre><code>python hello_matmul_gpu_generator.py\n</code></pre> <p>After this script runs, you should see a header file <code>hello_matmul_gpu.hat</code> and a source file (such as <code>hello_matmul_gpu.cu</code>). In Accera, we call these files the \"HAT package\".</p> <p>The <code>.cu</code> source file contains C++ functions required to launch the kernel on the GPU (the source code below is only shown as an example, the actual generated code might be different based on optimizations, GPU target, cosmetic changes etc. with future Accera releases). Note the HIP compiler intrinsics in the generated code below since we used the AMD GPU target, similar target specific code will be emitted if the plan is created for a different GPU target:</p>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#host-launcher","title":"Host launcher","text":"<pre><code>#if !defined(__HIP_DEVICE_COMPILE__)\nvoid hello_matmul_gpu_f77287579284bbac_impl_2389286605904206643(float *arg0, float *arg1, float *arg2) {\nhello_matmul_gpu_f77287579284bbac__gpu__&lt;&lt;&lt;dim3(32, 64, 1), dim3(32, 32, 1), 0&gt;&gt;&gt;(arg0, arg1, arg2);\nreturn;\n}\n\n\n#endif // !defined(__HIP_DEVICE_COMPILE__)\n#if !defined(__HIP_DEVICE_COMPILE__)\nextern \"C\" __host__ void hello_matmul_gpu_f77287579284bbac(float *arg0, float *arg1, float *arg2) {\nhello_matmul_gpu_f77287579284bbac_impl_2389286605904206643(arg0, arg1, arg2);\nreturn;\n}\n</code></pre>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#gpu-kernel","title":"GPU kernel","text":"<pre><code>extern \"C\" __global__  __launch_bounds__(1024) void hello_matmul_gpu_f77287579284bbac__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\nextern __shared__ char sharedMemBaseAddr[];\nint32_t var0 = __builtin_amdgcn_workitem_id_x();\nint32_t var1 = __builtin_amdgcn_workitem_id_y();\nint32_t var2 = __builtin_amdgcn_workgroup_id_x();\nint32_t var3 = __builtin_amdgcn_workgroup_id_y();\nint32_t var4 = var3 * 32;\nint32_t var5 = var1 + var4;\nint32_t var6 = var2 * 32;\nint32_t var7 = var0 + var6;\n\n// Main K-loop\nfor (int32_t idx8 = 0; idx8 &lt; 2048; idx8 += 1) {\n// Matrix multiplication\nconst auto arg0_offset0 = var5 * 2048 + idx8 * 1;\nfloat var9 = ((float*)arg0)[arg0_offset0];\nconst auto arg1_offset1 = idx8 * 1024 + var7 * 1;\nfloat var10 = ((float*)arg1)[arg1_offset1];\nfloat var11 = var9 * var10;\nconst auto arg2_offset2 = var5 * 1024 + var7 * 1;\nfloat var12 = ((float*)arg2)[arg2_offset2];\nfloat var13 = var12 + var11;\n\n// Store the result to global memory\nconst auto arg2_offset3 = var5 * 1024 + var7 * 1;\n((float*)arg2)[arg2_offset3] = var13;\n}\n}\n</code></pre>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#execution-and-benchmarking-gpu-kernels-using-hatlib-recommended","title":"Execution and Benchmarking GPU kernels using <code>hatlib</code> (recommended)","text":"<p><code>hatlib</code> provides a convenient way to benchmark GPU kernels generated by Accera using Python. To benchmark the HAT package on an AMD MI100 system, install these dependencies: * ROCm (&gt;= 5.1) * hatlib (&gt;= 0.0.38)</p> <p>Run the following command: <pre><code>python3 -m hatlib.benchmark_hat_package &lt;path to hello_matmul_gpu.hat&gt; --cpp --min_time_in_sec 10 --time_in_ms\n</code></pre></p> <p>This will compile the generated GPU source code and execute it on the device with the provided benchmarking parameters. More details about the hatlib benchmarking tool can be found here.</p> <p>The above invocation of the hatlib tool will output the time in milliseconds (~13.7 ms) to execute the GPU kernel on an AMD MI100 system: <pre><code>                       function_name        mean  median_of_means  mean_of_small_means  robust_mean  min_of_means\n0  hello_matmul_gpu_f77287579284bbac 13.69571655      13.70083130          13.68229834  13.69727234   13.65689209\n</code></pre></p>"},{"location":"Tutorials/GPU/Hello_MatMul_GPU/#execution-using-standalone-c-runner-not-recommended","title":"Execution using standalone C++ runner (not recommended)","text":"<p>Since the Accera generated GPU kernel is source code, it can be compiled using the HIP compiler into a standalone C++ runner. Here's an example that calls the host launcher function from C++. An example of such a runner code is shown below:</p> <pre><code>// hello_matmul_gpu_runner.cpp\n#include &lt;stdio.h&gt;\n#include &lt;algorithm&gt;\n\n// Include the HAT file that declares our MatMul function\n#include \"hello_matmul_gpu.hat\"\n\n#define M 1024\n#define N 512\n#define K 256\n\nint main(int argc, const char** argv)\n{\n// Prepare our matrices (using the heap for large matrices)\nfloat* A = new float[M*K];\nfloat* B = new float[K*N];\nfloat* C = new float[M*N];\n\n// Fill with data\nstd::fill_n(A, M*K, 2.0f);\nstd::fill_n(B, K*N, 3.0f);\nstd::fill_n(C, M*N, 0.42f);\n\nfloat* dev_A;\nfloat* dev_B;\nfloat* dev_C;\nhipMalloc(&amp;dev_A, M * K * sizeof(float));\nhipMalloc(&amp;dev_B, K * N * sizeof(float));\nhipMalloc(&amp;dev_C, M * N * sizeof(float));\nhipMemcpy(dev_A, A, M * K * sizeof(float), hipMemcpyHostToDevice);\nhipMemcpy(dev_B, B, K * N * sizeof(float), hipMemcpyHostToDevice);\nhipMemcpy(dev_C, C, M * N * sizeof(float), hipMemcpyHostToDevice);\n\nprintf(\"Calling MatMul M=%d, K=%d, N=%d\\n\", M, K, N);\nhello_matmul_gpu_bbe110463fdb1f6b(A, B, C);\n\nhipDeviceSynchronize();\nhipMemcpy(C, dev_C, M * N * sizeof(float), hipMemcpyDeviceToHost);\n\nprintf(\"Result (first 10 elements): \");\nfor (int i = 0; i &lt; 10; ++i)\n{\nprintf(\"%f \", C[i]);\n}\nprintf(\"\\n\");\n\ndelete[] A;\ndelete[] B;\ndelete[] C;\nhipFree(dev_A);\nhipFree(dev_B);\nhipFree(dev_C);\nreturn 0;\n}\n</code></pre> <p>The above code creates the <code>A</code>, <code>B</code>, and <code>C</code> matrices and calls the function <code>hello_matmul_gpu</code> to perform MatMul.</p> <p>Now that we have the code, compile and link it with the HAT package to create an executable. The compilation/execution steps are left as an exercise for the reader. For more details, you can refer to the HIP compiler documentation here.</p> <p>The output should look like this:</p> <pre><code>Calling MatMul M=1024, K=256, N=512\nResult (first 10 elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922\n</code></pre> <p>You can now experiment with the generated MatMul function with your own inputs.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/","title":"Tensor MatMul on GPU: Caching","text":"<p>In this tutorial, you will learn how to implement a Matrix Multiplication (MatMul) function that uses specialized matrix multiplication hardware on the GPU while caching data to minimize expensive data traffic to/from global memory.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have completed the Tensor_MatMul_GPU tutorial.</li> </ul>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#input-data-caching","title":"Input data caching","text":"<p>Since accessing the same data repeatedly from the global memory can be expensive, we will use the shared memory which is available much closer to the compute units to cache the input data to achieve much faster data accesses.</p> <p>Since for input data caching we use shared memory, we need to be careful how much of the global data we cache since shared memory is comparatively much smaller in size. For this reason, we introduce an additional split in the K-loop and we use this newly created loop index <code>kk</code> for caching: <pre><code>kk = schedule.split(k, 256)\n</code></pre></p>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#sequential-caching","title":"Sequential caching","text":"<p>In this approach, each thread block starts caching the next tile of input data only after the computation on the current tile is complete. This is the most simple form of shared memory caching which does not involve any overlapped execution of data copy and computation pipelines. This can be achieved by adding the following lines of DSL code:</p> <pre><code>plan.cache(A, index=kk, location=target.MemorySpace.SHARED)\nplan.cache(B, index=kk, location=target.MemorySpace.SHARED)\n</code></pre> <p>The complete python script with caching of input data into shared memory can be found here.</p> <p>This generates the following kernel code, note the barriers in the generated code to see how caching of the next tile waits for the computation of the current tile to finish: <pre><code>extern \"C\" __global__  __launch_bounds__(256) void tensor_input_cache_matmul_gpu_866f5763c1d8d520__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Declare shared memory caches for A and B\n__shared__ float var8[32][256];\n__shared__ float var9[256][32];\n\n// k-loop\nfor (int32_t idx16 = 0; idx16 &lt; 8; idx16 += 1) {\nint32_t var17 = idx16 * 256;\n\n// Wait for compute on previously cached items to finish\n__builtin_amdgcn_s_barrier();\n\n// Cache current tile of A into shared memory\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/32, 256, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::None, MemSpace::Shared, float&gt;(var11, (float*)arg0, var7, var17, affine_map_func_0_i0, (float*)var8);\n\n// Cache current tile of B into shared memory\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/256, 32, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::None, MemSpace::Shared, float&gt;(var11, (float*)arg1, var17, var5, affine_map_func_1_i0, (float*)var9);\n\n// Wait for input caching to finish\n__builtin_amdgcn_s_barrier();\n\n// kk-loop\nfor (int32_t idx18 = 0; idx18 &lt; 64; idx18 += 1) {\nint32_t var19 = idx18 * 4;\n\n// Declare matrix fragments for A, B and C\n/*...*/\n\n// Load C from global memory\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var11, mmaMatrix_22, arg2 + ...);\n\n// Load A and B from shared memory cache\nrocwmma::load_matrix_sync&lt;256&gt;(var11, mmaMatrix_20, &amp;var8[var12][var19]);\nrocwmma::load_matrix_sync&lt;32&gt;(var11, mmaMatrix_21, &amp;var9[var19][var14]);\n\n// Compute matrix multiplication\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_22, mmaMatrix_20, mmaMatrix_21, mmaMatrix_22);\n\n// Store result into global memory\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var11, arg2 + ..., mmaMatrix_22);\n}\n}\n}\n</code></pre></p>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#benchmarking-results-using-hatlib","title":"Benchmarking results using <code>hatlib</code>","text":"<p>Similar to the previous experiments we can use hatlib to benchmark this kernel using the following command: <pre><code>python3 -m hatlib.benchmark_hat_package &lt;path to tensor_input_cache_matmul_gpu.hat&gt; --cpp --min_time_in_sec 10 --time_in_ms\n</code></pre></p> <p>This produces the following output which shows that sequential caching reduces the runtime to ~3 ms which is ~30% faster than the non-cached version presented in Tensor_MatMul_GPU.md:</p> <pre><code>                                    function_name       mean  median_of_means  mean_of_small_means  robust_mean  min_of_means\n0  tensor_input_cache_matmul_gpu_866f5763c1d8d520 3.02507486       3.02532532           3.02407842   3.02519751    3.02233856\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#overlapped-caching-aka-double-buffering","title":"Overlapped caching (a.k.a. Double Buffering)","text":"<p>In this approach, each thread block prefetches the next tile into registers while the current tile is being computed. This overlapped execution of data copy and compute typically achieves better performance by utilizing different hardware pipelines more efficiently. Using Accera DSL, this can be done by setting the <code>double_buffer</code> flag in the <code>plan.cache</code> call:</p> <pre><code>plan.cache(A, index=kk, location=target.MemorySpace.SHARED, double_buffer=True, double_buffer_location=target.MemorySpace.PRIVATE)\nplan.cache(B, index=kk, location=target.MemorySpace.SHARED, double_buffer=True, double_buffer_location=target.MemorySpace.PRIVATE)\n</code></pre> <p>The complete python script with caching of input data using double buffering can be found here.</p> <p>The generated kernel code looks something like this, note how the prefetch of the next tile and the computation of the current tile happen without synchronization to achieve global memory latency hiding: <pre><code>extern \"C\" __global__  __launch_bounds__(256) void tensor_input_double_buffer_cache_matmul_gpu_ce60189b3e52267d__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Declare register caches for prefetching input data of A and B\nfloat var8[32][1];\nfloat var9[32][1];\n\n// Declare shared memory caches for A and B\n__shared__ float var10[32][256];\n__shared__ float var11[256][32];\n\n// Cache tile 0 of A from global memory to shared memory\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/32, 256, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::None, MemSpace::Shared, float&gt;(var13, (float*)arg0, var7, 0, affine_map_func_0_i0, (float*)var10);\n\n// Cache tile 0 of B from global memory to shared memory\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/256, 32, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::None, MemSpace::Shared, float&gt;(var13, (float*)arg1, 0, var5, affine_map_func_1_i0, (float*)var11);\n\n// Wait for tile 0 data to finish copying\n__builtin_amdgcn_s_barrier();\n\n// k-loop (Current tile)\nfor (int32_t idx18 = 0; idx18 &lt; 7; idx18 += 1) {\n// Prefetch next tile of A from global memory to registers\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/32, 256, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::None, MemSpace::Private, float&gt;(\nvar13, (float*)arg0, var7, var20, affine_map_func_0_i0, (float*)var9);\n\n// Prefetch next tile of B from global memory to registers\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/256, 32, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::None, MemSpace::Private, float&gt;(\nvar13, (float*)arg1, var20, var5, affine_map_func_1_i0, (float*)var8);\n\n// kk-loop\nfor (int32_t idx24 = 0; idx24 &lt; 64; idx24 += 1) {\n// Declare matrix fragments for A, B and C\n/*...*/\n\n// Perform matmul on the current tile from shared memory\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var13, mmaMatrix_28, arg2 + ...);\nrocwmma::load_matrix_sync&lt;256&gt;(var13, mmaMatrix_26, &amp;var10[var14][var25]);\nrocwmma::load_matrix_sync&lt;32&gt;(var13, mmaMatrix_27, &amp;var11[var25][var16]);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_28, mmaMatrix_26, mmaMatrix_27, mmaMatrix_28);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var13, arg2 + ..., mmaMatrix_28);\n}\n\n// Wait for matmul on current tile to finish\n__builtin_amdgcn_s_barrier();\n\n// Copy prefetched data of A from registers to shared memory\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/256, 32, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::Private, MemSpace::Shared, float&gt;(var13, (float*)var11, 0, 0, affine_map_func_4_i0, (float*)var8);\n\n// Copy prefetched data of B from registers to shared memory\nblock_copy&lt;CopyMode::Striped, /*SRC_ROW_MAJOR*/ 1, /*DST_ROW_MAJOR*/ 1, /*STRIDE*/ 1, /*WPT*/ 32, /*TILE_R,C*/32, 256, /*BLOCK_DIM_X,Y,Z*/ 128, 2, 1, MemSpace::Private, MemSpace::Shared, float&gt;(var13, (float*)var10, 0, 0, affine_map_func_3_i0, (float*)var9);\n\n// Wait for copy to finish before starting next tile\n__builtin_amdgcn_s_barrier();\n}\n\n// Last tile (loop peeling)\nfor (int32_t idx19 = 0; idx19 &lt; 64; idx19 += 1) {\n// Declare matrix fragments for A, B and C\n/*...*/\n\n// Perform matmul on the last tile from shared memory\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var13, mmaMatrix_23, arg2 + ...);\nrocwmma::load_matrix_sync&lt;256&gt;(var13, mmaMatrix_21, &amp;var10[var14][var20]);\nrocwmma::load_matrix_sync&lt;32&gt;(var13, mmaMatrix_22, &amp;var11[var20][var16]);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_23, mmaMatrix_21, mmaMatrix_22, mmaMatrix_23);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var13, arg2 + ..., mmaMatrix_23);\n}\n}\n</code></pre></p>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#benchmarking-results-using-hatlib_1","title":"Benchmarking results using <code>hatlib</code>","text":"<p>Benchmarking the above kernel with hatlib shows that double-buffer caching further reduces the runtime to ~1.45 ms which is ~66% faster than the non-cached version presented in Tensor_MatMul_GPU.md:</p> <pre><code>                                       function_name       mean  median_of_means  mean_of_small_means  robust_mean  min_of_means\n0  tensor_input_double_buffer_cache_matmul_gpu_ce... 1.45501032       1.45495605           1.45370367   1.45489838    1.45116257\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#output-data-caching","title":"Output data caching","text":"<p>Similar to input caching, the result data can also be cached to prevent unnecessary global memory accesses. Here we will see how we can accumulate the result in registers before copying it to global memory. This is can be done by adding:</p> <pre><code>plan.cache(C, index=k, location=target.MemorySpace.MMA_FRAGMENT)\n</code></pre> <p>The complete python script with both input and output caching can be found here.</p> <pre><code>extern \"C\" __global__  __launch_bounds__(256) void tensor_input_output_cache_matmul_gpu_1b4d39ede237d688__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Declare register caches for prefetching input data of A and B\nfloat var8[32][1];\nfloat var9[32][1];\n\n// Declare shared memory caches for A and B\n__shared__ float var10[32][256];\n__shared__ float var11[256][32];\n\n// Declare fragment cache (registers) for output, C\nrocwmma::fragment&lt;rocwmma::accumulator, 16, 16, 4, 1, 1, float&gt; mmaMatrix_12;\n\n// Fill output cache with data from global memory\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var18, mmaMatrix_12, arg2 + ...);\n\n// Cache tile 0 of A and B from global memory to shared memory\n/*...*/\n\n// Wait for tile 0 data to finish copying\n__builtin_amdgcn_s_barrier();\n\n// k-loop (Current tile)\nfor (int32_t idx19 = 0; idx19 &lt; 7; idx19 += 1) {\n// Prefetch next tile of A and B from global memory to registers\n/*...*/\n\n// kk-loop\nfor (int32_t idx25 = 0; idx25 &lt; 64; idx25 += 1) {\n// Declare matrix fragments for A and B\n/*...*/\n\n// Load A and B from shared memory cache\n/*...*/\n\n// Compute matrix multiplication and accumulate in fragment cache\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_12, mmaMatrix_27, mmaMatrix_28, mmaMatrix_12);\n}\n\n// Wait for matmul on current tile to finish\n__builtin_amdgcn_s_barrier();\n\n// Copy prefetched data of A and B from registers to shared memory\n/*...*/\n\n// Wait for copy to finish before starting next tile\n__builtin_amdgcn_s_barrier();\n}\n\n// Last tile (loop peeling)\nfor (int32_t idx20 = 0; idx20 &lt; 64; idx20 += 1) {\n// Declare matrix fragments for A and B\n/*...*/\n\n// Load A and B from shared memory cache\n/*...*/\n\n// Compute matrix multiplication and accumulate in fragment cache\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_12, mmaMatrix_22, mmaMatrix_23, mmaMatrix_12);\n}\n\n// Store result into global memory ONCE!\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var18, arg2 + ..., mmaMatrix_12);\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_Caching_GPU/#benchmarking-results-using-hatlib_2","title":"Benchmarking results using <code>hatlib</code>","text":"<p>Benchmarking the above kernel with hatlib shows that double-buffer caching combined with output caching further reduces the runtime to ~1.34 ms which is an overall ~69% improvement compared to the non-cached version presented in Tensor_MatMul_GPU.md:</p> <pre><code>                                       function_name       mean  median_of_means  mean_of_small_means  robust_mean  min_of_means\n0  tensor_input_output_cache_matmul_gpu_1b4d39ede... 1.33967323       1.33956711           1.33841698   1.33953149    1.33726944\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_ElementWiseOps_GPU/","title":"Tensor MatMul on GPU: Fused Element-wise Operations","text":"<p>In this tutorial, you will learn how to implement a composite op which performs Matrix Multiplication (MatMul) on GPU tensor cores along with element-wise operations on the result data in an optimized way.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_ElementWiseOps_GPU/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have completed the Tensor_MatMul_GPU tutorial.</li> </ul>"},{"location":"Tutorials/GPU/Tensor_MatMul_ElementWiseOps_GPU/#accera-implementation","title":"Accera implementation","text":"<p>Often times we want to perform element-wise operation (auxiliary) on the data before/after the main operation. For example, for non-unit alpha (\u03b1 != 1) in GEMM, we want to scale each element of the matrix multiplication result by a constant value. For these scenarios, we enable the use of prologue and epilogue ops on tensor data for pre/post processing of data held in tensor fragments. Note that when tensorization is not used, this can be simply achieved by fusion.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_ElementWiseOps_GPU/#prologue-tensor-operation","title":"Prologue Tensor Operation","text":"<p>This operation is applied on each element of the tensor fragment data before matrix multiplication is done. Here is an example where we use the <code>SET</code> operation to zero-initialize the result fragment before matmul is performed:</p> <pre><code>plan.tensorize(indices=tensor_indices, mma_shape=mma_shape, prologue_op=acc.MMAFragmentOp.SET, prologue_arg=0.0)\n</code></pre> <p>The complete python script with 0-init of result data can be found here. The generated source code looks something like this (note the <code>fill_fragment</code> call below instead of <code>load_matrix_sync</code>):</p> <pre><code>extern \"C\" __global__  __launch_bounds__(256) void tensor_zero_init_matmul_gpu_d6f8bdcfb87f53cc__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Main K-loop\nfor (int32_t idx14 = 0; idx14 &lt; 512; idx14 += 1) {\n// Declare matrix fragments for A, B and C\n/*...*/\n\n// Tensor Prologue: Fill result fragment with 0s\nrocwmma::fill_fragment(mmaMatrix_18, float{});\n\n// Matrix multiplication\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_17, arg1 + ...);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_18, mmaMatrix_16, mmaMatrix_17, mmaMatrix_18);\n\n// Store result into global memory\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, arg2 + ..., mmaMatrix_18);\n}\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_ElementWiseOps_GPU/#epilogue-tensor-operation","title":"Epilogue Tensor Operation","text":"<p>This operation is applied on each element of the tensor fragment data after matrix multiplication is done. Similar to <code>prologue_op</code>, we can also set the <code>epilogue_op</code> argument to achieve this. Here is an example of how alpha scaling of matmul result can be done with the following code:</p> <pre><code>plan.tensorize(indices=tensor_indices, mma_shape=mma_shape, prologue_op=acc.MMAFragmentOp.SET, prologue_arg=0.0, epilogue_op=acc.MMAFragmentOp.SCALE, epilogue_arg=5.0)\n</code></pre> <p>The complete python script with alpha-scaling of result data can be found here. The generated source code looks something like this:</p> <pre><code>extern \"C\" __global__  __launch_bounds__(64) void tensor_alpha_scaling_matmul_gpu_e5c7114024bfca18__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Main K-loop\nfor (int32_t idx14 = 0; idx14 &lt; 512; idx14 += 1) {\n// Declare matrix fragments for A, B and C\n/*...*/\n\n// Tensor Prologue: Fill result fragment with 0s\nrocwmma::fill_fragment(mmaMatrix_18, float{});\n\n// Matrix multiplication\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_17, arg1 + ...);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_18, mmaMatrix_16, mmaMatrix_17, mmaMatrix_18);\n\n// Tensor Epilogue: Alpha scaling of matmul result\n{\nfloat* mmaMatrix_18_data = reinterpret_cast&lt;float*&gt;(&amp;mmaMatrix_18);\nfor (int i = 0; i &lt; sizeof(mmaMatrix_18) / sizeof(float); ++i) {\nmmaMatrix_18_data[i] *= float{5.000000e+00};\n}\n}\n\n// Store result into global memory\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, arg2 + ..., mmaMatrix_18);\n}\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_ElementWiseOps_GPU/#general-matrix-multiply-gemm-using-prologue-and-epilogue-ops","title":"General Matrix Multiply (GEMM) using Prologue and Epilogue Ops","text":"<p>The general matric multiplication problem can be formulated as below, where <code>A</code>, <code>B</code>, and <code>C</code> are matrices and <code>\u03b1</code> and <code>\u03b2</code> are scalar constants: <pre><code>C = \u03b1.(A @ B) + \u03b2.C\n</code></pre></p> <p>GEMM with some of the different combinations of <code>\u03b1</code> and <code>\u03b2</code> can be computed using prologue and epilogue ops along with tensorization as summarized in the table below:</p> \u03b2 == 0 \u03b2 == 1 \u03b2 != 1 \u03b1==1 C = A @ B: <code>prologue_op</code> = MMAFragmentOp.SET, <code>prologue_arg</code> = 0 C += A @ B: no need to set prologue and epilogue args C = A @ B + \u03b2.C: <code>prologue_op</code> = MMAFragmentOp.SCALE, <code>prologue_arg</code> = \u03b2 \u03b1!=1 C = \u03b1.(A @ B): <code>prologue_op</code> = MMAFragmentOp.SET, <code>prologue_arg</code> = 0, <code>epilogue_op</code> = MMAFragmentOp.SCALE, <code>epilogue_arg</code> = \u03b1 C += \u03b1.(A @ B): use fusion C = \u03b1.(A @ B) + \u03b2.C: use fusion"},{"location":"Tutorials/GPU/Tensor_MatMul_GPU/","title":"Tensor MatMul on GPU: Basic Tutorial","text":"<p>In this tutorial, you will learn how to implement a Matrix Multiplication (MatMul) function that uses specialized matrix multiplication hardware on the GPU.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_GPU/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have familiarized yourself with the concepts presented in the Tensorization manual page.</li> <li>You have completed the Hello_MatMul_GPU tutorial.</li> </ul>"},{"location":"Tutorials/GPU/Tensor_MatMul_GPU/#accera-implementation","title":"Accera Implementation","text":"<p>To tensorize a nest we need to create additional splits such that the innermost 3 dimensions are of shape {m, n, p * k} for MMA shape MmxNnxKk_Bb, where p is the number of passes. For example, when using MMA shape <code>M16xN16xK4_B1</code>, for a single pass we need to split the innermost dimensions to be of shape {16, 16, 4} for them to be tensorizable. The split factors for a specific MMA shape and the given number of passes can be calculated by the use of the helper function <code>compute_tensor_splits</code> on the <code>TensorCoreInformation</code> class as shown below: <pre><code>mma_shape = acc.MMAShape.M16xN16xK4_B1\ntensor_splits = target.tensor_core_info.compute_tensor_splits(mma_shape) # num_total_passes = 1\niii, jjj, kk = schedule.tile({\n    ii: tensor_splits[0],\n    jj: tensor_splits[1],\n    k: tensor_splits[2]\n})\n</code></pre></p> <p>Type compatibility for the different values of <code>MMAShape</code> is documented here.</p> <p>Now we would want to schedule the block-level iteration space as the outermost iteration, followed by warp-level iteration space and finally the tensorized iteration space: <pre><code>block_indices = (i, j)\nwarp_indices = (ii, jj)\ntensor_indices = (iii, jjj, kk)\n\nschedule.reorder(*block_indices, k, *warp_indices, *tensor_indices)\n</code></pre></p> <p>Since tensor core primitives operate at a warp level rather than at the thread level, we bind the schedule dimensions to the corresponding warp indices: <pre><code>plan.bind({\n    i: target.GridUnit.BLOCK_Y,\n    j: target.GridUnit.BLOCK_X,\n    ii: target.GridUnit.WARP_Y,\n    jj: target.GridUnit.WARP_X\n})\n</code></pre></p> <p>After having done all the necessary splits and setting their order, we need to call <code>tensorize(...)</code> on the <code>plan</code> object. This will cause the lowering logic to produce warp-level matrix multiplication primitives which can utilize specialized tensor cores on the GPU: <pre><code>plan.tensorize(indices=tensor_indices, mma_shape=mma_shape)\n</code></pre></p> <p>By now, you have all the code necessary to generate an Accera MatMul function that runs on tensor cores on the GPU. You can find the complete Python script here.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_GPU/#generate-hat-package","title":"Generate HAT package","text":"<p>Next, we run the generator script to produce a HAT package.</p> <pre><code>python tensor_matmul_gpu_generator.py\n</code></pre> <p>The <code>.cu</code> source file now contains a launcher with different grid parameters based on the splits created for tensorization:</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_GPU/#host-launcher","title":"Host launcher","text":"<pre><code>#if !defined(__HIP_DEVICE_COMPILE__)\nvoid tensor_matmul_gpu_cb6af7a31162e75c_impl_2389286605904206643(float *arg0, float *arg1, float *arg2) {\ntensor_matmul_gpu_cb6af7a31162e75c__gpu__&lt;&lt;&lt;dim3(32, 64, 1), dim3(128, 2, 1), 0&gt;&gt;&gt;(arg0, arg1, arg2);\nreturn;\n}\n\n\n#endif // !defined(__HIP_DEVICE_COMPILE__)\n#if !defined(__HIP_DEVICE_COMPILE__)\nextern \"C\" __host__ void tensor_matmul_gpu_cb6af7a31162e75c(float *arg0, float *arg1, float *arg2) {\ntensor_matmul_gpu_cb6af7a31162e75c_impl_2389286605904206643(arg0, arg1, arg2);\nreturn;\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_GPU/#gpu-kernel","title":"GPU kernel","text":"<p>The GPU kernel now contains warp-level primitives for loading, multiplying and storing matrices using tensor cores: <pre><code>extern \"C\" __global__  __launch_bounds__(256) void tensor_matmul_gpu_cb6af7a31162e75c__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Main K-loop\nfor (int32_t idx14 = 0; idx14 &lt; 512; idx14 += 1) {\nint32_t var15 = idx14 * 4;\n\n// Declare matrix fragments for A, B and C\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_16;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_17;\nrocwmma::fragment&lt;rocwmma::accumulator, 16, 16, 4, 1, 1, float&gt; mmaMatrix_18;\n\n// Load matrix fragments from global memory\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, mmaMatrix_18, arg2 + ...);\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_17, arg1 + ...);\n\n// Compute matrix multiplication\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_18, mmaMatrix_16, mmaMatrix_17, mmaMatrix_18);\n\n// Store result into global memory\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, arg2 + ..., mmaMatrix_18);\n}\n}\n</code></pre></p>"},{"location":"Tutorials/GPU/Tensor_MatMul_GPU/#benchmarking-results-using-hatlib","title":"Benchmarking results using <code>hatlib</code>","text":"<p>Running the following command on machine with an AMD MI100 GPU: <pre><code>python -m hatlib.benchmark_hat_package &lt;path to tensor_matmul_gpu.hat&gt; --cpp --min_time_in_sec 10 --time_in_ms\n</code></pre> produces the following output. Note that compared to the non-tensorized implementation presented in Hello_MatMul_GPU.md, this implementation takes only 4.3 ms which is roughly 3x faster: <pre><code>                        function_name       mean  median_of_means  mean_of_small_means  robust_mean  min_of_means\n0  tensor_matmul_gpu_cb6af7a31162e75c 4.33855977       4.33607330           4.33025297   4.33672341    4.32588257\n</code></pre></p>"},{"location":"Tutorials/GPU/Tensor_MatMul_MultiPass/","title":"Tensor MatMul on GPU: Multi-Pass","text":"<p>In this tutorial, we will see how to direct Accera compiler to generate multi-pass tensorization code and how we can use pass fusion to control input data (matrices <code>A</code> and <code>B</code>) register allocation.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_MultiPass/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have completed the Tensor_MatMul_GPU tutorial.</li> </ul>"},{"location":"Tutorials/GPU/Tensor_MatMul_MultiPass/#background","title":"Background","text":"<p>Multi-pass tensorization allows for unrolling of tensor loops in the K-dimension as shown in the examples below. Fusion of passes allows for better control of registers required for input data (<code>A</code>/<code>B</code>). For example, if the total number of passes to be executed is 8 and number of fused passes is 2, this means there are 4 pass groups of 2 fused passes each. This generates code which does the following 4 times (once per pass group): - Allocate registers for input data required for 2 passes - Load input data for 2 passes - Compute matmul for 2 passes - Store result for 2 passes</p> <p>In effect, increasing the number of fused passes leads to increased register usage for input data and more densely packed memory I/O and compute instructions. Similarly, reducing the number of fused passes implies less register pressure and more interleaving among the memory I/O and the compute instructions. The number of passes to be fused can be controlled by setting the <code>num_fused_passes</code> parameter in the <code>plan.tensorize</code> function call.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_MultiPass/#full-pass-fusion","title":"Full Pass Fusion","text":"<p>We are going to change the call to <code>compute_tensor_splits</code> to pass the <code>num_total_passes</code> argument and also set it on the <code>plan.tensorize</code> call. Not setting the <code>num_fused_passes</code> argument on the <code>plan.tensorize</code> leaves it in its default setting which is to fuse all the passes:</p> <pre><code>num_passes = 8\ntensor_splits = target.tensor_core_info.compute_tensor_splits(mma_shape, num_passes)\n...\nplan.tensorize(indices=tensor_indices, mma_shape=mma_shape, num_total_passes=num_passes)\n</code></pre> <p>The generated source code allocates registers for input data for all the 8 passes and loads them all before performing matrix multiplication as shown below:</p> <pre><code>extern \"C\" __global__  __launch_bounds__(256) void tensor_multipass_full_fusion_matmul_gpu_9d020531735492ef__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Main K-loop\nfor (int32_t idx14 = 0; idx14 &lt; 64; idx14 += 1) {\nint32_t var15 = idx14 * 32;\n\n// Declare matrix fragments for A (8 passes)\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_16;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_17;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_18;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_19;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_20;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_21;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_22;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_23;\n\n// Declare matrix fragments for B (8 passes)\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_24;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_25;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_26;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_27;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_28;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_29;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_30;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_31;\n\n// Declare matrix fragment for C\nrocwmma::fragment&lt;rocwmma::accumulator, 16, 16, 4, 1, 1, float&gt; mmaMatrix_32;\n\n// Load matrix fragment for C\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, mmaMatrix_32, arg2 + ...);\n\n// Load matrix fragments for A (8 passes)\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nint32_t var33 = var15 + 4;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_17, arg0 + ...);\nint32_t var34 = var15 + 8;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_18, arg0 + ...);\nint32_t var35 = var15 + 12;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_19, arg0 + ...);\nint32_t var36 = var15 + 16;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_20, arg0 + ...);\nint32_t var37 = var15 + 20;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_21, arg0 + ...);\nint32_t var38 = var15 + 24;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_22, arg0 + ...);\nint32_t var39 = var15 + 28;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_23, arg0 + ...);\n\n// Load matrix fragments for B (8 passes)\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_24, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_25, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_26, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_27, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_28, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_29, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_30, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_31, arg1 + ...);\n\n// Matrix multiplication for Pass Group 0 (8 passes)\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_16, mmaMatrix_24, mmaMatrix_32);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_17, mmaMatrix_25, mmaMatrix_32);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_18, mmaMatrix_26, mmaMatrix_32);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_19, mmaMatrix_27, mmaMatrix_32);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_20, mmaMatrix_28, mmaMatrix_32);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_21, mmaMatrix_29, mmaMatrix_32);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_22, mmaMatrix_30, mmaMatrix_32);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_32, mmaMatrix_23, mmaMatrix_31, mmaMatrix_32);\n\n// Store matrix fragment for C\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, arg2 + ..., mmaMatrix_32);\n}\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_MultiPass/#partial-pass-fusion","title":"Partial Pass Fusion","text":"<p>With partial pass fusion, we will explicitly set the <code>num_fused_passes</code> argument to 2 on the <code>plan.tensorize</code> call as shown below:</p> <pre><code>plan.tensorize(indices=tensor_indices, mma_shape=mma_shape, num_total_passes=num_passes, num_fused_passes=2)\n</code></pre> <p>By doing this we are effectively reducing the register requirement for input data by a factor of 4 since matrix multiplication is performed for only 2 passes at a time:</p> <pre><code>extern \"C\" __global__  __launch_bounds__(256) void tensor_multipass_partial_fusion_matmul_gpu_0fcbcea0448782e8__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Main K-loop\nfor (int32_t idx14 = 0; idx14 &lt; 64; idx14 += 1) {\nint32_t var15 = idx14 * 32;\n\n// Declare matrix fragments for A (2 passes)\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_16;\nrocwmma::fragment&lt;rocwmma::matrix_a, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_17;\n\n// Declare matrix fragments for B (2 passes)\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_18;\nrocwmma::fragment&lt;rocwmma::matrix_b, 16, 16, 4, 1, 1, float, rocwmma::row_major&gt; mmaMatrix_19;\n\n// Declare matrix fragment for C\nrocwmma::fragment&lt;rocwmma::accumulator, 16, 16, 4, 1, 1, float&gt; mmaMatrix_20;\n\n// Load matrix fragment for C\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, mmaMatrix_20, arg2 + ...);\n\n// Matrix multiplication for Pass Group 0 (pass 0, 1)\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nint32_t var21 = var15 + 4;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_17, arg0 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_18, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_19, arg1 + ...);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_16, mmaMatrix_18, mmaMatrix_20);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_17, mmaMatrix_19, mmaMatrix_20);\n\n// Matrix multiplication for Pass Group 1 (pass 2, 3)\nint32_t var22 = var15 + 8;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nint32_t var23 = var15 + 12;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_17, arg0 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_18, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_19, arg1 + ...);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_16, mmaMatrix_18, mmaMatrix_20);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_17, mmaMatrix_19, mmaMatrix_20);\n\n// Matrix multiplication for Pass Group 2 (pass 4, 5)\nint32_t var24 = var15 + 16;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nint32_t var25 = var15 + 20;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_17, arg0 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_18, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_19, arg1 + ...);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_16, mmaMatrix_18, mmaMatrix_20);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_17, mmaMatrix_19, mmaMatrix_20);\n\n// Matrix multiplication for Pass Group 3 (pass 6, 7)\nint32_t var26 = var15 + 24;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_16, arg0 + ...);\nint32_t var27 = var15 + 28;\nrocwmma::load_matrix_sync&lt;2048&gt;(var7, mmaMatrix_17, arg0 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_18, arg1 + ...);\nrocwmma::load_matrix_sync&lt;1024&gt;(var7, mmaMatrix_19, arg1 + ...);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_16, mmaMatrix_18, mmaMatrix_20);\nrocwmma::mma_sync&lt;0, 0, 0&gt;(mmaMatrix_20, mmaMatrix_17, mmaMatrix_19, mmaMatrix_20);\n\n// Store matrix fragment for C\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var7, arg2 + var10 * 1024 + var13 * 1, mmaMatrix_20);\n}\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/","title":"Tensor MatMul on GPU: Scheduling Policy experiments","text":"<p>In this tutorial, you will learn how to use different scheduling policies to control GPU register usage while using tensor cores for matrix multiplication.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have completed the Tensor_MatMul_GPU tutorial.</li> </ul>"},{"location":"Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/#background","title":"Background","text":"<p>Some MMA shapes perform matrix multiplication over multiple invocations of the same MMA instruction with different arguments. Conventionally, MMA shape of MmxNnxKk_Bb performs matmul of A {m x k} and B {k x n} to produce the result of size {m x n} containing b chunks or blocks. For example, M64xN64xK1_B4 produces result matrix of size 64x64 in 4 blocks of size 16x64 each where the first 16 rows of the result form block 1, the next 16 rows form block 2 and so on.</p> <p>For MMA shapes where b is greater than 1, Accera allocates registers for output data based on the <code>scheduling_policy</code> parameter passed to the <code>plan.tensorize</code> call (conversely, when b is 1, this parameter will not affect the emitted GPU code). Currently, Accera supports 2 different values of <code>scheduling_policy</code>, as mentioned in MMASchedulingPolicy.md, which expose different register usage vs. memory I/O tradeoffs as explained in detail below.</p>"},{"location":"Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/#block-order","title":"Block order","text":"<p>In this mode blocks are computed sequentially, which means allocation for registers required for accumulator/output data is done for a single block. This mode can be enable by setting the <code>scheduling_policy</code> parameter as shown below:</p> <pre><code>plan.tensorize(indices=tensor_indices, mma_shape=acc.MMAShape.M64xN64xK1_B4, scheduling_policy=acc.MMASchedulingPolicy.BLOCK_ORDER)\n</code></pre> <p>The generated source code looks something like this (note the accumulator fragment <code>mmaMatrix_24</code> being reused for each block currently being computed):</p> <pre><code>extern \"C\" __global__  __launch_bounds__(64) void tensor_block_order_matmul_gpu_a0f5a6cd5453d086__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Main K-loop\nfor (int32_t idx20 = 0; idx20 &lt; 512; idx20 += 1) {\n// Declare matrix fragments for A, B and C\n/*...*/\n\n// Load matrix A data\nrocwmma::load_matrix_sync&lt;2048&gt;(var0, mmaMatrix_22, arg0 + ...);\n\n// Load matrix B data\nrocwmma::load_matrix_sync&lt;1024&gt;(var0, mmaMatrix_23, arg1 + ...);\n\n// Matrix multiplication yieling block 0\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_24, arg2 + ...);\nrocwmma::mma_sync&lt;2, 0, 0&gt;(mmaMatrix_24, mmaMatrix_22, mmaMatrix_23, mmaMatrix_24);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_24);\n\n// Matrix multiplication yieling block 1\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_24, arg2 + ...);\nrocwmma::mma_sync&lt;2, 1, 0&gt;(mmaMatrix_24, mmaMatrix_22, mmaMatrix_23, mmaMatrix_24);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_24);\n\n// Matrix multiplication yieling block 2\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_24, arg2 + ...);\nrocwmma::mma_sync&lt;2, 2, 0&gt;(mmaMatrix_24, mmaMatrix_22, mmaMatrix_23, mmaMatrix_24);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_24);\n\n// Matrix multiplication yieling block 3\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_24, arg2 + ...);\nrocwmma::mma_sync&lt;2, 3, 0&gt;(mmaMatrix_24, mmaMatrix_22, mmaMatrix_23, mmaMatrix_24);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_24);\n}\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/#pass-order","title":"Pass order","text":"<p>This operation is applied on each element of the tensor fragment data after matrix multiplication is done. Similar to <code>prologue_op</code>, we can also set the <code>epilogue_op</code> argument to achieve this. Here is an example of how alpha scaling of matmul result can be done with the following code:</p> <pre><code>plan.tensorize(indices=tensor_indices, mma_shape=acc.MMAShape.M64xN64xK1_B4, scheduling_policy=acc.MMASchedulingPolicy.PASS_ORDER)\n</code></pre> <p>The generated source code shows how accumulator data for all the 4 blocks is loaded before the compute phase.:</p> <pre><code>extern \"C\" __global__  __launch_bounds__(64) void tensor_pass_order_matmul_gpu_0d6383ac17fdfc9c__gpu__(float *arg0, float *arg1, float *arg2) {\n// Calculate threadid offsets and other locals\n/*...*/\n\n// Main K-loop\nfor (int32_t idx20 = 0; idx20 &lt; 512; idx20 += 1) {\n// Declare matrix fragments for A, B and C\n/*...*/\n\n// Load matric C data for block 0\nrocwmma::fragment&lt;rocwmma::accumulator, 64, 64, 4, 4, 1, float&gt; mmaMatrix_24;\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_24, arg2 + ...);\n\n// Load matric C data for block 1\nrocwmma::fragment&lt;rocwmma::accumulator, 64, 64, 4, 4, 1, float&gt; mmaMatrix_25;\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_25, arg2 + ...);\n\n// Load matric C data for block 2\nrocwmma::fragment&lt;rocwmma::accumulator, 64, 64, 4, 4, 1, float&gt; mmaMatrix_26;\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_26, arg2 + ...);\n\n// Load matric C data for block 3\nrocwmma::fragment&lt;rocwmma::accumulator, 64, 64, 4, 4, 1, float&gt; mmaMatrix_27;\nrocwmma::load_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, mmaMatrix_27, arg2 + ...);\n\n// Load matric A data\nrocwmma::load_matrix_sync&lt;2048&gt;(var0, mmaMatrix_22, arg0 + ...);\n\n// Load matric B data\nrocwmma::load_matrix_sync&lt;1024&gt;(var0, mmaMatrix_23, arg1 + ...);\n\n// Compute matrix multiplication for blocks [0, 4)\nrocwmma::mma_sync&lt;2, 0, 0&gt;(mmaMatrix_24, mmaMatrix_22, mmaMatrix_23, mmaMatrix_24);\nrocwmma::mma_sync&lt;2, 1, 0&gt;(mmaMatrix_25, mmaMatrix_22, mmaMatrix_23, mmaMatrix_25);\nrocwmma::mma_sync&lt;2, 2, 0&gt;(mmaMatrix_26, mmaMatrix_22, mmaMatrix_23, mmaMatrix_26);\nrocwmma::mma_sync&lt;2, 3, 0&gt;(mmaMatrix_27, mmaMatrix_22, mmaMatrix_23, mmaMatrix_27);\n\n// Store C data for blocks [0, 4)\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_24);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_25);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_26);\nrocwmma::store_matrix_sync&lt;0, rocwmma::layout_t::mem_row_major, 1024&gt;(var0, arg2 + ..., mmaMatrix_27);\n}\n}\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/#benchmarking-using-hatlib","title":"Benchmarking using <code>hatlib</code>","text":"<p>The complete python script with both scheduling policies can be found here. Benchmarking this <code>.hat</code> package on a AMD MI100 system outputs the following, which shows how using pass-order scheduling can yield slightly better performance:</p> <pre><code>                                    function_name        mean  median_of_means  mean_of_small_means  robust_mean  min_of_means\n0  tensor_block_order_matmul_gpu_a0f5a6cd5453d086 29.03946851      29.04570557          29.02557666  29.04261027   29.00701904\n1   tensor_pass_order_matmul_gpu_0d6383ac17fdfc9c 26.11595337      26.12025391          26.09279639  26.11305623   26.07321045\n</code></pre>"},{"location":"Tutorials/GPU/Tensor_MatMul_SchedulingPolicy_GPU/#exercises-for-the-reader","title":"Exercises for the reader","text":"<ol> <li>Try to tensorize a schedule with multiple tensor passes and see how different scheduling policies affect performance.</li> <li>Now, try to fuse some of these passes and see if that changes anything.</li> </ol>"}]}